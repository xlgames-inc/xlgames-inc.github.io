<?xml version="1.0"?>
<rss version="2.0">
  <channel>
    <title>XLE development blog</title>
    <description>XLE development blog</description>
    <link>http://xlgames-inc.github.io</link>
    <pubDate>January 02, 2017</pubDate>
    <item>
      <title>Spherical Harmonics and applications in real time graphics</title>
      <link>http://xlgames-inc.github.io/posts/sphericalharmonics0</link>
      <pubDate>December 27, 2016</pubDate>
      <description>&lt;p&gt;Yikes; it&amp;#39;s been awhile since my last update!&lt;/p&gt;

&lt;p&gt;I wanted to share a little bit of information from a technique I&amp;#39;ve recently been working on for an unrelated project. The technique uses a series of equations called &amp;quot;spherical harmonics&amp;quot; for extremely efficient high quality diffuse environment illumination.&lt;/p&gt;

&lt;p&gt;This is a technique that started to become popular maybe around 15 years ago -- possibly because of its usefulness on low power hardware. It fell out of favour for awhile, I was never entirely clear why. I got some good value from it back then, and I hope to get more good value from the technique now; so perhaps it&amp;#39;s time for spherical harmonic&amp;#39;s star to come around again?&lt;/p&gt;

&lt;p&gt;There&amp;#39;s a fair amount of information about spherical harmonics on the internet, but some of can be a little dense. There seems to be lack of information on how to take the first few steps in applying this math to the graphics domain (for example, for diffuse environment lighting). So I&amp;#39;ll try to keep this page approachable for graphics programmers, while also linking off to some of the more dense and abstract stuff later on. And I&amp;#39;ll focus specifically on how I&amp;#39;m using this math for graphics, how I&amp;#39;ve used it in the past, and how I&amp;#39;d like that to grow in the future.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;What are spherical harmonics&lt;/h2&gt;

&lt;p&gt;The &amp;quot;spherical harmonics&amp;quot; are a series of equations that we&amp;#39;ll use to compress lighting information greatly. The easiest way to understand them is to start with something simpler and analogous -- and that is cubic splines.&lt;/p&gt;

&lt;p&gt;Splines are a method of describing a curve through space with a finite number of points (or points and tangents). Even though the curve is defined by a finite number of parameters, the result is effectively an infinite number of points. To define the curve, we need equations of the form:&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/basic_spline.png"&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;strong&gt;t&lt;/strong&gt; is the distance along the spline (usually between 0 and 1) and &lt;strong&gt;x&lt;/strong&gt;, &lt;strong&gt;y&lt;/strong&gt; &amp;amp; &lt;strong&gt;z&lt;/strong&gt; are cartesian coordinates.
In the case of cubic splines, the functions &lt;strong&gt;f&lt;/strong&gt;, &lt;strong&gt;h&lt;/strong&gt; and &lt;strong&gt;g&lt;/strong&gt; are cubic polynomials. If you&amp;#39;ve used Bezier splines before, you may be familiar with a way to express these polynomials using a form called a &lt;em&gt;&amp;quot;Bernstein polynomial&amp;quot;&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id="toc_1"&gt;Bernstein basis&lt;/h3&gt;

&lt;p&gt;Imagine the folowing 4 equations:
&lt;br/&gt;&lt;img src="https://upload.wikimedia.org/wikipedia/commons/8/80/Bezier_basis.svg"&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;By Ben FrantzDale (Graphed in Matplotlib.) [GFDL (&lt;a href="http://www.gnu.org/copyleft/fdl.html"&gt;http://www.gnu.org/copyleft/fdl.html&lt;/a&gt;) or CC BY-SA 4.0-3.0-2.5-2.0-1.0 (&lt;a href="http://creativecommons.org/licenses/by-sa/4.0-3.0-2.5-2.0-1.0)"&gt;http://creativecommons.org/licenses/by-sa/4.0-3.0-2.5-2.0-1.0)&lt;/a&gt;], via Wikimedia Commons&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These are the Bernstein basis functions (&lt;strong&gt;B0()&lt;/strong&gt;, &lt;strong&gt;B1()&lt;/strong&gt;, &lt;strong&gt;B2()&lt;/strong&gt;, &lt;strong&gt;B3()&lt;/strong&gt;).
Using these functions, we can express a cubic polynomial in this form:
&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/bernsteinbasis.png"&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;strong&gt;w0&lt;/strong&gt;, &lt;strong&gt;w1&lt;/strong&gt;, &lt;strong&gt;w2&lt;/strong&gt; &amp;amp; &lt;strong&gt;w3&lt;/strong&gt; are constant weights. The four weight values will determine the shape of the curve we get. This is the important thing -- the Bernstein basis functions are defined so that we get any cubic polynomial function, just by changing the 4 weight values. And the four weight values provide a convenient way to express a spline (alternatively we can do similar things with other basis functions -- like the &lt;em&gt;Hermite basis&lt;/em&gt;, or for different types of equations, the &lt;em&gt;Fourier basis&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;When dealing with cubic polynormals, we use 4 Bernstein basis equations; but the equations are defined for any degree polynomials. So, for example, if we were working with 5th degree polynomials, then we could calculate 6 associated Bernstein basis functions.&lt;/p&gt;

&lt;h3 id="toc_2"&gt;SH basis&lt;/h3&gt;

&lt;p&gt;But we want to know how this relates to spherical harmonics! &lt;em&gt;(But the interested might want to diverge off to Wolfram &amp;amp; Wikipedia: &lt;a href="http://mathworld.wolfram.com/BernsteinPolynomial.html"&gt;http://mathworld.wolfram.com/BernsteinPolynomial.html&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/B%C3%A9zier_curve"&gt;https://en.wikipedia.org/wiki/B%C3%A9zier_curve&lt;/a&gt;)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;It turns out that the &amp;quot;spherical harmonics&amp;quot; are actually a series of equations which are a little bit similar to the Bernstein basis functions (&amp;quot;harmonic&amp;quot; being just a name for a certain kind of equation). But the spherical harmonics are special because they are defined for a very specific type of data. And that is values that are defined across the surface of a sphere...&lt;/p&gt;

&lt;p&gt;Above, we used Bernstein basis functions to express polynomials. But the type of function that can be expressed using spherical harmonics looks a little like this:
&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/spherical_function.png"&gt;&lt;/p&gt;

&lt;p&gt;Here, &lt;strong&gt;theta&lt;/strong&gt; and &lt;strong&gt;phi&lt;/strong&gt; are spherical coordinates. Also note that another way to express the spherical coordinates &lt;strong&gt;theta&lt;/strong&gt; and &lt;strong&gt;phi&lt;/strong&gt; is by using a unit length vector!&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Simplest application in graphics&lt;/h2&gt;

&lt;p&gt;So why is this type of equation useful? Well, there&amp;#39;s another common construct in graphics that represents values stored over the surface of sphere -- that is a &lt;em&gt;cubemap&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;When we use a cubemap for the background sky in a shader, we probably have a line of shader code a little like this:
&lt;code&gt;
float3 skyColor = MySkyCubeMap.Sample(MySampler, unitLengthVector.xyz).rgb;
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For any given direction, the cubemap will give us a colour value. Even though it&amp;#39;s a &lt;strong&gt;cube&lt;/strong&gt;map, it&amp;#39;s defining colour values across the surface of a sphere &lt;em&gt;(before cubemaps we used other mappings, like &lt;strong&gt;spheremaps&lt;/strong&gt; and &lt;strong&gt;paraboloid&lt;/strong&gt; maps -- but it turns out that, oddly, a cube is just the cheapest approximation to a sphere for shaders)&lt;/em&gt;. We could use spherical harmonics to do something very similar.&lt;/p&gt;

&lt;p&gt;In the spline example, the analogous operation would be fitting a curve to texture values. So imagine we had an RGB single dimensional texture. We would need 1 spline for each component, but let&amp;#39;s take red as an example. Since this is a 1D texture, we can graph the red values against the (1D) texture coordinate as so:&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/fit_curve.svg"&gt;&lt;/p&gt;

&lt;p&gt;In theory, we can find a best fit cubic spline for this data. If we do that, we can use the spline as a (non-discrete) replacement for the texture (if we were doing the reverse -- using a texture as a replacement for a function -- we would called it a lookup table).&lt;/p&gt;

&lt;p&gt;The degree of the spline will determine how accurately we can approach the source data. Most textures would need very high degree polynomials to even get close. In general, high frequency information is hard to replicate, but low frequency information can sometimes come through.&lt;/p&gt;

&lt;p&gt;In practice, this usually isn&amp;#39;t a very useful thing to do -- though there are applications along this line of thought (such as cubic interpolation between texels, particularly with terrain heightmaps). We&amp;#39;re also starting to tread on &amp;quot;signal processing&amp;quot; ground (which we might use for deep ocean wave simulations, like the one in XLE).&lt;/p&gt;

&lt;p&gt;So why would we ever want to replace a cubemap with a function using spherical harmonics? Isn&amp;#39;t that just as useless? The answer is no! There are some important cases where spherical harmonics are both accurate enough and useful enough to be worth our attention!&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Cartesian spherical harmonics&lt;/h2&gt;

&lt;p&gt;The spherical harmonics can be expressed in spherical coordinates or cartesian coordinates. For graphics, the cartesian form is much more useful.&lt;/p&gt;

&lt;p&gt;Our notation for the harmonics themselves will be this:
&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/cartesian_form.png"&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;em&gt;l&lt;/em&gt; (lower case L) is the &amp;quot;band index&amp;quot; and &lt;em&gt;m&lt;/em&gt; is a value with the constraint &lt;em&gt;-l&lt;/em&gt; &#x2264; &lt;em&gt;m&lt;/em&gt; &#x2264; &lt;em&gt;l&lt;/em&gt;.
The number of bands is the analogue of the degree of a polynomial -- more bands means greater accuracy and better reproduction of higher frequency data. For realtime stuff, we&amp;#39;ll focus on the first 3 &amp;quot;bands&amp;quot;.
If you look around Wolfram or Wikipedia, you&amp;#39;ll see more complex forms of the equations. But this form is good for our applications.&lt;/p&gt;

&lt;p&gt;There are the first 3 bands, in the form we&amp;#39;ll use in XLE:
&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/cartesian_form_eqns.png"&gt;&lt;/p&gt;

&lt;p&gt;To get a sense of how these equations look in 3D space, here&amp;#39;s a picture of the first 4 bands from Wikipedia (note that the blue represents positive numbers, the yellow is negative):
&lt;br&gt;&lt;a title="By Inigo.quilez (Own work) [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3ASpherical_Harmonics.png"&gt;&lt;img width="512" alt="Spherical Harmonics" src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Spherical_Harmonics.png/512px-Spherical_Harmonics.png"/&gt;&lt;/a&gt;
&lt;br&gt;&lt;em&gt;By Inigo.quilez (Own work) [CC BY-SA 3.0 (&lt;a href="http://creativecommons.org/licenses/by-sa/3.0)"&gt;http://creativecommons.org/licenses/by-sa/3.0)&lt;/a&gt;], via Wikimedia Commons&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It&amp;#39;s important to be aware that you&amp;#39;ll see slightly different variations of these equations in different publications. This is why it&amp;#39;s not always safe to combine equations from different sources (such as the rotation equations and optimization equations we&amp;#39;ll get to later).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;this form employs the &amp;quot;&lt;strong&gt;Condon-Shortley phase&lt;/strong&gt;&amp;quot;. This basically just adds the negative sign in front of some of the equations. This reduces the complexity of some of derived equations (such as the equations for rotation). Not all graphics literature use this form (for example, Robin Green doesn&amp;#39;t, but Peter-Pike Sloan does)&lt;/li&gt;
&lt;li&gt;the order in which &lt;em&gt;y&lt;/em&gt;, &lt;em&gt;z&lt;/em&gt; &amp;amp; &lt;em&gt;x&lt;/em&gt; appear in band 1 can be different in different forms (this ordering seems to be most common, but it does make the rotation code a little more confusing)&lt;/li&gt;
&lt;li&gt;this form assumes that the vector [&lt;em&gt;x&lt;/em&gt; &lt;em&gt;y&lt;/em&gt; &lt;em&gt;z&lt;/em&gt;] is unit length (ie: &lt;em&gt;x&lt;/em&gt;^2+&lt;em&gt;y&lt;/em&gt;^2+&lt;em&gt;z&lt;/em&gt;^2=1). For our applications, that&amp;#39;s always true -- but many sources don&amp;#39;t mention that as an assumption&lt;/li&gt;
&lt;li&gt;the equation for &lt;em&gt;m&lt;/em&gt;=0 &amp;amp; &lt;em&gt;l&lt;/em&gt;=2 is often expressed with 2&lt;em&gt;z&lt;/em&gt;^2 - &lt;em&gt;x&lt;/em&gt;^2 - &lt;em&gt;y&lt;/em&gt;^2 in the brackets. But, given the unit length assumption, this equivalent&lt;/li&gt;
&lt;li&gt;finally, there seems to be some uncertainty about the constant in the equation when &lt;em&gt;m&lt;/em&gt;=2 &amp;amp; &lt;em&gt;l&lt;/em&gt;=2 (maybe?). While working on this page, I noticed that some sources have 15/16, others have 15/32. I&amp;#39;m not sure what&amp;#39; going on there -- I might have to re-derive the equation to figure that out&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In short, we can&amp;#39;t just copy/paste our why through spherical harmonics. We have to know what&amp;#39;s going on. (It&amp;#39;s also a good example of why you should never trust the correctness of any code you find on the internet!)&lt;/p&gt;

&lt;h2 id="toc_5"&gt;Projecting into spherical harmonics and reconstruction&lt;/h2&gt;

&lt;p&gt;As mentioned above, we want to combine the spherical harmonics with weight values, and use this combination to express some complex spherical function. Converting a function into spherical harmonics is called &amp;quot;projection.&amp;quot; Using the spherical harmonics to get back an approximation of the original function is called &amp;quot;reconstruction.&amp;quot;&lt;/p&gt;

&lt;p&gt;The projected form of a function is just the set of weights -- one weight for each spherical harmonic (up to some limit on the number of bands). The weights are called coefficients. Let&amp;#39;s say our input function is the red channel of a cubemap. We&amp;#39;ll use 3 bands to approximate that -- that&amp;#39;s 9 equations, and 9 coefficients. Since we also want the green and blue channels, that&amp;#39;s another 9 coefficients per channel.&lt;/p&gt;

&lt;p&gt;In effect, we end up with an approximation of the input cubemap in just 9 color values. Obviously we can&amp;#39;t represent much high frequency data with so few colours. We could get higher quality versions with more bands (4 bands would be 16 colour coefficients, 5 would be 25, 6 would be 36). This might be useful for spherical reflections, but for diffuse (as we&amp;#39;ll see later) there&amp;#39;s a good reason to stop at 3 bands.&lt;/p&gt;

&lt;h3 id="toc_6"&gt;Projection and Reconstruction&lt;/h3&gt;

&lt;p&gt;Projection is surprising simple. We can calculate each coefficient separately, and we do that by just finding the integral of the input function multiplied by the corresponding spherical harmonic.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/projection.png"&gt;&lt;/p&gt;

&lt;p&gt;Since our input function is discreet (ie, it&amp;#39;s just pixels in a cubemap), the integral becomes simple. We just need to multiply the pixel values by the spherical harmonic for the cubemap direction -- the only difficulty is to weight for the solid angle of the cubemap pixel (see the XLE shader code for this equation).&lt;/p&gt;

&lt;p&gt;The reconstruction formula is also very simple; and should now look oddly familiar:&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/reconstruction.png"&gt;&lt;/p&gt;

&lt;p&gt;Here, we just take the weighted sum of the spherical harmonics for a given direction (where &lt;em&gt;b&lt;/em&gt; is the band number limit). It&amp;#39;s pretty straight forward; but it gets more complex from here.&lt;/p&gt;

&lt;h3 id="toc_7"&gt;Example&lt;/h3&gt;

&lt;p&gt;So what does it look like? Here&amp;#39;s an example environment texture and the equivalent constructed environment. I&amp;#39;m going to skip ahead just a bit and show how it looks when we construct diffuse reflectance from an environment texture.
These are equirectangular panorama maps; trivially compressed to LDR jpgs.&lt;/p&gt;

&lt;h3 id="toc_8"&gt;Source texture&lt;/h3&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/mans_0.jpg"&gt;
&lt;br/&gt;(from sIBL Archive: &lt;a href="http://www.hdrlabs.com/sibl/archive.html"&gt;http://www.hdrlabs.com/sibl/archive.html&lt;/a&gt;)&lt;/p&gt;

&lt;h3 id="toc_9"&gt;Reference diffuse response&lt;/h3&gt;

&lt;p&gt;ie, for each direction in the panorama map, this shows the correct diffuse response of the environment, from a plain lambert diffuse BRDF.
&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/reference.jpg"&gt;&lt;/p&gt;

&lt;h3 id="toc_10"&gt;Reconstructed response with spherical harmonics compression&lt;/h3&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/reconstruct.jpg"&gt;&lt;/p&gt;

&lt;h3 id="toc_11"&gt;Absolute difference multiplied by 20&lt;/h3&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/SphericalHarmonics/diff.jpg"&gt;&lt;/p&gt;

&lt;p&gt;The reconstructed response is using 3 bands; so it&amp;#39;s stored using just 9 colour values.
As you can see, there is some information loss -- but we can actually prove mathematically that the maximum error is pretty small.&lt;/p&gt;

&lt;h2 id="toc_12"&gt;To be continued&lt;/h2&gt;

&lt;p&gt;So, we&amp;#39;ve got the very basics down. We know enough to be able to compress a cubemap down to spherical harmonic form, and reconstruct an approximation from that.&lt;/p&gt;

&lt;p&gt;Alone, it may sound like an interesting thing to do; but it&amp;#39;s not very useful yet. I&amp;#39;ll follow this up with some details on how we can use spherical harmonics for some really interesting stuff. Still to cover:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;combining our compressed environment information with the lambert diffuse equation&lt;/li&gt;
&lt;li&gt;building optimized reconstruction code for shaders&lt;/li&gt;
&lt;li&gt;rotating and manipulating spherical harmonics at runtime (and optimization thereof)&lt;/li&gt;
&lt;li&gt;applications in global illumination schemes&lt;/li&gt;
&lt;li&gt;some examples from around the web, and a list of the right literature to follow up with&lt;/li&gt;
&lt;li&gt;some cool examples of SH used in games (eg; &lt;em&gt;The Last of Us&lt;/em&gt;, &lt;em&gt;The Order: 1886&lt;/em&gt;)!&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    <item>
      <title>Latest Update</title>
      <link>http://xlgames-inc.github.io/posts/juneupdate</link>
      <pubDate>June 19, 2016</pubDate>
      <description>&lt;p&gt;Work is still continuing on XLE! Over the last few weeks, I&amp;#39;ve have been very distracted by other priorities. I&amp;#39;m likely to be somewhat busy and distracted during for a few more weeks until things return to a more normal situation.
However, I&amp;#39;m still finding time to work on XLE, and make some improvements and fixes!&lt;/p&gt;

&lt;p&gt;Lately, my focus has been on &lt;strong&gt;Vulkan&lt;/strong&gt; support (in the &lt;em&gt;experimental&lt;/em&gt; branch). Vulkan is looking more and more stable and reliable, and the HLSL -&amp;gt; SPIR-V path is working really well now!
However, there are still some big things I want to improve on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Improved interface for BoundUniforms, that can allow for precreation of descriptor sets during loading phases&lt;/li&gt;
&lt;li&gt;Improved interface for Metal::ConstantBuffer and temporary vertex/index buffers that would rely on a single large circular device buffer&lt;/li&gt;
&lt;li&gt;Pre-create graphics pipelines when using &lt;code&gt;SharedStateSet&lt;/code&gt; (eg, when rendering &lt;code&gt;RenderCore::Assets::ModelRenderer&lt;/code&gt;)

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;(this might also involve a better solution for the render state resolver objects used by materials)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Fix triangle hit-test supported used by the tools (which is complicated by the render pass concept and requires stream output support)&lt;/li&gt;
&lt;li&gt;Tessellation support for terrain rendering&lt;/li&gt;
&lt;li&gt;Integration of MSAA resolve for Vulkan&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these changes, we should start to see much more efficient results with the Vulkan pipeline. Some SceneEngine features won&amp;#39;t work immediately, but otherwise we will be getting very good Vulkan results, while also retaining the DirectX compatibility! &lt;/p&gt;

&lt;p&gt;Lately, there have been many cool improvements related to Vulkan, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Lots of improvements to the HLSLcc cross compiler that coverts HLSL -&amp;gt; GLSL (which becomes SPIR-V)

&lt;ul&gt;
&lt;li&gt;this is now working for almost all shaders, and adds support for many HLSL features that previously weren&amp;#39;t well supported by the HLSLcc project&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Support for using Vulkan within the GUI tools!&lt;/li&gt;
&lt;li&gt;Properly tracking GPU progress for object destruction, allowing full descynronisation of CPU and GPU&lt;/li&gt;
&lt;li&gt;Support for event pools, and the GPUProfiler&lt;/li&gt;
&lt;li&gt;Reduction of dependencies on the RenderCore::Metal project by projects that should be gfx-api agnostic&lt;/li&gt;
&lt;li&gt;Core lighting support including IBL and shadows&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please keep checking back regularly for the latest updates!&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Vulkan prototype - metal layer refactoring</title>
      <link>http://xlgames-inc.github.io/posts/metallayerrefactoring</link>
      <pubDate>May 09, 2016</pubDate>
      <description>&lt;p&gt;XLE has a thin layer over the underlying graphics API called &amp;quot;Metal&amp;quot;. This was originally built for DirectX11 and OpenGLES. But over time it became more DirectX-focused. Part of the goal of building in Vulkan support was to provide a basis for refactoring and improving the metal layer.&lt;/p&gt;

&lt;p&gt;The goals for this layer are simple:
- compile time polymorphism between underlying graphics APIs
 - not link time or run time. We know the target during compilation of client code
- &amp;quot;leaky&amp;quot; abstraction layer
 - meaning that most client code is independent of the underlying graphics API
 - but the underlying objects are still accessible, so client code can write API-specific code when needed
- very thin, minimal overhead
 - for example, many DeviceContext methods get inlined into client code, meaning that performance is similar to using the underlying API directly&lt;/p&gt;

&lt;p&gt;To make this kind of layer work, we need to find abstractions that work well for all target APIs. Usually this means finding abstractions that are great for one API, and pretty good for other APIs. That can be tricky, particularly as APIs are changing and evolving over time.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Descriptor Set concept&lt;/h2&gt;

&lt;p&gt;Ideally we want the concept of &amp;quot;descriptor sets&amp;quot; to exist in the metal API somewhere. There are two reasons for this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Clean up the DeviceContext interface so there are fewer BindXX(...) methods&lt;/li&gt;
&lt;li&gt;pre-cook permanent descriptor sets using BoundUniforms (or otherwise), so that they can be reused frame to frame&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DirectX11 has no concept of descriptor sets, though, so this could be a bit awkward. There are a couple of tricky problems&lt;/p&gt;

&lt;h3 id="toc_1"&gt;Sharing descriptors across shader stages&lt;/h3&gt;

&lt;p&gt;In modern APIs, we can bind a descriptor set to multiple shader stages; meaning that binding a resource one can make it accessible to multiple shaders. However, DirectX11 strictly separates the stages so that we explicitly bind only to a single stage.&lt;/p&gt;

&lt;p&gt;So how to we create a single solution that handles both these cases?&lt;/p&gt;

&lt;p&gt;One option is to expand the &amp;quot;root signature&amp;quot; concept. This defines a set of virtual binding points, which can be redirected to the true underlying binding points. The virtual binding points can know which shader stages they apply to -- and so can calls the appropriate underlying binding functions.&lt;/p&gt;

&lt;p&gt;This might have some long-term advantages because it also allows us to abstract the C++ code completely from the true binding points expressed in the shader files. There&amp;#39;s a little extra overhead, but maybe it&amp;#39;s not a huge issue.&lt;/p&gt;

&lt;p&gt;That would be a great solution for &amp;quot;frame-global&amp;quot; or long-term bindings. It would also be great for our few global SamplerState objects (which can easily become immutable in Vulkan, and handled automatically in DirectX).&lt;/p&gt;

&lt;p&gt;But for short-term dynamic bindings, it may not be great. Often we&amp;#39;re just binding one or two resources or constant buffers to the first few binding points, and using them for a single draw call. For this, we might need something different.&lt;/p&gt;

&lt;p&gt;If the &amp;quot;dynamic&amp;quot; descriptor set in Vulkan was very large, we could more easily have separate binding points for resources that are shared between stages, and those that are used only in one. However, currently we need to keep this descriptor set small because we need to write to all binding points, regardless of whether they are used or not. If a shader accesses an unused descriptor, we will get a device-lost error.&lt;/p&gt;

&lt;p&gt;This could be made better by calculating a mask of the used slots in the dynamic descriptor set for each shader. This would allow us to make better decisions about when to write to this dynamic descriptor set. Most draw calls don&amp;#39;t need this dynamic set (it&amp;#39;s mostly required by procedurally driven effects, and less important for data driven geometry like models), and this approach should reduce the overhead in the most common cases.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Vulkan latest progress -- core lighting working</title>
      <link>http://xlgames-inc.github.io/posts/vulkanmoreprogress</link>
      <pubDate>May 04, 2016</pubDate>
      <description>&lt;p&gt;The Vulkan build is steadily getting more and more functionality. Now the core rendering pipeline in SceneEngine is working -- which means we can have deferred lighting, shadows, IBL, tonemapping, etc. Simple scene should render correctly now. But there are some inefficiencies and issues (see below).&lt;/p&gt;

&lt;p&gt;Unfortunately the DirectX11 version isn&amp;#39;t working at the moment. This is all in the &amp;quot;experimental&amp;quot; branch.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Declarative render passes&lt;/h2&gt;

&lt;p&gt;Tone-mapping now works, and it was a good prototype for the &amp;quot;declarative&amp;quot; render pass model. This allows us to specify render passes and render targets required using a &amp;quot;description&amp;quot; structure. The system will do some caching and correlate request to resources, creating and binding as necessary.&lt;/p&gt;

&lt;p&gt;There is some overhead with this design because it involves doing some per-frame hash and lookups as we go along. It&amp;#39;s not as efficient as (for example) just pre-creating all of the frame buffer / render pass objects in a configure step. However, this design is maybe a little more flexible and easier to tie into existing scene engine code. In effect, we&amp;#39;re building a new layer that is just one step more abstract from the underlying Vulkan objects.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve pushed some of the &amp;quot;busy-work&amp;quot; (like declaring subpass dependencies) down into the RenderCore::Metal layer. This makes the interface easier to use... But the downside is that my abstraction is not expressive enough for some unusual cases. For example, I came across a cases where we want to bind the &amp;quot;depth &amp;amp; stencil&amp;quot; aspects of a depth texture in one subpass; and in the second subpass only the &amp;quot;stencil&amp;quot; aspect is bound. This apparently needs a dependency... But it&amp;#39;s just really inconvenient with this interface.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve also build a concept called &amp;quot;named resources&amp;quot; into the Metal::DeviceContext. This allows us to get TextureViews for  attachments from the device context. It feels out of place because it&amp;#39;s an operation that doesn&amp;#39;t involve the hardware, but there doesn&amp;#39;t seem to be any better way to handle this case.&lt;/p&gt;

&lt;p&gt;Fundamentally we want to define attachments FrameBufferDesc objects, so that we can later refer to them again by binding id. It would be better if some of this functionality was in the RenderCore::Techniques library... But it would be just too much hassle to split it better Techniques and Metal.&lt;/p&gt;

&lt;p&gt;Anyway, it&amp;#39;s working now in Vulkan. However, I still haven&amp;#39;t got to the caching and reuse part. And it also needs to be implemented for DirectX11, also!&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Compute shader work!&lt;/h2&gt;

&lt;p&gt;I added in support for the compute pipeline. It actually was pretty easy. I decided to switch some of the tonemapping code from pixel shaders to compute shaders -- because this seems to be more natural in Vulkan. Working with viewports and render targets is much more complex in Vulkan than DirectX11.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Render state objects working &amp;amp; dynamic pipeline objects&lt;/h2&gt;

&lt;p&gt;Now all of the render state objects (like BlendState, SamplerState, etc) work. However all of the &amp;quot;pipeline objects&amp;quot; are dynamically created as needed. Vulkan allows these to be pre-calculated and optimised and load time. This is pretty smart, because a lot of that render state information can just become shader instructions and combined into the shader code.&lt;/p&gt;

&lt;p&gt;I think the SharedStateSet techniques might be able to precalculate pipeline objects. But a lot of pipeline objects will have to be dynamically created like this. But perhaps I can use the &amp;quot;inheritance&amp;quot; stuff in pipeline objects to calculate some stuff earlier (for example, in ShaderPrograms).&lt;/p&gt;

&lt;p&gt;For the moment, pipeline objects are just created and recreated like crazy!&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Separated Samplers and Textures in HLSLCrossCompiler&lt;/h2&gt;

&lt;p&gt;I found that GL_KHR_vulkan_glsl added support for separate Sampler and Texture objects to GLSL. So I added support for this to the HLSLCrossCompiler! This is a huge help, because otherwise it would be a hassle to work around the standard HLSL model of separate objects.&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Many fixes in HLSLCrossCompiler&lt;/h2&gt;

&lt;p&gt;As I go through, I&amp;#39;m finding issues in the HLSLCrossCompiler. It&amp;#39;s quite strange because the compiler supports so many instructions and modes... But they there are certain things that are just incorrect. In particular, I&amp;#39;ve had some fixes to certain swizzle operations, and made some fixes dealing with constant buffers.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s really quite fascinating because this cross compiler path works so well for so many shaders... I&amp;#39;m really using a lot of HLSL features in XLE, and I&amp;#39;m often happily surprised by the cross compiler just &amp;quot;figuring it out.&amp;quot; But then suddenly some shader will use an instruction in a slightly different way, and everything falls apart.&lt;/p&gt;

&lt;p&gt;Anyway, it&amp;#39;s getting more reliable as I go along.&lt;/p&gt;

&lt;h2 id="toc_5"&gt;Better model for descriptor sets&lt;/h2&gt;

&lt;p&gt;I finally settled on a much better model for descriptor sets. I borrowed some ideas from the DirectX12 &amp;quot;root signatures&amp;quot; -- these allow us to creating a mapping between the linear register mapping in HLSL (eg, register(t2), register(b3), etc) and a system of descriptor sets and binding points.&lt;/p&gt;

&lt;p&gt;So I&amp;#39;ve re-purposed that idea for XLE. The root signatures actually also defines the &amp;quot;descriptor set layouts.&amp;quot; This allows us to create a few global layouts, and reuse them for all shaders. This seems to be the best way.&lt;/p&gt;

&lt;p&gt;We have 4 descriptor sets available; I&amp;#39;m using one for BoundUniforms, one for &amp;quot;dynamic&amp;quot; bindings (not-BoundUniforms) and one for global resources that remain bound the entire frame. And so there&amp;#39;s one left over; maybe it will be used for input attachments.&lt;/p&gt;

&lt;p&gt;There&amp;#39;s still some thrashing that can occur. But this design seems reasonable. I may expand the interface for BoundUniform that will allow for every efficient use.&lt;/p&gt;

&lt;h2 id="toc_6"&gt;More to come...&lt;/h2&gt;

&lt;p&gt;So it&amp;#39;s working! But there&amp;#39;s still a lot to come. Some things got broke while changing things, and there&amp;#39;s still a lot of perform improvements to make. But it&amp;#39;s looking ok so far.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Vulkan prototype - how many separate render passes in the lighting parser?</title>
      <link>http://xlgames-inc.github.io/posts/vulkandeclarativelightingparser</link>
      <pubDate>April 22, 2016</pubDate>
      <description>&lt;p&gt;The lighting parser is a set of steps (including geometry rendering and full screen passes) that occur in a similar order and configuration every frame. In some ways it is like a higher level version of a &amp;quot;renderpass&amp;quot; (or frame buffer layout). Each &amp;quot;subpass&amp;quot; in the renderpass is like a step in the lighting parser process for evaluating the frame.&lt;/p&gt;

&lt;p&gt;But how do we map the lighting parser steps onto render pass subpasses? Do we want to use one single huge render pass for everything? Or a number of small passes?&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Declarative lighting parser&lt;/h2&gt;

&lt;p&gt;Since we&amp;#39;re using a &amp;quot;declarative&amp;quot; approach for &amp;quot;frame buffer layouts&amp;quot;, we could do the same for the lighting parser. Imagine a LightingParserConfiguration object that takes a few configuration variables (related to the current platform and quality mode, and also scene features) and produces the set of steps that the lighting parser will perform.&lt;/p&gt;

&lt;p&gt;We could use the &lt;code&gt;VariantFunctions&lt;/code&gt; interface for this, along with some structures to define the &amp;quot;attachment&amp;quot; inputs and outputs for each step. If we have a step (such as tonemapping) we just define the input attachments, output attachments, and temporary buffers required, and add the tone map function.&lt;/p&gt;

&lt;p&gt;Then we would just need a small system that could collect all this information, and generate a &lt;code&gt;RenderCore::FrameBufferDesc&lt;/code&gt; object from it.&lt;/p&gt;

&lt;p&gt;This would allow us to have a very configurable pipeline, because we could just build up the list of steps as we need them, and from there generate everything we need. In effect, we would declare everything that is going to happen in the lighting parser before hand; and then the system would automatically calculate everything that needs to happen when we call Run().&lt;/p&gt;

&lt;p&gt;This could also allow us to have a single render pass for the entire lighting parser process. But is that what we really want?&lt;/p&gt;

&lt;h2 id="toc_1"&gt;How many render passes?&lt;/h2&gt;

&lt;p&gt;There are a few things to consider:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Compute shader usage&lt;/li&gt;
&lt;li&gt;frame-to-frame variations in the render pass&lt;/li&gt;
&lt;li&gt;parallization within the lighting parser&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="toc_2"&gt;Compute shaders&lt;/h2&gt;

&lt;p&gt;We can&amp;#39;t execute compute shaders during a render pass. &lt;code&gt;vkCmdDispatch&lt;/code&gt; can only be used outside of a render pass. This is an issue because some of our lighting parser work current uses compute shaders (such as the tone mapping) -- effectively meaning that the render pass must be split there.&lt;/p&gt;

&lt;p&gt;This could be converted to pixel shaders, I guess. But it would mean that we would have to either rewrite all of the compute shader stuff, or move it to a precalculation phase before the render pass.&lt;/p&gt;

&lt;p&gt;Also, sometimes we might want to call vkCmdCopyImage to duplicate a buffer. Again, it can be emulated with pixel shaders, I guess.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Frame-to-frame variations&lt;/h2&gt;

&lt;p&gt;We don&amp;#39;t really want the render pass to change from frame to frame, because the render pass is a parameter to Vulkan graphics pipelines. There is some flexibility for &amp;quot;compatibility&amp;quot; between render passes. However, what we don&amp;#39;t want is a situation where a change related to one subpass effects the pipelines of another subpass. In that case, it would be better to use 2 separate render passes, so that changes in one stage of the pipeline don&amp;#39;t effect other stages.&lt;/p&gt;

&lt;p&gt;This could be an issue with postprocessing effects that are sometimes enabled (such as a radial blur when damaged, or refractions in water). We could always just skip a subpass by calling NextSubpass immediately, I guess. But having many optional subpasses might blunt the benefit of having a huge render pass, anyway.&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Parallization within the lighting parser&lt;/h2&gt;

&lt;p&gt;Most of the steps in the lighting parser are not really very parallizable with respect to each other. For example, the steps before tonemapping must all be complete before tonemapping occurs, and the steps after tonemapping can&amp;#39;t begin until tonemapping ends.&lt;/p&gt;

&lt;p&gt;So, in this situation, it&amp;#39;s not clear how much benefit there is in combining everything in a giant render pass.&lt;/p&gt;

&lt;h2 id="toc_5"&gt;Simple lighting parser&lt;/h2&gt;

&lt;p&gt;The lighting parser was originally intended to have a very simple structure involving just conditions and function calls. It represents a fundamentally procedural operation, so it makes sense to implement it as basic procedures. That seems like the clearest way to give it a clear order and structure.&lt;/p&gt;

&lt;p&gt;However, moving to a more declarative pattern divorces us from that goal a little bit. That is, adding extra levels of abstraction within the lighting parser could also make it more difficult to understand and maintain.&lt;/p&gt;

&lt;h2 id="toc_6"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So, all in all, it&amp;#39;s not really clear what benefit there would be to have a single huge render pass. It seems more sensible to instead use a number of smaller render passes, one for each phase of the pipeline.&lt;/p&gt;

&lt;p&gt;This would probably mean a structure such as this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;initial opaque stuff

&lt;ul&gt;
&lt;li&gt;creating the gbuffer&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;opaque light resolve

&lt;ul&gt;
&lt;li&gt;perform deferred lighting&lt;/li&gt;
&lt;li&gt;ScreenSpaceReflections requires some compute shader work taking gbuffer as input and consumed during lighting resolve&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;multiple separate render passes for translucent rendering

&lt;ul&gt;
&lt;li&gt;in here we have things like ocean rendering, ordered transparency methods, particles, etc&lt;/li&gt;
&lt;li&gt;many of these methods require extra buffers and special requirements&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;possibly MSAA resolve, post processing steps and tonemapping could be combined into a single render pass&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The configurable lighting parser structure from above might still be useful for the post processing steps at the end.&lt;/p&gt;

&lt;p&gt;This means that the &amp;quot;gbuffer&amp;quot; will only be an &amp;quot;attachment&amp;quot; during the very first render pass. In other passes, it can be a normal image view / shader resource.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Vulkan prototype - the big issues</title>
      <link>http://xlgames-inc.github.io/posts/vulkanbigissues</link>
      <pubDate>April 15, 2016</pubDate>
      <description>&lt;p&gt;I&amp;#39;m making more progress on the &lt;em&gt;Vulkan&lt;/em&gt; prototype. Now the SceneEngine compiles, BufferUploads works, and it&amp;#39;s now possible to render basic geometry. Many features aren&amp;#39;t supported. But the core rendering features (such as binding textures, compiling shaders, pushing geometry through the pipeline) have an early implementation.&lt;/p&gt;

&lt;p&gt;So, while a working &lt;em&gt;Vulkan&lt;/em&gt; version is now close, I have a much better idea of what major hurdles there would be to get a really streamlined and efficient &lt;em&gt;Vulkan&lt;/em&gt; implementation!&lt;/p&gt;

&lt;h2 id="toc_0"&gt;RenderPasses&lt;/h2&gt;

&lt;p&gt;This is going to be the big issue. RenderPasses are a new concept in &lt;em&gt;Vulkan&lt;/em&gt; (though with heritage from &lt;em&gt;Apple Metal&lt;/em&gt; and &lt;em&gt;OpenGL&lt;/em&gt;), and not very compatible with the DirectX way of handling render targets. This is going to require some deep thought!&lt;/p&gt;

&lt;p&gt;I haven&amp;#39;t researched this thoroughly -- but this is my basic understanding of what might be happening...&lt;/p&gt;

&lt;p&gt;RenderPasses seem to be fundamentally designed around PowerVR-style hardware (such as that found in IPhones and some Android hardware). Now, if I understand the situation correctly, this type of hardware doesn&amp;#39;t need a traditional depth buffer. Instead it collates triangles as they are submitted, then splits them into buckets for tiles on the screen, and finally decomposes them into scanlines. The scanlines can be sorted using an old algorithm, so that for every pixel we know the front-most scanline.&lt;/p&gt;

&lt;p&gt;The key issue here is that that we collate triangle information in the frame buffer, and do not produce a final pixel value until all triangles are finished. This requires the hardware to maintain buffers of triangle scanlines attached to the frame buffer.&lt;/p&gt;

&lt;p&gt;The key problem here is this: what happens if we have render targets A and B, both of which are used in the same frame. We render to A, then to B, and then switch back to A and start rendering again?&lt;/p&gt;

&lt;p&gt;In a traditional DirectX &lt;code&gt;OMSetRenderTargets&lt;/code&gt; environment, this is not a big issue. We might do this (for example) to render a reflection into target B, which appears on geometry in target A. Maybe we cause a bit of a GPU pipeline stall; but it&amp;#39;s probably not going to be a major issue.&lt;/p&gt;

&lt;p&gt;With the PowerVR-style hardware, however, it&amp;#39;s a bigger deal. We want to continue collating triangle information in A throughout the entire frame. We don&amp;#39;t want to separate that into 2. It would be better if A and B were actually 2 separate viewports onto the same frame buffer.&lt;/p&gt;

&lt;p&gt;In other words, when we switch away from A, we kind of want to know if we will be returning to it. That&amp;#39;s what RenderPasses are useful for. We can express to the API that A is not finished yet, and that B just contains temporary data that will eventually be used on A.&lt;/p&gt;

&lt;p&gt;AMD claim on their website that this expressiveness is also useful for traditional forward rendering hardware. But it probably isn&amp;#39;t quite so significant.&lt;/p&gt;

&lt;p&gt;Nevertheless, Vulkan appears to be built entirely around RenderPasses, and the interface is very different from &lt;code&gt;OMSetRenderTargets&lt;/code&gt;. So we need to figure out what to do.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;RenderPass integration&lt;/h2&gt;

&lt;p&gt;There are 2 approaches:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Make RenderPasses a first-class feature of &lt;code&gt;RenderCore::Metal&lt;/code&gt;, and deprecate binding render targets

&lt;ul&gt;
&lt;li&gt;this would require all client code to change to using RenderPasses. For DirectX, the RenderPass object would just generate &lt;code&gt;OMSetRenderTarget&lt;/code&gt; calls&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Dynamically build simple RenderPasses when binding render targets

&lt;ul&gt;
&lt;li&gt;the client code wouldn&amp;#39;t need to change. However, the render passes generated would not be ideal. It would also require calling &lt;code&gt;VkCreateRenderPass&lt;/code&gt; and &lt;code&gt;VkCreateFramebuffer&lt;/code&gt; during the frame (which is probably not ideal)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Support both RenderPasses and binding render targets

&lt;ul&gt;
&lt;li&gt;clients can choose to use either&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#39;m encouraged to choose method one, even though it will require the most work.&lt;/p&gt;

&lt;p&gt;When there are these kinds of incompatibilities between APIs, it&amp;#39;s normally best to follow the design of one of the APIs fairly closely. So, we can write Vulkan code in a DirectX-like way, or we can write DirectX code in a Vulkan like way. In this case, the Vulkan method is more expressive and is a clear superset of the DirectX behaviour. Ignoring RenderPasses would make targeting PowerVR-style hardware (at a late date) much more difficult.&lt;/p&gt;

&lt;p&gt;But it seems like it will be a little difficult to make the transition to RenderPasses. It&amp;#39;s probably going to require significant changes in the SceneEngine. But it&amp;#39;s also a good opportunity to think about other improvements to render target management within the SceneEngine.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Descriptors&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Vulkan&lt;/em&gt; has a slightly different way to look at binding resources and constant buffers to shaders. We have a lot of new concepts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Descriptor sets&lt;/li&gt;
&lt;li&gt;Descriptor set layouts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These relate most closely to the &lt;code&gt;Metal::BoundUniforms&lt;/code&gt; object in XLE.&lt;/p&gt;

&lt;p&gt;The descriptor set layout contains a list of all of the bindings of a shader (or set of shaders) and the descriptor sets contain the list of objects to actually bind.&lt;/p&gt;

&lt;p&gt;Currently, the &lt;code&gt;BoundUniforms&lt;/code&gt; objects own both descriptor sets and descriptor set layouts. But I&amp;#39;m not sure this is a great idea, mostly because we end up with just a 1:1 relationship (which seems redundant).&lt;/p&gt;

&lt;p&gt;Both of these relate to the &amp;quot;binding&amp;quot; number set in the shader. My understanding at the moment is that the binding number should be a low number, and numbers should be used &amp;quot;mostly&amp;quot; sequentially -- like the register binding numbers in &lt;em&gt;DirectX&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Since XLE binds things via string names, we still need some way to associate the string name to a particular binding number. One idea is to use the hash of some string value for the binding numbers... I haven&amp;#39;t seen any documentation to say this is a bad idea -- but, Nevertheless, I still suspect that this is a bad idea. Near-sequential is going to be safer.&lt;/p&gt;

&lt;p&gt;We can choose 2 different designs for descriptor set layouts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;keep different descriptor set layouts bound tightly to specific shaders

&lt;ul&gt;
&lt;li&gt;ie, each shader could have it&amp;#39;s own layout, containing only the bindings relevant to that particular shader&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;share descriptor set layouts broadly

&lt;ul&gt;
&lt;li&gt;so layouts may contain a superset of bindings for any given shader&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we&amp;#39;re sharing descriptor sets, we could choose to do that on a per-technique level. That would kind of make sense. Or we could do it on a global level. That is, we could just have the single huge layout for each descriptor set binding index.&lt;/p&gt;

&lt;p&gt;The descriptor set binding indices are similar to the &amp;quot;bound uniforms stream index&amp;quot; in XLE. In particular, it makes sense to have a single shared layout for the global binding stream index. This would also move us closer to a &amp;quot;bindless&amp;quot; approach to these things.&lt;/p&gt;

&lt;p&gt;However, that introduces the problem of mapping between our current string binding names at the binding indices used by the shader. Since no single shader contains all bindings, the normal method of using reflection doesn&amp;#39;t work.&lt;/p&gt;

&lt;p&gt;In addition to the layout management issues, there are also issues managing the descriptor sets. Some descriptor sets have temporary data (ie, we expect it might change in a future frame). But some descriptor sets contain only static data. For example, the texture bindings of a model are set at load time and remain constant.&lt;/p&gt;

&lt;p&gt;For static descriptor set data, we would ideally write this once, and just reuse it every frame. That seems possible by adapting the &lt;code&gt;SharedStateSet&lt;/code&gt; interfaces. But it would require a few changes.&lt;/p&gt;

&lt;p&gt;For temporary descriptor set data, I suspect that the simple approach of just writing every frame and then throwing it away might be fine, and it would require fewer changes.&lt;/p&gt;

&lt;p&gt;Also, we want to be able to attach the &amp;quot;set&amp;quot; layout qualifier to the GLSL code. However, there is no equivalent to this in HLSL, so no way to get it through the HLSL -&amp;gt; GLSL approach. Maybe we can do some hacks via register bindings, but it might be awkward.&lt;/p&gt;

&lt;p&gt;We might possibly need to build a global table of bindings -- this table could assign the string name to binding ids, and also assign a &amp;quot;set&amp;quot; qualifier.&lt;/p&gt;

&lt;p&gt;At the moment it&amp;#39;s not perfectly clear what&amp;#39;s the best approach to dealing with descriptor sets. It might take some experimentation.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Pipelines&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Vulkan&lt;/em&gt; has some support for precalculating and reusing what it calls &amp;quot;pipelines.&amp;quot; These are a combination of all  shader stages, the descriptor layouts and the input layouts. But they also have some render state information mixed in.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s another big departure from &lt;em&gt;DirectX&lt;/em&gt;, because it pre-binds these objects together. Again, it&amp;#39;s a superset of &lt;em&gt;DirectX&lt;/em&gt; behaviour.&lt;/p&gt;

&lt;p&gt;It feels like we may have to dynamically create some pipelines during the frame with XLE. That said, for models (which should be the majority of geometry) we should be able to leverage the &lt;code&gt;SharedStateSet&lt;/code&gt; code, so that these can be reused over multiple frames.&lt;/p&gt;

&lt;p&gt;This would mean that some draw calls would use precreated pipelines, while others would use dynamically created pipelines. I think this might be the best balance between practicality and efficiency...&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Shader constant push buffers&lt;/h2&gt;

&lt;p&gt;In &lt;em&gt;Vulkan&lt;/em&gt;, we have a few new approaches for uploading shader constants. We still have uniform buffers (but we have a lower level interface for them). We also have &amp;quot;push buffers&amp;quot; which presumably use the command buffer to pass frame-by-frame data to the constant registers.&lt;/p&gt;

&lt;p&gt;Push buffers will be great for data that changes every frame. Uniform buffers are best for data that is retained for multiple frames, or reused in many draw calls.&lt;/p&gt;

&lt;p&gt;Fortunately, we also have the &lt;code&gt;ConstantBufferPacket&lt;/code&gt; concept in &lt;code&gt;Metal::BoundUniforms&lt;/code&gt;. This could be extended to be used with push constants. But push constants seem like they also require shader changes. That is, the maybe the shader must know if it&amp;#39;s going to receive those constants via push constants or a uniform buffer.&lt;/p&gt;

&lt;p&gt;It might be a good time to rethink how to handle the LocalTransform constants for model rendering. Often, the local transform will be constant over the model&amp;#39;s lifetime. In these cases, we might want to store the transform in a retained uniform buffer.&lt;/p&gt;

&lt;p&gt;But another alternative is to just use push constants. This will be required in the case of animated transforms. And maybe it could be reasonable efficient? Anyway, it might be interesting to think about.&lt;/p&gt;

&lt;h2 id="toc_5"&gt;Image layouts&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Vulkan&lt;/em&gt; has a new &amp;quot;image layout&amp;quot; concept. Each image object has a specific layout and we set these asynchronously using a command on the command buffer.&lt;/p&gt;

&lt;p&gt;Normally, we only care about layout while we&amp;#39;re creating the image or initializing it with data. But it&amp;#39;s sort of architecturally awkward, because we create the object synchronously using a &lt;code&gt;VkDevice&lt;/code&gt; call, but we don&amp;#39;t set the layout until we first use it in a command buffer. That&amp;#39;s a problem, because we usually don&amp;#39;t know when a use is the first time.&lt;/p&gt;

&lt;p&gt;All our images should be initialized via buffer uploads. This might make easier to solve this efficiently, because we can add in a &amp;quot;layout initialization&amp;quot; command list that always get executed as part of the buffer uploads update. In this way, buffer uploads will manage layouts, and other code can just ignore it. But it is going to create some difficulties for &lt;code&gt;Transaction_Immediate&lt;/code&gt; resources.&lt;/p&gt;

&lt;h2 id="toc_6"&gt;Resource deletion&lt;/h2&gt;

&lt;p&gt;XLE is often lazy about retaining references to objects. In general, we&amp;#39;re taking advantage of the way &lt;em&gt;DirectX11&lt;/em&gt; keeps objects alive while they&amp;#39;re required by the device.&lt;/p&gt;

&lt;p&gt;Often we allocate some object, use it for one frame, and then destroy (expecting the memory to be freed up when the GPU is finished with that frame). While might not be perfectly efficient, it is very convenient.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Vulkan&lt;/em&gt; doesn&amp;#39;t automatically track the GPUs progress when we free an object. So we need to know when it is currently referenced by a command buffer, and when it might be used in the future (or even if it&amp;#39;s currently being used by the GPU). This is complicated by cases with secondary command buffers (such as those used by the buffer uploads).&lt;/p&gt;

&lt;p&gt;So we need some new concepts for tracking GPU progress, and destroying objects when they are no longer needed.&lt;/p&gt;

&lt;p&gt;Likewise, some objects don&amp;#39;t need to be destroyed, but they might need to be overwritten. This happens in systems that are built for streaming. These cases are very similar -- &amp;quot;safe to delete&amp;quot; is often the same as &amp;quot;safe to overwrite.&amp;quot;&lt;/p&gt;

&lt;p&gt;The majority of objects can follow a simple path that just tracks a GPU frame index. After the GPU finishes a frame, then resources associated with that frame can be destroyed.&lt;/p&gt;

&lt;p&gt;In some cases, the destroy might actually be a &amp;quot;reset.&amp;quot; For example, if we have a number of pools of temporary descriptor sets, we must track the GPU progress to know when it&amp;#39;s safe to reset and reuse a pool.&lt;/p&gt;

&lt;p&gt;Some cases might be more complicated -- such as if we have resources tied to secondary command buffers that are retained over several frames. In this case, the resources must be maintained until after the command buffer is no longer used used and destroyed.&lt;/p&gt;

&lt;h2 id="toc_7"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Many of the &lt;em&gt;Vulkan&lt;/em&gt; concepts are familiar to me from previous projects... So to some extent, many ideas from &lt;em&gt;Vulkan&lt;/em&gt; are already built into XLE. But at the same time it&amp;#39;s going to require some refactoring and improvements of the current code before we can use &lt;em&gt;Vulkan&lt;/em&gt; very efficiently.&lt;/p&gt;

&lt;p&gt;While I&amp;#39;ve been working on &lt;em&gt;Vulkan&lt;/em&gt;, I&amp;#39;ve also been improving the &lt;code&gt;RenderCore::Metal&lt;/code&gt; interface. There are aspects of this interface that I&amp;#39;ve always meant to improve -- but I&amp;#39;ve never had the chance before. When working just with &lt;em&gt;DirectX&lt;/em&gt;, it didn&amp;#39;t really need to be very polished; but adding &lt;em&gt;Vulkan&lt;/em&gt; to the mix changes that.&lt;/p&gt;

&lt;p&gt;Anyway, it feels like &lt;em&gt;Vulkan&lt;/em&gt; is going to be around for awhile, the prototype so far has shown where the most important difficulties are going to be!&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Important Vulkan tips</title>
      <link>http://xlgames-inc.github.io/posts/vulkantips</link>
      <pubDate>April 12, 2016</pubDate>
      <description>&lt;p&gt;After 1 week of playing around with Vulkan, here are 4 tips I&amp;#39;ve learned. These are maybe not very obvious (some aren&amp;#39;t clearly documented), but I think they&amp;#39;re important to know.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;1. Use buffers as staging areas to initialize images&lt;/h2&gt;

&lt;p&gt;The crux of this is we need to use a function, &lt;em&gt;vkCmdCopyBufferToImage&lt;/em&gt;, to implement staging images with Vulkan. It&amp;#39;s almost impossible to do any graphics work with Vulkan without doing this -- but a first glance it might seem a bit counter-intuitive. First a bit of a background.&lt;/p&gt;

&lt;p&gt;Vulkan images have a &amp;quot;tiling&amp;quot; property, which can be either &amp;quot;linear&amp;quot; or &amp;quot;optimal.&amp;quot; This is property relates to how pixels are arranged in memory within a single mip level.&lt;/p&gt;

&lt;p&gt;In linear tiling, texels are arranged in row by row, column by column. So a texel&amp;#39;s linear address follows this pattern:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;address = (y*rowPitch) + x*bitsPerPixel/8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we have images on disk, we usually expect them to have this tiling.&lt;/p&gt;

&lt;p&gt;However, this has a big problem. Texels in the same row will be near each other in memory. But texels in the same column will be quite separate in memory.&lt;/p&gt;

&lt;p&gt;Given that images can be mapped arbitrarily on the final 2D triangles, we can&amp;#39;t control the order in which the GPU will access the texels. So this is big issue for the memory cache.&lt;/p&gt;

&lt;p&gt;So all GPUs are able to rearrange the texels into a &amp;quot;swizzled&amp;quot; order that tries to guarantee that nearby texels in 2D (or 3D) image space will be nearby in linear memory. This is called &amp;quot;optimal&amp;quot; tiling in Vulkan.&lt;/p&gt;

&lt;p&gt;Optimal tiling really is going to be much faster in almost all cases. Some GPUs have limited support for reading linear iamges in shaders -- but this will rarely be a good idea. The existential advantage of linear tiling is that it is easy to program the CPU code that reads and writes image memory. Also, optimal tiling isn&amp;#39;t defined as any particular pattern -- it&amp;#39;s vendor specific. I have a feeling that it might actually the same pattern for all hardware (that is, hierarchically packing into 2x2 or 2x2x2 blocks), but I don&amp;#39;t know that for sure.&lt;/p&gt;

&lt;p&gt;Because optimal tiling isn&amp;#39;t defined, we can&amp;#39;t pass the initial image data in that tiling. We must initialize the image data in linear tiling.&lt;/p&gt;

&lt;p&gt;This is why staging images are required. We initialize a linear staging image in &amp;quot;host visible&amp;quot; memory first. Then we issue a command, and the GPU will copy from the staging image into the final image, swizzling into optimal layout as it goes. It&amp;#39;s been like this for many, many years -- but to PC programmers, it may seem new because DirectX hides this behaviour.&lt;/p&gt;

&lt;p&gt;So, I said &amp;quot;staging image&amp;quot; above. But what I really meant to say was &amp;quot;staging buffer.&amp;quot; Here&amp;#39;s the problem -- even though there are &amp;quot;linear&amp;quot; tiling images that can be positioned in &amp;quot;host visible&amp;quot; memory, these can only have a single mip level and a single array layer. So how do we initialize textures with multiple mip levels and multiple array layers?&lt;/p&gt;

&lt;p&gt;It seems that we&amp;#39;re intended to use a VkBuffer for this. That&amp;#39;s the trick -- and every Vulkan programmer needs to know it :).&lt;/p&gt;

&lt;p&gt;It may seem strange to do this, but it does make sense... In Vulkan, a &lt;em&gt;VkImage&lt;/em&gt; contains functionality for driving the &amp;quot;sample&amp;quot; shader operations. All these concepts of tiling, mip levels, pixel formats, etc, are all related to shader sampling. But a staging texture will never be sampled by a shader. So, in effect, the &lt;em&gt;VkImage&lt;/em&gt; concepts are redundant. We just want an area of video memory with no special restrictions (and &lt;em&gt;VkBuffer&lt;/em&gt; fits that requirement better).&lt;/p&gt;

&lt;p&gt;It does mean we have to implement our own functions for arranging mip levels and array layers within the buffer space. In one sense, this means writing a custom implementation of &lt;em&gt;vkGetImageSubresourceLayout&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So, first we create the staging buffer, and initialize the device memory. Then we can issue the copy command with &lt;em&gt;vkCmdCopyBufferToImage&lt;/em&gt;. This allows us to copy into the image subresources from arbitrary memory within the buffer. I find the interface for this function a little awkward (and there are some limitations) but it&amp;#39;s not to bad.&lt;/p&gt;

&lt;p&gt;There&amp;#39;s a thread on nvidia&amp;#39;s site that seems to verify that this was intended: &lt;a href="https://devtalk.nvidia.com/default/topic/926085/texture-memory-management/"&gt;https://devtalk.nvidia.com/default/topic/926085/texture-memory-management/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, it&amp;#39;s an important trick. Because (as far as I can tell) this is the only way to create textures with multiple mip levels or multiple array layers in Vulkan!&lt;/p&gt;

&lt;h2 id="toc_1"&gt;2. Write custom reflection for SPIR-V bytecode&lt;/h2&gt;

&lt;p&gt;The SPIR-V bytecode is a very simple format, and it&amp;#39;s also an open standard. The bytecode also contains many of the variable and type names from the original source code (which are called decorations). These aren&amp;#39;t used during execution -- but they are useful for debugging and reflection.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve implemented a little bit of code to read SPIR-V bytecode and extract layout bindings and other useful information. This is similar to the ID3DShaderReflection interface in DirectX. But since it&amp;#39;s custom coded, it&amp;#39;s crazy efficient.&lt;/p&gt;

&lt;p&gt;I recommend checking out the following files in the Vulkan SDK for a starting point for working with SPIR-V byte code:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;glslang/SPIRV/disassemble.cpp&lt;/li&gt;
&lt;li&gt;glslang/SPIRV/doc.cpp&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="toc_2"&gt;3. Set the VK_LAYER_PATH variable!&lt;/h2&gt;

&lt;p&gt;The Vulkan SDK has a bunch of debugging &amp;quot;layers&amp;quot; built in. These are really useful!&lt;/p&gt;

&lt;p&gt;But to get them working, you need to set the VK_LAYER_PATH variable to your sdk &amp;quot;bin&amp;quot; directory (eg C:/VulkanSDK/1.0.5.0/Bin), and maybe do a bunch of other things. This may not be documented anywhere...?!&lt;/p&gt;

&lt;p&gt;When things go wrong with Vulkan, usually you&amp;#39;ll just get a program crash (that is, if the video card driver doesn&amp;#39;t crash and bluescreen). You won&amp;#39;t get much debugging information normally. To get error and warning messages, you&amp;#39;ll need the layers installed.&lt;/p&gt;

&lt;p&gt;If you read the Vulkan specs document, you&amp;#39;ll notice that all functions have a set of rules about input parameters -- written in contract style. These are the kinds of things the layers check for. But they also check for threading access and other frequent usage problems.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s really helpful, believe me! Get it working early on. Play with the &amp;quot;enable&lt;em&gt;validation&lt;/em&gt;with_callback&amp;quot; sample until it&amp;#39;s working.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;4. Download the source code for RenderDoc&lt;/h2&gt;

&lt;p&gt;Compile a debug build of RenderDoc for yourself, and run it in the debugger. You&amp;#39;ll get asserts and debugging information in those cases where your code is so screwy that even RenderDoc can&amp;#39;t handle it.&lt;/p&gt;

&lt;p&gt;Just go to renderdoc.org -- it&amp;#39;ll redirect to github!&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Vulkan prototype slowly progressing</title>
      <link>http://xlgames-inc.github.io/posts/vulkanprototypeslowprogress</link>
      <pubDate>April 08, 2016</pubDate>
      <description>&lt;p&gt;So, the Vulkan prototype is progress... But I&amp;#39;m running into many problems working with the drivers and the associated tools. Here&amp;#39;s some examples of the problems I&amp;#39;m finding.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;RenderDoc crashing&lt;/h2&gt;

&lt;p&gt;RenderDoc is the best tool for debugging Vulkan at the moment... But every time I tried to capture a log, it just crashed! The crash report didn&amp;#39;t contain any useful information. All I could do was guess at the problem.&lt;/p&gt;

&lt;p&gt;Fortunately, RenderDoc has one really great feature... It&amp;#39;s open-source! So, I just downloaded the code and ran from Visual Studio (it compiled first time).&lt;/p&gt;

&lt;p&gt;RenderDoc is still very unstable for Vulkan. But now that I have my own compiled version, that&amp;#39;s not really a big issue. I can just debug any crashes and make changes as required. All of the other GPU debugging tools I&amp;#39;ve ever used (PIX, console tools, GPA, nsight, etc) have been unstable, as well. But they were all closed source. So whenever I got an error, my only choices were to either use another debugging, or guess at the problem. With this in mind, (open-source + very unstable) is probably better than (closed-source + mostly stable).&lt;/p&gt;

&lt;p&gt;My issue was related to &amp;quot;binding&amp;quot; decorations in SPIR-V. RenderDoc requires that all resources have binding decorations. I found this was also an issue for run-time XLE code. Even though the GLSL compiler is capable of generating SPIR-V code without binding decorations, it seems like all practical uses require them.&lt;/p&gt;

&lt;p&gt;My shaders weren&amp;#39;t getting &amp;quot;bindings&amp;quot; for any resources, and this was the cause of RenderDoc&amp;#39;s crashes!&lt;/p&gt;

&lt;h2 id="toc_1"&gt;HLSL cross compiler and &amp;quot;bindings&amp;quot;&lt;/h2&gt;

&lt;p&gt;Part of the issue is related to the HLSL cross compiler. In some cases, the cross compiler can attach &amp;quot;location&amp;quot; values, but it never attaches &amp;quot;binding&amp;quot; values for textures or constant buffers.&lt;/p&gt;

&lt;p&gt;Fortunately, the HLSL cross compiler is also open-source... So I can create a fork with the modifications I need. That seems to be required in this case. I could try to attach the binding information later in the pipeline (eg, by modifying the output GLSL, or by inserting instructions into the SPIR-V bytecode)... But changing and improving the cross compiler seems like the best option.&lt;/p&gt;

&lt;p&gt;We ideally also want to specify the &amp;quot;descriptor set&amp;quot; in the GLSL code. Unfortunately, HLSL 5 doesn&amp;#39;t have an equivalent concept. That is going to require some more effort.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Cross compiler incorrect translation&lt;/h2&gt;

&lt;p&gt;The next problem was some HLSL instructions were generating incorrect GLSL code. In particular, I was using an expression like &amp;quot;float4(localPosition.xy, 0, 1)&amp;quot; to generate a 4D vector. But this was being treated as &amp;quot;float4(localPosition.xy, 0, 0)&amp;quot;.&lt;/p&gt;

&lt;p&gt;There are a number of unknown instructions that are incorrectly translated by the HLSL cross compiler... So it looks like I&amp;#39;ll need to do some work improving the code.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Vulkan validation layer&lt;/h2&gt;

&lt;p&gt;It took me awhile to figure out how to enable the Vulkan validation layer. This is really important for finding usage errors! But it&amp;#39;s not really documented and it&amp;#39;s a very unclear how to get it working.&lt;/p&gt;

&lt;p&gt;Eventually, I found out I needed to set the VK_LAYER_PATH environment variable. It seems like this should be set by the SDK installer, but maybe it was an oversight.&lt;/p&gt;

&lt;p&gt;Anyway, I also needed to use the VK_EXT_DEBUG_REPORT_EXTENSION_NAME extension to install a message handler. It looks like there are other ways to use the validation layers. But I still don&amp;#39;t know how to get them working. For now, I&amp;#39;m just catching errors and warnings and pushing them into the XLE logging system.&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Binding samplers and images together&lt;/h2&gt;

&lt;p&gt;HLSL separates samplers and textures, but Vulkan seems to prefer to combine them together into one. This is going to cause a bit of any issue with the way XLE shaders use samplers... Probably to start with, I just use a single point filtering sampler for all textures.&lt;/p&gt;

&lt;h2 id="toc_5"&gt;vkCmdBindDescriptorSets ending command buffers&lt;/h2&gt;

&lt;p&gt;For some reason, vkCmdBindDescriptorSets is silently &amp;quot;ending&amp;quot; the command buffer. It not clear why -- there must be some error. But I haven&amp;#39;t found it yet. Even the validation layers aren&amp;#39;t much help in this case.&lt;/p&gt;

&lt;h2 id="toc_6"&gt;&amp;quot;Image layouts&amp;quot; concept confusing&lt;/h2&gt;

&lt;p&gt;There seems to be some confusion in the API over a concept called &amp;quot;image layouts.&amp;quot; This appears to be related to how images are stored in memory. The exact detail are very implementation specific and opaque. We need to instruct the GPU (rather than the CPU side of the API) to change the layout of an image. So changing the layout involves appending commands to the command buffer.&lt;/p&gt;

&lt;p&gt;But there are multiple different ways to do this... It&amp;#39;s not really clear how to best handle this currently. The samples have their own way of dealing with image layouts. But that doesn&amp;#39;t look like the most optimal approach -- and anyway, it&amp;#39;s architecturally awkward (because it mixes unsynchronised initialisation functions with synchronised command buffer functions).&lt;/p&gt;

&lt;p&gt;So, I&amp;#39;ll need to do some experimentation to find the best way!&lt;/p&gt;

&lt;h2 id="toc_7"&gt;Now rendering geometry!&lt;/h2&gt;

&lt;p&gt;But, I&amp;#39;ve finally got some basic geometry rendering! It&amp;#39;s just a few 2D triangles, but it&amp;#39;s something!&lt;/p&gt;

&lt;p&gt;In short, there are a lot of problems and difficulties with using Vulkan currently.&lt;/p&gt;

&lt;p&gt;I think I&amp;#39;ve found problems with every step in the chain so far. But many of the tools and library are open-source, and that is helping a lot. If (for example) RenderDoc had been closed source, I would just be making guesses now, and probably not getting very far.&lt;/p&gt;

&lt;p&gt;It would be nice if everything was stable and polished... But for now, as long as I can identify the particular cause of each problem, I think I can make educated decisions about how to navigate through the minefield.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>HLSL shader prototype with Vulkan</title>
      <link>http://xlgames-inc.github.io/posts/vulkanshaderprototype</link>
      <pubDate>April 05, 2016</pubDate>
      <description>&lt;p&gt;As part of the Vulkan prototype, I&amp;#39;m experimenting with compiling HLSL shaders to SPIR-V.&lt;/p&gt;

&lt;p&gt;The Vulkan API doesn&amp;#39;t have a high level shader language attached. Instead, it works with a new intermediate bytecode format, called SPIR-V. This works with the LLVM compiler system, so in theory we plug in different front ends to allow various high level languages to compile to SPIR-V.&lt;/p&gt;

&lt;p&gt;That sounds great for the future... But right now, there doesn&amp;#39;t seem to be a great path for generating the SPIR-V bytecode.&lt;/p&gt;

&lt;p&gt;All of the XLE shaders are HLSL... So how do we use them with Vulkan? Let&amp;#39;s consider the options.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Preprocessor framework&lt;/h2&gt;

&lt;p&gt;One option is to convert the shader code to GLSL using a preprocessor framework. HLSL and GLSL are closely related... But they vary significantly in the way the shader interface is declared (eg, shader resources, vertex elements, built in values, etc).&lt;/p&gt;

&lt;p&gt;We can get around this by using a system of preprocessor macros and complex #includes. We would end up with native HLSL and GLSL code that does the same thing. The core shader code that does the math and lighting (etc) should be fairly compatible between languages... It&amp;#39;s just the interface that is an issue.&lt;/p&gt;

&lt;p&gt;However this kind of approach it&amp;#39;s a bit awkward, and difficult maintain in the long term. And it might mean dropping support for some of the more unusual D3D features I&amp;#39;m using (such as for dynamic linking).&lt;/p&gt;

&lt;p&gt;Also, it looks like GLSL might not be around a very long time. It could possibly go the way of OpenGL in the medium term. So it doesn&amp;#39;t make sense to invest a lot of time into GLSL, just to have to be replaced with something else later.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Cross compile&lt;/h2&gt;

&lt;p&gt;Another option is to try to convert the HLSL output to SPIR-V by way of a cross compiler.&lt;/p&gt;

&lt;p&gt;There is an interesting project here &lt;a href="https://github.com/James-Jones/HLSLCrossCompiler"&gt;https://github.com/James-Jones/HLSLCrossCompiler&lt;/a&gt;. This will take as input HLSL bytecode, and output GLSL high level shader code.&lt;/p&gt;

&lt;p&gt;HLSL bytecode is a much simpler than HLSL source (given that it&amp;#39;s mostly assembler like instructions). So this approach should be more maintainable.&lt;/p&gt;

&lt;p&gt;The issue here is there is no standard equivalent bytecode form of GLSL. The output from the cross compile is just high level GLSL code.&lt;/p&gt;

&lt;p&gt;Once we have the GLSL code, we can generate SPIR-V using the GLSL pipeline in the Vulkan SDK.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Cross compile problems&lt;/h2&gt;

&lt;p&gt;The process for compiling a shader becomes quite long:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;compile HLSL to HLSL bytecode using D3DCompiler_47.dll (ie, just like D3D11)&lt;/li&gt;
&lt;li&gt;translate HLSL bytecode to GLSL&lt;/li&gt;
&lt;li&gt;parse GLSL to AST&lt;/li&gt;
&lt;li&gt;translate GLSL AST to SPIR-V&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The big problem here is that each step adds it&amp;#39;s own restrictions. So in the end we up with the sum of all those restrictions.&lt;/p&gt;

&lt;p&gt;Another issue is that a few of these steps have know bugs and version support issues. The HLSLCrossCompiler seems to translate several instruction incorrectly. And the GLSL AST to SPIR-V seems to currently only support a subset of GLSL.&lt;/p&gt;

&lt;p&gt;For example, the [earlydepthstencil] annotations in HLSL get correctly converted into GLSL -- but then the SPIR-V translator doesn&amp;#39;t support them!&lt;/p&gt;

&lt;p&gt;Also, GLSL to SPIR-V doesn&amp;#39;t fully support GLSL version 4 shaders yet... So I&amp;#39;m using GLSL 3.3. But that may not be able to support all of the features I&amp;#39;m using in HLSL.&lt;/p&gt;

&lt;p&gt;Fortunately, both the HLSL cross compile and the GLSL to SPIR-V compiler are open-source... So there is room to make fixes and improvements if necessary. And it means those projects will be more open about what is working, and what isn&amp;#39;t.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Prototype results&lt;/h2&gt;

&lt;p&gt;It seems that the cross-compile approach is actually going to be best, despite the problems. So, I&amp;#39;ve got an early version of this process working in the &amp;quot;experimental&amp;quot; branch.&lt;/p&gt;

&lt;p&gt;In this branch, there is a new implementation of RenderCore::ShaderService::ILowLevelCompiler that does HLSL -&amp;gt; SPIR-V translation.&lt;/p&gt;

&lt;p&gt;It requires some libraries from the Vulkan SDK currently... I&amp;#39;m not sure how I will incorporate those into linking process (perhaps it will mean a new VULKAN_SDK environment variable).&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve only tested a few shaders so far.... But it works! It actually does compile, and we end up with SPIR-V at the end. Wow, pretty incredible.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve added a new &amp;quot;VULKAN=1&amp;quot; shader define. This will be probably be required for enabling and disabling certain shader features for this pipeline.&lt;/p&gt;

&lt;p&gt;The next step will be working on how to best extract reflection information from the shaders. We&amp;#39;ve actually got many ways to do that now... But I want to check to make sure we can still get the information we need out.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Starting to experiment with Vulkan</title>
      <link>http://xlgames-inc.github.io/posts/vulkan</link>
      <pubDate>April 04, 2016</pubDate>
      <description>&lt;p&gt;So, it&amp;#39;s a simple story -- boy meets API, yada, yada, yada...&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve started to build some initial experiments with the Vulkan API. Version 1.0 of the API was just released -- and there&amp;#39;s an SDK from Valve (called LunarG) around.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Initial impressions&lt;/h2&gt;

&lt;p&gt;My first impressions are very positive! Many of the design ideals and structures of the API are familiar from my days working with consoles (particularly the Sony consoles). This type of API has not been available on open platforms (like Windows) before -- so for people who don&amp;#39;t have experience with consoles, it might give an idea of what it&amp;#39;s like.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m also really happy how much of the surrounding resources have been made open-source... The samples, shader compilation tool-chain, etc... It&amp;#39;s all very effective use of github.&lt;/p&gt;

&lt;p&gt;Khronos have drawn attention to how much input they&amp;#39;ve gotten from the game engine development community. And it shows in the results. This is the kind of API that an experienced engine developer wants to see.&lt;/p&gt;

&lt;p&gt;I think it&amp;#39;s also a model that is much more viable for cross platform development than anything we&amp;#39;ve seen before. OpenGL had a lot of problems, and DirectX was so tied to the Windows platform. But this feels like something that is truly viable across many platforms (including low end and high end).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m really impressed with how a third party group has managed to build an API that balances the needs of engine developers with the needs of hardware designers. I think Khronos has really shown how this kind of thing should be done -- and it seems like a good model for other APIs (sound, physics hardware, etc).&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Long term viability&lt;/h2&gt;

&lt;p&gt;Vulkan feels like an API that could stick around for awhile. OpenGL has been on it&amp;#39;s last legs for a long time... And DirectX always needs constant refreshes to survive. But Vulkan feels like it will be here for awhile. Due to it&amp;#39;s cross-platform and long term viability, it&amp;#39;s really undermined DirectX and Apple&amp;#39;s Metal.&lt;/p&gt;

&lt;p&gt;I felt the same about C++11, when I started using it. The new C++14 is a huge step forward from the old C++98 days. Many of the design patterns we C++ programmers always wanted to use are not much more viable in C++14. Stroustrup said that C++14 &amp;quot;completes&amp;quot; C++11 -- but really, I think it &amp;quot;completes&amp;quot; C++ as a whole.&lt;/p&gt;

&lt;p&gt;C++17, C++20 -- these are going to be even better, also. And Vulkan will get better over time. But my gut feeling is that code written in C++14 and Vulkan 1.0 is likely to be viable for a very long time.&lt;/p&gt;

&lt;p&gt;By comparison, code written in OpenGL, Apple&amp;#39;s Metal (or, help us, DirectX9) is going to feel stale pretty quickly.&lt;/p&gt;

&lt;p&gt;The unfinished part of the equation is the shader language. Vulkan uses SPIR-V as an intermediate language... And that&amp;#39;s great. But it&amp;#39;s not clear yet what high level language should be used. The current SDK uses GLSL -- but maybe we could do better? Perhaps C++ is an option here... But maybe we would be better off with some next iteration forward from HLSL and GLSL.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;But it&amp;#39;s not easy!&lt;/h2&gt;

&lt;p&gt;Clearly, Vulkan is not intended as a beginner&amp;#39;s API. DirectX always tried to find a balance between power and accessibility for it&amp;#39;s API. But Vulkan is designed by and for experienced developers.&lt;/p&gt;

&lt;p&gt;This is clearest in the threading and reference counting approaches. Vulkan is mostly leaving it up to the engine developer to handle these things. That&amp;#39;s great when you want to write a really efficient engine... But it&amp;#39;s going to be a major hassle for first timers.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;XLE code&lt;/h2&gt;

&lt;p&gt;You can see the Vulkan code in the &lt;em&gt;&amp;quot;experimental&amp;quot;&lt;/em&gt; branch.&lt;/p&gt;

&lt;p&gt;I will be adapting and improving platform abstraction layer in XLE as I plug in Vulkan features. Fortunately, it seems that the XLE architecture should mostly work well with Vulkan.&lt;/p&gt;

&lt;p&gt;I will also try to use HLSL with Vulkan by cross-compiling first to GLSL and then to SPIR-V.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Comparing different methods for order independent transparency</title>
      <link>http://xlgames-inc.github.io/posts/compareoitrans</link>
      <pubDate>March 31, 2016</pubDate>
      <description>&lt;p&gt;There are has been a lot research on order independent transparency recently. There are a few screenshots comparing the following methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sorted&lt;/strong&gt; -- this mode sorts back-to-front per fragment. It&amp;#39;s very expensive, but serves as a reference.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stochastic&lt;/strong&gt; --  as per nVidia&amp;#39;s Stochastic Transparency research. This uses MSAA hardware to estimate the optical depth covering a given sample.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Depth weighted&lt;/strong&gt; -- as per nVidia&amp;#39;s other white paper, &amp;quot;A Phenomenological Scattering Model
for Order-Independent Transparency.&amp;quot; This is very cheap, and uses a fixed equation based on fragment depth to weight samples.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Sorted&lt;/strong&gt;
&lt;br/&gt;&lt;img src="/assets/media/OI/Sorted.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stochastic&lt;/strong&gt;
&lt;br/&gt;&lt;img src="/assets/media/OI/Stochastic.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Depth Weighted&lt;/strong&gt;
&lt;br/&gt;&lt;img src="/assets/media/OI/DepthWeighted.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Unordered&lt;/strong&gt;
&lt;br/&gt;&lt;img src="/assets/media/OI/Unordered.png"&gt;&lt;/p&gt;

&lt;p&gt;You can see the artifacts we get with stochastic modes clearly here. Also, you can see that the depth weighted mode is holding up well in some areas of the image, but produces weird results in other areas.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a scene more similar to what might appear in a game:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sorted&lt;/strong&gt;
&lt;br/&gt;&lt;img src="/assets/media/OI/Sorted3.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stochastic&lt;/strong&gt;
&lt;br/&gt;&lt;img src="/assets/media/OI/Stochastic3.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Depth Weighted&lt;/strong&gt;
&lt;br/&gt;&lt;img src="/assets/media/OI/DepthWeighted3.png"&gt;&lt;/p&gt;

&lt;p&gt;Here, the artifacts in the stochastic method are less obvious. Also, the volume and shape of the geometry is preserved well.&lt;/p&gt;

&lt;p&gt;The depth weighted version looks ok when there are few layers of transparency. But as soon as we pile on a few layers, it quickly turns to mush. The shape of the trees has become lost, and we end up with a trees merging into other trees.&lt;/p&gt;

&lt;p&gt;When rendering trees like this, we need to calculate the lighting on every layer (whereas, with alpha tested geometry we just calculate it once per pixel). This can become extremely expensive. One of the advantages of the stochastic method is we can estimate the optical depth in front of a layer, and fall back to cheaper lighting for layers that are heavily occluded.&lt;/p&gt;

&lt;p&gt;Likewise, in all methods there some room to move some aspects of the lighting from per-layer to per-pixel (for example, the Phenomenological Scattering Model paper does the refraction lookup once per pixel regardless of the number of layers).&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Environment Rendering Screenshots</title>
      <link>http://xlgames-inc.github.io/posts/envshots</link>
      <pubDate>March 30, 2016</pubDate>
      <description>&lt;p&gt;Here are a few screenshots of environment rendering in XLE. I don&amp;#39;t have a lot of art I can take screenshots of, so I can&amp;#39;t show a very polished scene... But you can see some of the rendering features.&lt;/p&gt;

&lt;p&gt;Look for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;shallow water surface animation&lt;/li&gt;
&lt;li&gt;order independent blending for foliage&lt;/li&gt;
&lt;li&gt;volumetric fog (&amp;amp; related effects)&lt;/li&gt;
&lt;li&gt;dynamic imposters for distant trees&lt;/li&gt;
&lt;li&gt;high res terrain geometry&lt;/li&gt;
&lt;li&gt;terrain decoration spawn&lt;/li&gt;
&lt;li&gt;&amp;quot;contact hardening&amp;quot; shadows&lt;/li&gt;
&lt;li&gt;infinite terrain shadows&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(see &lt;a href="../first/index.html"&gt;older screenshots here&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/EnvShots/shot47.jpg"&gt;
&lt;br/&gt;&lt;img src="/assets/media/EnvShots/shot50.jpg"&gt;
&lt;br/&gt;&lt;img src="/assets/media/EnvShots/shot52.jpg"&gt;
&lt;br/&gt;&lt;img src="/assets/media/EnvShots/shot55.jpg"&gt;
&lt;br/&gt;&lt;img src="/assets/media/EnvShots/shot57.jpg"&gt;
&lt;br/&gt;&lt;img src="/assets/media/EnvShots/shot58.jpg"&gt;
&lt;br/&gt;&lt;img src="/assets/media/EnvShots/shot65.jpg"&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a shot of a turbulent ocean (though the indoor environment reflections are weird!). The ocean animation is dynamically calculated on the GPU, and can simulate both small waves on a calm ocean surface and turbulant large waves.
&lt;br/&gt;&lt;img src="/assets/media/EnvShots/shot83.jpg"&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(some of the sky textures in these screenshots are from &lt;a href="http://www.hdrlabs.com/sibl/archive.html"&gt;sIBL Archive&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Release v0.04.0</title>
      <link>http://xlgames-inc.github.io/posts/release0040</link>
      <pubDate>March 26, 2016</pubDate>
      <description>&lt;p&gt;I&amp;#39;ve attached a compiled version of the XLE tools to the Github repo.&lt;/p&gt;

&lt;p&gt;This is still an early build -- and hasn&amp;#39;t be extensively tested on different hardware. So some features may not work on configurations. If you run in problems, or if you find it interesting, I recommended downloading the source and compiling for yourself.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Download&lt;/h2&gt;

&lt;p&gt;XLE v0.04.1:
&lt;a href="https://github.com/xlgames-inc/XLE/releases/download/v0.04.1/XLE0.04.1.zip"&gt;XLE v0.04.1 Windows x64&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You will need &lt;a href="https://www.microsoft.com/en-us/download/details.aspx?id=40784"&gt;Visual C++ Redistributable Packages for Visual Studio 2013&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For older versions of Windows, you will also need to install the .Net runtime.&lt;/p&gt;

&lt;p&gt;Edit -- updated to link to v0.04.1 (which fixes a problem locating the shader compiler dll on some versions of windows).&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Documentation&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve included a couple of &amp;quot;Getting Started&amp;quot; docs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href="/assets/media/v0.04.0/XLE%20Getting%20Started%20Exporting.pdf"&gt;Getting Started Exporting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/assets/media/v0.04.0/XLE%20Getting%20Started%20with%20Terrain.pdf"&gt;Getting Started with Terrain&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These will help get you through the first few steps.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Environment Sample Streamlining</title>
      <link>http://xlgames-inc.github.io/posts/environmentsample</link>
      <pubDate>March 25, 2016</pubDate>
      <description>&lt;p&gt;Along with the tools improvements, I&amp;#39;ve added a few features to make the &amp;quot;Environment&amp;quot; sample a little easier to use.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Exporting to the Environment sample&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Start a new world&lt;/li&gt;
&lt;li&gt;Find the &amp;quot;palette&amp;quot; window and drag these objects into the main viewport:

&lt;ul&gt;
&lt;li&gt;CharacterSpawn&lt;/li&gt;
&lt;li&gt;AmbientSettings&lt;/li&gt;
&lt;li&gt;DirLight
&lt;br/&gt;&lt;img src="/assets/media/EnvSample/StarterObjects.png"&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;You may want to position the character spawn and directional light using the move manipulator&lt;/li&gt;
&lt;li&gt;Save your world to some location (with &amp;quot;File/Save As&amp;quot;). The next step will create an export relative to this location.&lt;/li&gt;
&lt;li&gt;Do an &amp;quot;Export to game&amp;quot;:

&lt;ul&gt;
&lt;li&gt;in the level editor, the world is stored as large XML tree. But this isn&amp;#39;t loaded into the environment sample. Instead we have a number of small configuration files.
&lt;br/&gt;&lt;img src="/assets/media/EnvSample/Export.png"&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Now execute the environment sample:
&lt;br/&gt;&lt;img src="/assets/media/EnvSample/ExecuteEnvSample.png"&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This should pop up a separate process with the environment sample, and you will have a simple character running around.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Environment sample&lt;/h2&gt;

&lt;p&gt;The Environment sample is only the most basic of client apps for XLE. But it shows a few important concepts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;use of ISceneParser to construct a scene of objects&lt;/li&gt;
&lt;li&gt;rendering models with animated data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;High level scene management and an game objects framework are not features of XLE. XLE is intended to be integrated with other solutions that provide this functionality. So, if you are familiar with OpenSceneGraph or some other favourite game framework -- then this can be used together with the core XLE rendering and asset management functionality.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Reloading changes&lt;/h2&gt;

&lt;p&gt;Note that that Environment sample will reload assets as they change. So if you keep the window open, you can just do another &amp;quot;Export To Game,&amp;quot; and the sample will load any changes. This is great for rapid development, because you can keep the Level Editor and the sample open at the same time and they work well together.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Latest Tool Features</title>
      <link>http://xlgames-inc.github.io/posts/newtoolfeatures</link>
      <pubDate>March 14, 2016</pubDate>
      <description>&lt;p&gt;Here&amp;#39;s a rundown of some of the latest improvements to the tools. These are just a few additions and improvements made over about a week&amp;#39;s time.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Search and Replace for placements&lt;/h2&gt;

&lt;p&gt;Find placements using complex queries:
&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/SearchAndReplace.png"&gt;&lt;/p&gt;

&lt;p&gt;Once found, they can be selected from the results menu (or run a replace operation).&lt;/p&gt;

&lt;p&gt;Sometimes it&amp;#39;s useful to search for placements that use a specific model:
&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/SearchShortcut.png"&gt;&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Grouping support for placements&lt;/h2&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/Grouping.png"&gt;&lt;/p&gt;

&lt;p&gt;Group together objects to make them easier to use. Support for hierarchical groups. Use the &#x201C;Any Object (ignore groups)&#x201D; filter to ignore groups when selecting.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Randomize transforms&lt;/h2&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/RandomizeTransforms.png"&gt;
&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/RandomTransMenu.png"&gt;&lt;/p&gt;

&lt;p&gt;Give objects random scale and rotation values. Can be applied to many objects at the same time.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Prefab support&lt;/h2&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/CreatePrefabMenu.png"&gt;
&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/PrefabResource.png"&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt; Select a group of objects&lt;/li&gt;
&lt;li&gt; Save them to a separate &#x201C;.prefab&#x201D; file&lt;/li&gt;
&lt;li&gt; That prefab file can now be dragged into the world in multiple places&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For example, a house + a tree + a fence can be combined to make home.prefab. We can then add home.prefab in multiple places in the world.&lt;/p&gt;

&lt;p&gt;This is pretty cool because we can edit each instance of home.prefab separately... The system will load changes to the prefab file, but also remembers what has changed in each instance!&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Terrain editing optimization and fixes&lt;/h2&gt;

&lt;p&gt;In the terrain code, there&#x2019;s a system called &#x201C;short-circuit&#x201D;. This applies to terrain changes to the streaming assets.
This has been optimised. I also fixed an old problem where new streamed assets weren&amp;#39;t being &#x201C;short-circuited&#x201D; with the latest changes (which fixes issues when some terrain tiles show the wrong height values).&lt;/p&gt;

&lt;h2 id="toc_5"&gt;Terrain fine tune&lt;/h2&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/TerrainFineTune.png"&gt;&lt;/p&gt;

&lt;p&gt;Click to place a circle on the terrain. Now you can drag up and down, and the terrain will follow the cursor.&lt;/p&gt;

&lt;p&gt;This can be used to fine-tune small areas of the terrain (such as moving a single height value up and down).&lt;/p&gt;

&lt;h2 id="toc_6"&gt;Quick access to manipulator properties&lt;/h2&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/SelectNamedCoverageMaterial.png"&gt;&lt;/p&gt;

&lt;p&gt;There is now control that floats over the bottom right corner of the screen. This is manipulator specific, and can be used to change frequently used settings. For the Paint Coverage manipulator, it can be used to select the material to paint with.&lt;/p&gt;

&lt;p&gt;Also, the base texture materials can be given arbitrary names. This is just for convenience, so you can label a grass texture &amp;quot;grass&amp;quot; (for example).&lt;/p&gt;

&lt;h2 id="toc_7"&gt;Hidden placements&lt;/h2&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/NewToolFeatures/HiddenPlacements.png"&gt;&lt;/p&gt;

&lt;p&gt;Hidden placements now appear like a transparent shadow.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Transmitted specular</title>
      <link>http://xlgames-inc.github.io/posts/transmittedspecular3</link>
      <pubDate>February 05, 2016</pubDate>
      <description>&lt;p&gt;I&amp;#39;ve been playing with the transmitted specular implementation for IBL, and working on getting the right balance and visual impression for glass.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/GlassSphere.png"&gt;
&lt;br/&gt;&lt;em&gt;Background textures from &lt;a href="http://www.hdrlabs.com/sibl/archive.html"&gt;http://www.hdrlabs.com/sibl/archive.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It&amp;#39;s curious to think about how terms have changed over the last few years. &amp;quot;Specular transmission&amp;quot; is the term I&amp;#39;ve been using to talk about what we might have previously called &amp;quot;refraction mapping&amp;quot;, and &amp;quot;specular IBL&amp;quot; is the new term for &amp;quot;reflection mapping.&amp;quot; The new terms show how real-time methods are now encompassing ideas previously only used in ray tracers.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Basic method&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve using the basic method from the GGX paper, &lt;em&gt;&amp;quot;Microfacet Models for Refraction through Rough Surfaces&amp;quot;&lt;/em&gt; from Walter, et al. This matches our reflection implementation, of course, because the math for our reflection method is also based on that paper.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been able to adapt the math to the split-term concept used for reflected specular IBL. It works just the same way; we have one lookup-table that tell us the brightness of the refraction, and a cubemap that is pre-blurred according to the shape of our lighting equation.&lt;/p&gt;

&lt;p&gt;See the page on &lt;a href="improvedibl"&gt;Improved IBL&lt;/a&gt; for more information that that split-term stuff.&lt;/p&gt;

&lt;p&gt;The basic principles are simple. But followed blindly, the results do not quite feel right. It turns out that specular transmission is more complicated than specular reflections. I&amp;#39;ll talk about some of the complications involved in the implementation...&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Relationship between &amp;quot;specular&amp;quot; and index of refraction&lt;/h2&gt;

&lt;p&gt;It&amp;#39;s important to remember that there is a relationship between the &amp;quot;specular&amp;quot; material parameter and the index of refraction.&lt;/p&gt;

&lt;p&gt;Remember that the specular parameter is used to determine &lt;strong&gt;&amp;quot;F0&amp;quot;&lt;/strong&gt; -- which is the brightness of specular reflections in the center of a sphere. This phenomenon of reflections being brighter around the edges of a sphere is driven by the &amp;quot;fresnel&amp;quot; effect.&lt;/p&gt;

&lt;p&gt;The fresnel effect determines how light is reflected off a material, and how much gets absorbed into (or transmitted through) the material. Remember that is the ratios of the indices of refractions on either side of the boundary that determine the quantity of reflection and absorption.&lt;/p&gt;

&lt;p&gt;So, there is a 1:1 relationship between F0, &amp;quot;specular&amp;quot; and the index of refraction. These 3 values all represent the same physical property.&lt;/p&gt;

&lt;p&gt;The index of refraction important to us because it also determines the direction of refraction. And it&amp;#39;s important that this agrees with the fresnel effect calculated for specular reflection, because the refraction looks strange if it doesn&amp;#39;t merge into the reflection correctly.&lt;/p&gt;

&lt;p&gt;So, this means that the &amp;quot;specular&amp;quot; material parameter effects both the apparent brightness of the refracted image, and the quantity of distortion applied to it.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;3 parameter lookup-table&lt;/h2&gt;

&lt;p&gt;Our &amp;quot;brightness&amp;quot; lookup table now requires 3 parameters (one more than the reflection case):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;N dot V&lt;/li&gt;
&lt;li&gt;&amp;quot;roughness&amp;quot;&lt;/li&gt;
&lt;li&gt;index of refraction ratio&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Also, the lookup-table can contain values greater than 1 (because refraction can focus light).&lt;/p&gt;

&lt;p&gt;Generally it should be ok if specular transmission is less accurate than specular reflections, so I&amp;#39;ve settled on using a 64x64x32 Float16 texture. This is actually the same total size as the reflection lookup-table.&lt;/p&gt;

&lt;p&gt;It should be noted, however, that subtle effects in the brightness of the refraction seem to have a big impact on  visual believability. It&amp;#39;s important to get exactly the right&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Fixed index of refraction?&lt;/h2&gt;

&lt;p&gt;We can potentially optimize the solution by using a fixed index of refraction for all transmitting surfaces. An ideal number might be around 1.5 (which is glass). The fresnel effect for the transmission should match the reflection case, but apart from that, the quantity of distortion applied to the image doesn&amp;#39;t seem very important to visual believability. So it might be sensible to use a single fixed index of refraction for all materials, just to simplify the shader math.&lt;/p&gt;

&lt;h2 id="toc_4"&gt;GGX transmission focusing term&lt;/h2&gt;

&lt;p&gt;Most of the equations in the GGX paper can be brought across as is. However, there is one term that causes many problems.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/FocusingEqn.png"&gt;&lt;/p&gt;

&lt;p&gt;This term is responsible for dealing with the focusing of light. Where the refracted image appears shrunk, more light has been focused together, and so the result is brighter. Likewise a magnified image is darker.&lt;/p&gt;

&lt;p&gt;Walter describes the derivation of this term, and explains that is an approximation of the following equation:&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/JacobianEqn.png"&gt;&lt;/p&gt;

&lt;p&gt;Where the straight brackets represent the determinant of a Jacobian partial derivatives matrix. The omega values are angles of the outgoing ray and the half vector.&lt;/p&gt;

&lt;p&gt;The core equation is the derivative of the angle of half-vector with respect to the angle of outgoing vector. Remember that the half-vector is a direction somewhere between the incident ray and the outgoing ray. In Walter&amp;#39; equations, the incident ray is constant. So, we can think about this equation as measuring how much the outgoing ray moves as we shift the half-vector. If it moves a lot, we know the incident light is being scattered in a wide angle due to microfacet detail.&lt;/p&gt;

&lt;p&gt;There&amp;#39;s another interesting approach to this same equation in Jos Stam&amp;#39;s &amp;quot;An Illumination Model for a Skin Layer
Bounded by Rough Surfaces&amp;quot; from 2001. He mentions that he initially overlooked the importance of a term like this and found that the results looked like they were missing something. So developed his own approximation to this term.&lt;/p&gt;

&lt;p&gt;There is a problem with this term, however, because the sampling that Walter is doing is not the same as what we want to do for real-time. In our sampling of this equation, we want the outgoing ray to be constant, and the incident ray will move -- this is the opposite of Walter&amp;#39;s sampling.&lt;/p&gt;

&lt;p&gt;So, we need to rebuild this focusing term for our needs. We could use the same approach as Walter, and just flip the terms around... But I&amp;#39;ve been playing with some other approaches.&lt;/p&gt;

&lt;h2 id="toc_5"&gt;New focusing term equation&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve been experimenting with an algebraic solution to the derivative equation. Our algebraic solution isn&amp;#39;t necessarily going to be perfectly accurate, but the end result appears visually acceptable.&lt;/p&gt;

&lt;p&gt;A simple algebraic solution involves comparing &lt;strong&gt;&amp;quot;O dot H&amp;quot;&lt;/strong&gt; to &lt;strong&gt;&amp;quot;I dot H&amp;quot;&lt;/strong&gt; (ie, these are the cosines of the angles between these directions and the half-vector). These 2 dot products are related, and we can come up with a simple equation for their relationship. We can take the derivative of the angles use this as an approximation.&lt;/p&gt;

&lt;p&gt;Since &lt;strong&gt;O&lt;/strong&gt; is constant, a change to &lt;strong&gt;&amp;quot;O dot H&amp;quot;&lt;/strong&gt; represents a movement in the half-vector. So this equation tells us how much &lt;strong&gt;I&lt;/strong&gt; changes relative to &lt;strong&gt;H&lt;/strong&gt; whenever &lt;strong&gt;H&lt;/strong&gt; changes.&lt;/p&gt;

&lt;p&gt;However, that&amp;#39;s not perfect. We really want to know the change in the angle between &lt;strong&gt;I&lt;/strong&gt; and some fixed direction. The simplest solution is just to use &lt;strong&gt;O&lt;/strong&gt; as that fixed direction.&lt;/p&gt;

&lt;p&gt;So, we can adjust our equations to find the relationship between &lt;strong&gt;&amp;quot;O dot H&amp;quot;&lt;/strong&gt; to &lt;strong&gt;&amp;quot;I dot O&amp;quot;&lt;/strong&gt;. We can then find the derivative of that equation (with respect to the angles involved). This should give us an idea of how &lt;strong&gt;I&lt;/strong&gt; changes when &lt;strong&gt;H&lt;/strong&gt; changes.&lt;/p&gt;

&lt;h2 id="toc_6"&gt;Derivative equation&lt;/h2&gt;

&lt;p&gt;Unfortunately our derivative equation is very complicated. It contains many terms, and many trigonometric operations.&lt;/p&gt;

&lt;p&gt;In plain text form, it&amp;#39;s:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(a sin(2 x)-(sin(2 x) (a^2 cos(2 x)+1))/(sqrt(2) sqrt(cos^2(x) (a^2 cos(2 x)-a^2+2))))/sqrt(1-(sqrt(cos^2(x) (a^2 cos^2(x)-a^2+1))-a cos^2(x)+a)^2)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;... Ok, it&amp;#39;s a crazy-looking equation.&lt;/p&gt;

&lt;p&gt;However, we&amp;#39;re going to be mostly just using it for pre-calculating tables, so that shouldn&amp;#39;t be a problem.&lt;/p&gt;

&lt;p&gt;This equation is showing the right kinds of effects we expect it to -- we get a brightening around focused areas, and a diminishing around magnified areas.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve found that I&amp;#39;ve needed to multiply the results by &amp;quot;4&amp;quot; to get correctly balance. That part isn&amp;#39;t clear to me yet.&lt;/p&gt;

&lt;p&gt;Also, Walter explains how he is using the determinant of a Jacobian matrix of partial derivatives. However, our derivative equation has just the one parameter. I haven&amp;#39;t quite wrapped my head around how that fits in yet.&lt;/p&gt;

&lt;p&gt;That said, the results are visually satisfying. Even those there are these uncertainties, I would say that the results are good enough for our needs.&lt;/p&gt;

&lt;h2 id="toc_7"&gt;Simplified equation&lt;/h2&gt;

&lt;p&gt;We can simplify the monster of a derivative equation. By hand, I found that the following equation seems like a good approximation:&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/SimpleEqn.png"&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;strong&gt;a&lt;/strong&gt; the ratio of the indices of refraction.
This shows all of the same behaviours as the complex equation. For our purposes, the differences are negligible.&lt;/p&gt;

&lt;h2 id="toc_8"&gt;Transmission blurriness&lt;/h2&gt;

&lt;p&gt;For blurring the refracted image, we could simply build a new cubemap with new filtering intended for the transmission case. Indeed, this is possible. However, given the similarities between the filtering for transmission and the filtering for reflection (ie, they are both dominated by the &lt;em&gt;&amp;quot;D&amp;quot;&lt;/em&gt; Trowbridge-Reitz distribution factor), it would be better if we could share the same texture.&lt;/p&gt;

&lt;p&gt;An ideal solution would compare the blurring kernel for a given sample to some generically blurred texture, and pick around the right brightness from there. It&amp;#39;s almost like we need some metric that can tell us how much of this GGX-filtered blurring to apply.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been thinking of this metric as &lt;strong&gt;&amp;quot;GGX-iness&amp;quot;&lt;/strong&gt;. Well, maybe it needs a better name. But the principle should be clear -- how broad should the filtering be.&lt;/p&gt;

&lt;p&gt;In the reflection case, we&amp;#39;ve been assuming that the quantity of blurring only varies with roughness. This might be a reasonable approximation in that case (the biggest issue is that the blurring can&amp;#39;t be directionally biased).&lt;/p&gt;

&lt;p&gt;But in the refraction case, it&amp;#39;s not as accurate. The quantity of distortion to the image varies with the viewing angle. As a result, the quantity of blurring should also vary.&lt;/p&gt;

&lt;p&gt;So, our &lt;em&gt;GGX-iness = roughness&lt;/em&gt; equation needs to get more complex.&lt;/p&gt;

&lt;h2 id="toc_9"&gt;Focusing term!&lt;/h2&gt;

&lt;p&gt;As it turns out, the focusing term is the answer here, as well. Remember that the focusing term tells us the quantity of angular distortion. That&amp;#39;s exactly what we need to modify the GGX-iness equation.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve implemented a simple equation below for this calculation:
&lt;br/&gt;&lt;img src="/assets/media/TransSpec/GGXinessEqn.png"&gt;&lt;/p&gt;

&lt;p&gt;This is just based on my visual impression of what looks right (not any physical basis). It may seem strange to come this far using so much math with strong physical bases, and then finish it of with a empirical hack... But I guess sometimes the simple solutions are the best...!&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/GlassSphere2.png"&gt;&lt;/p&gt;

&lt;p&gt;Importantly, on the very edges the image become very blurred. This is more obvious on cubes. Its helps the refraction blend into the reflection better, and generally just gives a more believable result.&lt;/p&gt;

&lt;h2 id="toc_10"&gt;New Years Day in Korea&lt;/h2&gt;

&lt;p&gt;It&amp;#39;s the start of the Lunar New Year holiday in Korea. So, &#xC5EC;&#xB7EC;&#xBD84;, &#xC0C8;&#xD574; &#xBCF5; &#xB9CE;&#xC774; &#xBC1B;&#xC73C;&#xC138;&#xC694;!
Have a good weekend!&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Improved IBL</title>
      <link>http://xlgames-inc.github.io/posts/improvedibl</link>
      <pubDate>February 01, 2016</pubDate>
      <description>&lt;p&gt;I&amp;#39;ve been working on improving the accuracy of the Imaged Based Lighting (IBL) solution for XLE. This is the technology that allows us to load in a background panorama map and use it for both diffuse and specular lighting.&lt;/p&gt;

&lt;p&gt;The best way to do this is by comparing our real-time result to other renderers. So, for example, I&amp;#39;ve been experimenting with Substance Designer (more on that later). It has nVidia&amp;#39;s &amp;quot;IRay&amp;quot; raytracer built-in -- so we can compare the non-real-time results from IRay with real-time XLE. I have other ways to do this, also -- for example, the shader in ToolsHelper/BRDFSlice.sh can generate textures directly comparable with Disney&amp;#39;s BRDFExplorer tool.&lt;/p&gt;

&lt;p&gt;While doing this (and working on the specular transmission stuff), I&amp;#39;ve found some interesting improvements to the IBL solution!&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is a really long post; so I&amp;#39;ll start with the conclusion. I&amp;#39;ve made a number of improvements that make the IBL solution appear more realistic, and more similar to ray tracers.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/Spheres.png"&gt;
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/SpheresMetal.png"&gt;
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/Jamie.png"&gt;
&lt;em&gt;&amp;quot;Jamie Gnome&amp;quot; model from here: &lt;a href="http://www.turbosquid.com/3d-models/free-obj-mode-jamie-hyneman-gnome/789391"&gt;http://www.turbosquid.com/3d-models/free-obj-mode-jamie-hyneman-gnome/789391&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I found some errors in some source material I referenced and fixed some incorrect assumptions I made. Each change is small. But when added up, they make a big difference. The IBL specular is has a lot much punch, things are jumping off the screen better.&lt;/p&gt;

&lt;p&gt;Now, I&amp;#39;ll go into detail about what&amp;#39;s changed.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;IBL basis&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve been using a split-term style IBL solution, as per Brian Karis&amp;#39; Siggraph 2013 course (see &lt;a href="http://blog.selfshadow.com/publications/s2013-shading-course/"&gt;http://blog.selfshadow.com/publications/s2013-shading-course/&lt;/a&gt;). As far as I know, this is the same method used in Unreal (though I haven&amp;#39;t checked that, there may have been some changes since 2013).&lt;/p&gt;

&lt;p&gt;This splits the IBL equation into two separate equations. In one equation, we calculate the reflected light from a uniform white environment. In a sense, this is calculating our reflective a given pixel is, with equal weighting to each direction. Our solution has to make some simplifications to the BRDF (to reduce the number of variables), but we use most of it.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Microfacets!&lt;/h2&gt;

&lt;p&gt;We can think about this on a microfacet level. Remember our specular equation is an approximate simulation of the microfacet detail of a surface.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/eyeontechfig2fibre.jpg"&gt;&lt;/p&gt;

&lt;p&gt;This is a microscopic photo by &lt;em&gt;Gang Xiong, Durham University&lt;/em&gt;. It shows a smooth plastic surface.&lt;/p&gt;

&lt;p&gt;Each microscopic surface has a normal -- this is the microfacet normal.
Remember that &lt;strong&gt;&amp;quot;D&amp;quot;&lt;/strong&gt; represents how the microfacet normal are distributed, relative to the surface normal. For example, in very smooth surfaces, microfacet normals are more likely to be close to the surface normal. This is the most important part of the equation in terms of the actual shape of highlights.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;&amp;quot;G&amp;quot;&lt;/strong&gt; term is for microscopic scale shadowing between different microfacets. This is why &lt;strong&gt;G&lt;/strong&gt; is close to 1 for most of the range and drops off to zero around the edges. It &amp;quot;fights&amp;quot; the &lt;strong&gt;F&lt;/strong&gt; term, and prevents excessive halos around the edges of things.&lt;/p&gt;

&lt;p&gt;And &lt;strong&gt;&amp;quot;F&amp;quot;&lt;/strong&gt; is for the fresnel effect, which determines how much incident light is actually reflected at a given angle. In the specular equation, we&amp;#39;re calculating the fresnel effect on a microfacet level, not on the level of the surface normal.&lt;/p&gt;

&lt;h3 id="toc_3"&gt;A little bit of history&lt;/h3&gt;

&lt;p&gt;Most of this stuff actually dates back to 1967 with &lt;em&gt;Torrance &amp;amp; Sparrow&lt;/em&gt;. It evolved up to around 1981 with &lt;em&gt;Cook &amp;amp; Torrance&lt;/em&gt; (yes, same Torrance). But then nothing much happened for awhile. Then &lt;em&gt;Walter, et al.,&lt;/em&gt; rediscovered some old math in 2007 (and introduced the term, GGX). And we kind of discovered everything we were doing between 1981 and 2007 was a little bit wrong.&lt;/p&gt;

&lt;p&gt;More recently, &lt;em&gt;Brent Burley and Disney&lt;/em&gt; found a novel way to visualize multi-parameter BRDFs in a 2D image. They used this to compare our equations to real world surfaces. And so now we have a clearer idea of what&amp;#39;s really working and not working.&lt;/p&gt;

&lt;p&gt;Anyway, that&amp;#39;s why specular highlights in PS4 games feel much &amp;quot;brighter&amp;quot; than previous generations. The shape of the highlight greatly affects the impression of brightness.&lt;/p&gt;

&lt;p&gt;There&amp;#39;s a great picture here:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/ggx_vs_Phong.jpg"&gt;&lt;/p&gt;

&lt;p&gt;The &amp;quot;GGX&amp;quot; dot is not any brighter... But it appears to glow because of the shape of the falloff.&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Second split term part&lt;/h2&gt;

&lt;p&gt;Anyway, the second part of the split term equation is the environment texture itself. In XLE, this is a cubemap.&lt;/p&gt;

&lt;p&gt;Each microfacet will reflect a slightly different part of the image. And our specular equation (&lt;strong&gt;D&lt;/strong&gt;, &lt;strong&gt;G&lt;/strong&gt; &amp;amp; &lt;strong&gt;F&lt;/strong&gt;) effect the brightness of that particular reflection.&lt;/p&gt;

&lt;p&gt;However, in this approximation, we only know the average effect of all microfacets, and we can only afford to take a single sample of this cubemap. So, we want to pre-blur the cubemap to most closely approximate the results if we had sampled every microfacet separately.&lt;/p&gt;

&lt;p&gt;In XLE, we can use the cvar &lt;em&gt;&amp;quot;cv.IBLRef = true&amp;quot;&lt;/em&gt; to enable a &amp;quot;reference&amp;quot; specular equation that samples many evenly distributed microfacets. We want to try to match that.&lt;/p&gt;

&lt;h2 id="toc_5"&gt;Sampling microfacets&lt;/h2&gt;

&lt;p&gt;Since our reflection simulation is  based on the microfacet normal, in order to calculate the specular for a pixel, we want to know what microfacets there are in that pixel. But that&amp;#39;s far too difficult. We can simplify it down a little bit. What we do know is the probability for the angle between the microfacet normal and the surface normal. This is the &lt;strong&gt;&amp;quot;D&amp;quot;&lt;/strong&gt; term.&lt;/p&gt;

&lt;p&gt;For our pre-calculation steps, we sometimes need an accurate sample for a pixel. To do this, we can generate random microfacet normals, and use the &lt;strong&gt;D&lt;/strong&gt; term to weight the effect of each microfacet normal. A little more on this later...&lt;/p&gt;

&lt;h2 id="toc_6"&gt;Blurring convolution&lt;/h2&gt;

&lt;p&gt;What is the ideal convolution to use? Brian Karis suggests a simple convolution involving &lt;em&gt;N dot L&lt;/em&gt;. Previously, I was using a method suggested by another project that weights the textures by the entire specular equation. So what is the best method to use?&lt;/p&gt;

&lt;p&gt;The blurring method is actually a little similar to the reference specular equation. For every direction in the cubemap, we imagine a parallel flat surface that is reflecting that point. For roughness=0, we should get a perfectly clear reflection. As the roughness value increases, our surface should become more and more blurry. The result is actually only current for surfaces that are parallel to what they are reflecting. On the edges, the reflection should stretch out -- but this phenomenon doesn&amp;#39;t occur in our approximation. In practice, this is not a major issue.&lt;/p&gt;

&lt;p&gt;The roughness value affects the microfacet distribution and this is what leads to the blurriness. So, we can generate a random set of microfacets, weight them by &lt;strong&gt;&amp;quot;D&amp;quot;&lt;/strong&gt; and then find the average of those normals. That should cause the blurriness to vary against roughness correctly.&lt;/p&gt;

&lt;p&gt;But is there better filtering than that? What is the ideal &amp;quot;weight&amp;quot; value for each microfacet normal?&lt;/p&gt;

&lt;p&gt;Our goals for filtering should be twofold:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Try to replicate the &amp;quot;GGX&amp;quot; falloff shape. A bright pixel in the reflection should appear the same as a dynamically calculated specular highlight. The GGX falloff is also just nice, visually. So we want the falloff to match closely.&lt;/li&gt;
&lt;li&gt;Try to prevent sampling errors. This can occur when low probability samples are weighted too highly.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I played around with this for awhile and eventually decided that there is a simple, intuitive answer to this equation.&lt;/p&gt;

&lt;p&gt;In our simple case where the surface is parallel to the reflection, when we sample the final blurred cubemap, we will be sampling in the direction of the normal. We&amp;#39;re actually trying to calculate the effect of the microfacets on this reflection. So, why don&amp;#39;t we just weight the microfacet normal by the ratio of the specular for the microfacet normal to the specular for the surface normal?&lt;/p&gt;

&lt;p&gt;Since the specular for the normal is constant over all samples, that factors out. And the final weighting is just the specular equation for that microfacet normal (what I mean is, we use the sampled microfacet normal as the &amp;quot;half-vector&amp;quot;, or &lt;strong&gt;M&lt;/strong&gt;, in the full specular equation).&lt;/p&gt;

&lt;p&gt;This weighting gives us the GGX shape (because it is GGX). It also makes sense intuitively. So, in the end the best method was actually what I originally had. But now the code &amp;amp; the reasons are a little clearer.&lt;/p&gt;

&lt;h3 id="toc_7"&gt;Example&lt;/h3&gt;

&lt;p&gt;Here is an example of a filtered specular map:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/Ueno0.png"&gt;&lt;img src="/assets/media/ImprovedIBL/Ueno1.png"&gt;&lt;img src="/assets/media/ImprovedIBL/Ueno2.png"&gt;&lt;img src="/assets/media/ImprovedIBL/Ueno3.png"&gt;&lt;img src="/assets/media/ImprovedIBL/Ueno4.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Original image from sIBL archive: &lt;a href="http://www.hdrlabs.com/sibl/archive.html"&gt;http://www.hdrlabs.com/sibl/archive.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Notice that bright point still appears bright as they are filtered down. They are retaining the GGX shape. Also, when there are bright points, the light energy from those points will get spread out in lower mipmaps. So the bright points become successively wider but darker.&lt;/p&gt;

&lt;p&gt;Currently, we have a linear mapping from roughness -&amp;gt; mipmap. But as you can see, often we have more resolution that we actually need. So we could change this to a non-linear mapping.&lt;/p&gt;

&lt;h3 id="toc_8"&gt;Sampling errors&lt;/h3&gt;

&lt;p&gt;At high roughness values, we can get some sampling errors. In HDR environment textures, some pixels can be hundreds or thousands of times brighter than the average pixel. As a result, these pixels overwhelm hundreds or thousands of other pixels. Our sampling pattern attempts to cluster more samples in more areas. If a pixel like this is picked up in an &amp;quot;unimportant&amp;quot; area, where are are few samples, they can lead to artifacts.&lt;/p&gt;

&lt;p&gt;The following mip chain was generated from an image where the point in a middle of the sun is so bright that it can&amp;#39;t be represented as a 16-bit float. This causes errors, and you can see how the errors radiate outwards according to the sampling pattern.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/Spec0.png"&gt;&lt;img src="/assets/media/ImprovedIBL/Spec1.png"&gt;&lt;img src="/assets/media/ImprovedIBL/Spec2.png"&gt;&lt;img src="/assets/media/ImprovedIBL/Spec3.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Original image CC-Zero from &lt;a href="http://giantcowfilms.com/"&gt;http://giantcowfilms.com/&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This particular case can only be solved by a better float32 -&amp;gt; float16 converter than clamps values against the valid range. But it&amp;#39;s a good way to visualize how a few pixels can affect many other pixels.&lt;/p&gt;

&lt;p&gt;It may be possible to get improvements by sampling every input pixel for every output pixel. This might not be such a bad idea, so if I get a chance, I&amp;#39;ll try it.&lt;/p&gt;

&lt;p&gt;There&amp;#39;s also another possible sampling improvement related to the &lt;strong&gt;&amp;quot;G&amp;quot;&lt;/strong&gt; term. Using the microfacet normal rather than the surface normal could help reduce the effect of samples around the extreme edge of the sampling pattern. See &lt;em&gt;&amp;quot;M vs N in G&amp;quot;&lt;/em&gt; below for more details on that.&lt;/p&gt;

&lt;h2 id="toc_9"&gt;Improved microfacet sampling&lt;/h2&gt;

&lt;p&gt;When we build the set of random microfacets, we don&amp;#39;t necessarily have to distribute those microfacets evenly. We want some &amp;quot;important&amp;quot; microfacets to be more likely, and &amp;quot;unimportant&amp;quot; microfacets to be less likely.&lt;/p&gt;

&lt;p&gt;Previously, I was using the sampling pattern suggested by Karis in his course notes. However, it&amp;#39;s possible that there is an error in that equation.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve gone back to the original microfacet distribution function by Walter in &amp;quot;Microfacet Models for Refraction through Rough Surfaces&amp;quot; (the GGX paper). He built this pattern specifically for GGX, and he also designed it for both reflection and transmission. This means the view direction isn&amp;#39;t considered in the distribution (which is great for us).&lt;/p&gt;

&lt;p&gt;Karis&amp;#39; function for generating the microfacet direction actually agrees with this paper (though it is in an optimized form). However, there is a difference in the &amp;quot;Probability Density Functions&amp;quot; (or PDFs) for both:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Karis&amp;#39; PDF is &lt;em&gt;(D * NdotH) / (4.f * VdotH)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Walter&amp;#39;s PDF is &lt;em&gt;D * NdotH&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here, Karis&amp;#39; PDF seems immediately suspicious because the view direction is a factor in the PDF, but not a factor in the microfacet function itself. That seems odd. The factor of &amp;quot;4&amp;quot; is also and area of confusion, because it appears in some versions of the Cook-Torrance microfacet equations, but not others. It&amp;#39;s not clear why Karis included it, but it may be because he is using a different form of the Cook-Torrance equation to XLE?&lt;/p&gt;

&lt;p&gt;When I switched to Walter&amp;#39;s sampling equations, I found 2 immediate improvements:&lt;/p&gt;

&lt;h3 id="toc_10"&gt;Floating point errors in microfacet function&lt;/h3&gt;

&lt;p&gt;Karis&amp;#39; microfacet distribution function is not working perfectly, possibly because of floating point errors (or weird edge cases). I&amp;#39;ve replaced his equation with another equation that is mathematically identically, but I get much better results.&lt;/p&gt;

&lt;p&gt;Sometimes the shader compiler&amp;#39;s optimizations can change the mathematics to a point where is it visibly wrong. Could this be the case here?&lt;/p&gt;

&lt;p&gt;Old equation:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/MicroFacetSamplingBad.png"&gt;&lt;/p&gt;

&lt;p&gt;New equation
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/MicroFacetSamplingGood.png"&gt;&lt;/p&gt;

&lt;p&gt;They should be producing the same results, but the old version appears to be producing incorrect results for inputs within a certain range. The inputs are &amp;quot;dithered&amp;quot; according to a 4x4 pattern in this example, which is why is appears regular.&lt;/p&gt;

&lt;h3 id="toc_11"&gt;Edges are too dark&lt;/h3&gt;

&lt;p&gt;The differences in the PDF equation are actually making the edges too dark. This is related to the &lt;em&gt;VdotH&lt;/em&gt; term. It makes sense to remove &lt;em&gt;VdotH&lt;/em&gt; from the PDF for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The view direction has no impact on the microfacets generated&lt;/li&gt;
&lt;li&gt;We want to use this sampling in view independent equations&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Old version:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/MicrofacetEdgesOld.png"&gt;&lt;/p&gt;

&lt;p&gt;New version:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/MicrofacetEdgesNew.png"&gt;&lt;/p&gt;

&lt;p&gt;In the new version, the extreme edges are much brighter. The dark halo we get with the old version seems unnatural &amp;amp; incorrect.&lt;/p&gt;

&lt;h2 id="toc_12"&gt;NdotL&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve added the &lt;strong&gt;NdotL&lt;/strong&gt; term into IBL, as well. This was previously excluded but is required to match our dynamic specular equation.&lt;/p&gt;

&lt;p&gt;This is visually important -- without it, the edges become excessively bright, brighter than the thing they are reflecting. With &lt;em&gt;NdotL&lt;/em&gt;, the brightness correctly matches the reflected object.&lt;/p&gt;

&lt;p&gt;In effect, we&amp;#39;ve removed a &lt;strong&gt;VdotH&lt;/strong&gt; term, and added a &lt;strong&gt;NdotL&lt;/strong&gt;. Note that &lt;strong&gt;VdotH&lt;/strong&gt; is the same as &lt;strong&gt;LdotH&lt;/strong&gt; when the &lt;strong&gt;&amp;quot;H&amp;quot;&lt;/strong&gt; is the half-vector between &lt;strong&gt;L&lt;/strong&gt; and &lt;strong&gt;V&lt;/strong&gt;... So this might be source for the mysterious &lt;strong&gt;VdotH&lt;/strong&gt; term in Karis&amp;#39; PDF! He may have assumed that this was part of the PDF, when in fact he was just compensating for a part of his specular equation. If that&amp;#39;s the case, though, why isn&amp;#39;t it explained in his course notes?&lt;/p&gt;

&lt;h2 id="toc_13"&gt;Rebuilding the lookup table&lt;/h2&gt;

&lt;p&gt;Our split-term solution involves a texture lookup table &amp;quot;glosslut.dds.&amp;quot; This is built from all of the math on this page. Our previous version was very similar to Karis&amp;#39; version. But let&amp;#39;s rebuild it, and see what happens!&lt;/p&gt;

&lt;p&gt;Old version:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/OldR.png"&gt;&lt;img src="/assets/media/ImprovedIBL/OldG.png"&gt;&lt;/p&gt;

&lt;p&gt;New version:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/NewR.png"&gt;&lt;img src="/assets/media/ImprovedIBL/NewG.png"&gt;&lt;/p&gt;

&lt;p&gt;So, the changes are subtle, but important. The specular is general a little bit darker, except at the edges of objects where it has become significantly brighter.&lt;/p&gt;

&lt;h3 id="toc_14"&gt;Comparisons&lt;/h3&gt;

&lt;p&gt;Old look up table:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/OldGlossLUT.png"&gt;&lt;/p&gt;

&lt;p&gt;New look up table:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/NewGlossLUT.png"&gt;&lt;/p&gt;

&lt;p&gt;Reference:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/ReferenceIBL.png"&gt;&lt;/p&gt;

&lt;p&gt;This is just changing the lookup-table. You can see how the reflection is slightly darker in the new version, except that the edges are much brighter. Also, new version matches our reference image much better.&lt;/p&gt;

&lt;p&gt;At high F0 values (and along the edges) the reflection appears to be the correct brightness to match the environment.&lt;/p&gt;

&lt;h3 id="toc_15"&gt;Alpha remapping&lt;/h3&gt;

&lt;p&gt;One thing to note is that in XLE, the same alpha remapping is used for IBL that is used for dynamic specular maps. Karis suggests using different remapping for IBL, explaining that the normal remapping produces edges that are too dark. It seems like we&amp;#39;ve discovered the true cause of the dark edges and the results seem to look fine for us with our normal remapping.&lt;/p&gt;

&lt;p&gt;Also, since we&amp;#39;re using the exact same specular equation as we use for dynamic lighting, the two will also match well.&lt;/p&gt;

&lt;h2 id="toc_16"&gt;Diffuse normalization&lt;/h2&gt;

&lt;p&gt;Previously, I had been assuming our texture pipeline had was premultiplying the &lt;em&gt;&amp;quot;1/pi&amp;quot;&lt;/em&gt; diffuse normalization factor into the diffuse IBL texture. However, that turned out to be incorrect. I&amp;#39;ve compensated for now by adding this term into the shader. However, I would be ideal if we could take care of this during the texture processing step.&lt;/p&gt;

&lt;p&gt;With this change (and the changes to specular brightness), the diffuse and specular are better matched. This is particularly important for surfaces that are partially metal and partially dielectric -- because the diffuse on the dielectric part must feel matched when compared to the specular on the metal part.&lt;/p&gt;

&lt;h2 id="toc_17"&gt;Further research&lt;/h2&gt;

&lt;p&gt;Here are some more ideal for further improvements.&lt;/p&gt;

&lt;h3 id="toc_18"&gt;H vs N in G calculation&lt;/h3&gt;

&lt;p&gt;There is an interesting problem related to the use of the normal in the &amp;quot;G&amp;quot; part of the specular equation. Different sources actually feed different dot products into this equation:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Some use &lt;strong&gt;N dot L&lt;/strong&gt; and &lt;strong&gt;N dot V&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Others use &lt;strong&gt;H dot L&lt;/strong&gt; and &lt;strong&gt;H dot V&lt;/strong&gt; (where H is the half-vector or microfacet normal)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For our dynamic specular lights, we are using a single value for &lt;strong&gt;N&lt;/strong&gt; and &lt;strong&gt;H&lt;/strong&gt;, so the difference should be very subtle. But when sampling multiple microfacets, we are using many different values for &lt;strong&gt;H&lt;/strong&gt;, with a constant &lt;strong&gt;N&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For example, when using the specular equation as a weight while filtering the cubemap, &lt;strong&gt;N dot L&lt;/strong&gt; and &lt;strong&gt;N dot V&lt;/strong&gt; are constant. So they will have no effect on the filtering.&lt;/p&gt;

&lt;p&gt;Remember that &lt;strong&gt;&amp;quot;G&amp;quot;&lt;/strong&gt; is the shadowing term. It drops off to zero around the extreme edges. So by using &lt;strong&gt;H dot L&lt;/strong&gt; and &lt;strong&gt;H dot V&lt;/strong&gt;, we could help reduce the effect of extreme samples.&lt;/p&gt;

&lt;p&gt;Also, when building the split-term lookup table, &lt;strong&gt;N dot V&lt;/strong&gt; is constant. So it has very little effect. It would be interesting experimenting with the other form in these cases.&lt;/p&gt;

&lt;p&gt;On the other hand, in the dynamic specular equation &lt;strong&gt;H dot L&lt;/strong&gt; and &lt;strong&gt;H dot V&lt;/strong&gt; should be equal and very rarely low numbers. So, in that case, it may not make sense to use the half-vector.&lt;/p&gt;

&lt;h3 id="toc_19"&gt;Problems when NdotV is &amp;lt; .15f&lt;/h3&gt;

&lt;p&gt;Rough objects current tend to have a few pixels around the edges that have no reflection. It&amp;#39;s isn&amp;#39;t really visible in the dielectric case. But with metals (where there is no diffuse) it shows up as black and looks very strange.&lt;/p&gt;

&lt;p&gt;The problem is related to how we&amp;#39;re sampling the microfacet normals. As you can imagine, when normals are near perpendicular to view direction, sometimes the microfacet normal will end up pointing away from the view direction. When this happens, we can reject the sample (or calculate some value close to black). When the number of samples rejected is too high, we will end up with a result that is not accurate.&lt;/p&gt;

&lt;p&gt;This tends to only affect a few pixels, but it&amp;#39;s enough to pop out. We need some better solution around these edge parts.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Transmitted specular progress</title>
      <link>http://xlgames-inc.github.io/posts/transmittedspecular2</link>
      <pubDate>January 29, 2016</pubDate>
      <description>&lt;p&gt;Just a quick update... I&amp;#39;ve been making some great progress with transmitted specular for IBL!&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/TransSphereMat.png"&gt;&lt;/p&gt;

&lt;p&gt;These screenshots will look a little strange (I mean that black borders &amp;amp; grainyness), because it&amp;#39;s a debugging rendering mode.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/TransSphereMat2.png"&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve got the split-term stuff going; and it looks like it should be practical. I&amp;#39;d prefer to avoid having yet another cubemap, so maybe there&amp;#39;s some way to just reuse the reflection filtered cubemap. Its seems reasonable to say that the filtering should be similar. We just need some way to calculate the amount of blurriness that is correct for transmissions.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/TransSphereMat3.png"&gt;&lt;/p&gt;

&lt;p&gt;Speaking of that, I&amp;#39;ve been thinking the type of filtering that is applied to the specular map. More on that later.&lt;/p&gt;

&lt;p&gt;Next week I&amp;#39;ll post some proper screenshots &amp;amp; a whole lot of details!&lt;/p&gt;

&lt;p&gt;I made some interesting observations while doing this, I&amp;#39;ve got a bunch of new changes and improvements for IBL. Check out the &amp;quot;experimental&amp;quot; branch for now.&lt;/p&gt;

&lt;p&gt;BTW, the node diagram for the GGX BSDF equation looks a little nicer now --&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/BSDFNodes2.png"&gt;&lt;/p&gt;

&lt;p&gt;(See the older version here:&lt;a href="transmissionnodegraph"&gt;Transmission Node Diagram&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;As you can see in the diagram, the equations are causing a lot of light to get focused in around the edges. This seems to be exaggerated when roughness is very high. That might require a little more investigation next week.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Processing textures with the XLE scriptable asset path</title>
      <link>http://xlgames-inc.github.io/posts/assetpathscripts</link>
      <pubDate>January 26, 2016</pubDate>
      <description>&lt;p&gt;I&amp;#39;ve included a few Gradle scripts in the XLE distribution. This is mostly just a simple set of tools I use for my own testing. But you may find it useful for your own needs.&lt;/p&gt;

&lt;p&gt;Of course this system isn&amp;#39;t designed to be 100% robust and fool proof. Actually, it&amp;#39;s just a few simple scripts. But it is scalable and flexible.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Gradle&lt;/h2&gt;

&lt;p&gt;At heart, an &lt;strong&gt;&amp;quot;asset path&amp;quot;&lt;/strong&gt; is some system that can identify changed assets, recognize processing working that needs to be performed with those assets, and schedule that work.&lt;/p&gt;

&lt;p&gt;For example, we might have an asset for a sky background texture. Various processing tools need to pre-filter this texture so it can be used for image based lighting. Whenever the texture changes (or when the processing tools change) we want to execute the processing steps and produce intermediate assets.&lt;/p&gt;

&lt;p&gt;XLE does some processing at runtime. But that is only practical for short processing steps. Expensive processing steps need some other solution.&lt;/p&gt;

&lt;p&gt;So we need some build system to manage assets and dependences. Are requirements are similar to build systems we use for code. But most code-oriented build systems don&amp;#39;t work well for assets.&lt;/p&gt;

&lt;p&gt;I picked &lt;em&gt;Gradle&lt;/em&gt; because of it&amp;#39;s procedural nature. It allows us to specify input assets -- but also to give instructions to the build path on how to handle that object. For example, we need to tell the build path if a texture is a sky texture, a normals texture, or some other type... That kind of thing just falls out of &lt;em&gt;Gradle&lt;/em&gt; very easily.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Executing Gradle&lt;/h2&gt;

&lt;p&gt;First, you need to install Gradle, from: &lt;a href="http://gradle.org/gradle-download/"&gt;http://gradle.org/gradle-download/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In Tools/AssetPath, we have the &amp;quot;settings.gradle&amp;quot; root project file. Also, this folder contains some &amp;quot;groovy&amp;quot; source files that contain gradle task type implementations. So, there is a task type for processing sky textures -- which basically just involves executing a number of command line operations.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve also defined a root task called &lt;strong&gt;&amp;quot;tex&amp;quot;&lt;/strong&gt;. This is where gradle starts to shine. The &amp;quot;tex&amp;quot; task searches through the entire working folder for build.gradle files that contain tasks whose names being with &lt;strong&gt;&amp;quot;tex...&amp;quot;&lt;/strong&gt;. All of the tasks found become subtasks of the root &amp;quot;tex&amp;quot; task.&lt;/p&gt;

&lt;p&gt;So, we can create a &lt;strong&gt;&amp;quot;build.gradle&amp;quot;&lt;/strong&gt; in any folder in the working directory. And if we add tasks to that file that being with &amp;quot;tex&amp;quot; (typically &amp;quot;tex0&amp;quot;, &amp;quot;tex1&amp;quot;, etc), they will be automatically added as subtasks of the root &amp;quot;tex&amp;quot; task.&lt;/p&gt;

&lt;p&gt;This is important because when we execute gradle (using the &lt;em&gt;&amp;quot;Tools/AssetPath/execute.bat&amp;quot;&lt;/em&gt; batch file) and pass the root task &amp;quot;tex&amp;quot; on the command line, this has the effect of execute all &amp;quot;tex...&amp;quot; tasks in the entire working directory.&lt;/p&gt;

&lt;p&gt;Pretty cool, right? We can also execute individual tasks using the normal gradle command line.&lt;/p&gt;

&lt;p&gt;There is an example in git in &lt;em&gt;&amp;quot;Working/Game/xleres/DefaultResources/build.gradle.&amp;quot;&lt;/em&gt; This project will generate the standard lookup tables.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Processing Textures&lt;/h2&gt;

&lt;p&gt;Executing Gradle is just the first step. Gradle is just going to chain together command line operations. Those operations require a bunch of tools.&lt;/p&gt;

&lt;p&gt;One of the most useful task types is &amp;quot;xle.EquiRectEnv.&amp;quot; This takes in a equirectangular (ie, paranoramic) environment map and produces 3 important textures: Cubemap background, Specular IBL texture and Diffuse IBL texture.&lt;/p&gt;

&lt;p&gt;This get a little complicated. This is just the pipeline I use for processing these textures myself. So it&amp;#39;s a little complicated and involved right now.&lt;/p&gt;

&lt;p&gt;First, install these:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;AMD CubemapGen&lt;/em&gt; (&lt;a href="http://developer.amd.com/tools-and-sdks/archive/legacy-cpu-gpu-tools/cubemapgen/"&gt;http://developer.amd.com/tools-and-sdks/archive/legacy-cpu-gpu-tools/cubemapgen/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;S&#xE9;bastien Lagarde&amp;#39;s modified CubemapGen&lt;/em&gt;: &lt;a href="https://seblagarde.wordpress.com/2012/06/10/amd-cubemapgen-for-physically-based-rendering/"&gt;https://seblagarde.wordpress.com/2012/06/10/amd-cubemapgen-for-physically-based-rendering/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;nvcompress&lt;/em&gt; ** from nvidia-texture-tools (see below)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;TextureProcess&lt;/em&gt; sample from XLE&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All of these must in the system environment variable &lt;strong&gt;path&lt;/strong&gt;. In the case of TextureProcess, don&amp;#39;t move the executable from the Finals_** folders (because it needs to find the working directory for shaders).&lt;/p&gt;

&lt;p&gt;I use a slightly slightly modified version of nvidia-texture-tools that works better with HDR textures. The standard nvidia-texture-tools always tonemaps HDR textures on load-in (frustratingly). You can find my modified version here: &lt;a href="https://github.com/djewsbury/nvidia-texture-tools"&gt;https://github.com/djewsbury/nvidia-texture-tools&lt;/a&gt;. This modified version allows us to read HDR files (from .hdr and other formats) and write unprocessed floating point .dds file.&lt;/p&gt;

&lt;p&gt;S&#xE9;bastien Lagarde&amp;#39;s &lt;strong&gt;ModifiedCubeMapGen&lt;/strong&gt; is used to generate the diffuse IBL texture. This goes via a spherical harmonic representation before arriving at a small cubemap.&lt;/p&gt;

&lt;p&gt;XLE&amp;#39;s &lt;strong&gt;TextureProcess.exe&lt;/strong&gt; is used to generate the specular IBL texture, as well as compressing to BC6. The specular IBL process reads from a equirectangular HDR map and writes a cubemap.&lt;/p&gt;

&lt;p&gt;All of these textures need to be compress to BC6. Awkwardly, nvidia-texture-tools doesn&amp;#39;t support compressing HDR data to BC6 properly, but &lt;strong&gt;TextureProcess.exe&lt;/strong&gt; can do that via the &lt;strong&gt;DirectXTex&lt;/strong&gt; library.&lt;/p&gt;

&lt;p&gt;So, once you&amp;#39;ve got all of that, compiled it all, put in all in the path... Then you&amp;#39;re ready to process textures.&lt;/p&gt;

&lt;p&gt;Intermediate outputs from this process get written into the &amp;quot;int/u&amp;quot; (or intermediate/universal) directory. From there, you can copy it out, or do what you like. You should get a &lt;em&gt;(file).dds&lt;/em&gt;, &lt;em&gt;(file)_diffuse.dds&lt;/em&gt; &amp;amp; &lt;em&gt;(file)_specular.dds&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Make it your own!&lt;/h2&gt;

&lt;p&gt;It&amp;#39;s difficult to get this working the first time. There&amp;#39;s a lot of work. But once it&amp;#39;s going, all of the parts are extensible and exchangeable. It&amp;#39;s a simple, but very flexible system. Handy for managing small, everyday processing tasks.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Transmitted specular</title>
      <link>http://xlgames-inc.github.io/posts/transmittedspecular</link>
      <pubDate>January 25, 2016</pubDate>
      <description>&lt;p&gt;I&amp;#39;ve modified the lighting equations to allow for transmitted (as well as reflected) specular for dynamic lightings. Transmitted specular means the light is on the opposite side of the object, but light is coming through the object towards the viewer.&lt;/p&gt;

&lt;p&gt;This is important for thin materials (such as leaves)&lt;/p&gt;

&lt;h3 id="toc_0"&gt;No transmitted specular:&lt;/h3&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/LeavesNoTransSpec.png"&gt;&lt;/p&gt;

&lt;h3 id="toc_1"&gt;With transmitted specular:&lt;/h3&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/LeavesTransSpec.png"&gt;&lt;/p&gt;

&lt;p&gt;Here the amount of transmission is maybe slightly too high -- but it shows the effect well.
This works particularly well with shadowed directional lights and geometry using order independent transparency. The above screenshot is exactly that situation. The tree leaves are rendered with stochastic transparency (which works particularly well for this model). So we get a nice halo effect around the edges of the tree.&lt;/p&gt;

&lt;p&gt;And, if you look closely, you can see that the tree shadowing is blocking the transmitted specular, giving a volumetric look.&lt;/p&gt;

&lt;p&gt;This model is built in a way to create a lot of noise in the normals. As you can see here:
&lt;br/&gt;&lt;img src="/assets/media/TransSpec/LeavesNormals.png"&gt;&lt;/p&gt;

&lt;p&gt;Normals point in every direction, and there is a lot of local variation.&lt;/p&gt;

&lt;p&gt;The transmitted specular really helps highlight this noise. It helps to make the tree canopy feel more 3D, and it helps to hide the &amp;quot;true&amp;quot; triangular geometry. The preview reflected-only specular just wasn&amp;#39;t able to do this as well.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;It&amp;#39;s expensive!&lt;/h2&gt;

&lt;p&gt;The transmitted specular calculation is actually a second, additional, specular calculation -- and it&amp;#39;s a little more expensive than the reflected specular. It&amp;#39;s using the node diagram from the previous post, &lt;a href="transmissionnodegraph"&gt;Transmission Node Diagram&lt;/a&gt;. So it&amp;#39;s not really cheap. It&amp;#39;s particularly expensive in this case, because stochastic transparency can result in many lighting calculations per pixel (for the different depth layers).&lt;/p&gt;

&lt;h2 id="toc_3"&gt;IBL implementation&lt;/h2&gt;

&lt;p&gt;This is not currently working with image based lighting. Part of the &lt;strong&gt;IBL&lt;/strong&gt; principle is that every effect applied to &amp;quot;dynamic&amp;quot; lighting should also be applied to &lt;strong&gt;IBL&lt;/strong&gt; -- (so material appear the same under both types of lights, and so we can fade distant dynamic lights into precalculated IBL).&lt;/p&gt;

&lt;p&gt;It might be interesting to try to get it working under IBL, so I&amp;#39;ll give it a shot.&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Better doubled sided GGX specular equation&lt;/h2&gt;

&lt;p&gt;When the shader flag &lt;em&gt;&amp;quot;MAT&lt;em&gt;DOUBLE&lt;/em&gt;SIDED_LIGHTING&amp;quot;&lt;/em&gt; is set, the specular equation is now double sided. The results will be symmetrical for a flipped normal (ie, &lt;em&gt;Specular(normal)&lt;/em&gt;==&lt;em&gt;Specular(-normal)&lt;/em&gt;, for all normals). It can be handy in cases like this, where the normals are pointing in all directions.&lt;/p&gt;
</description>
    </item>
  </channel>
</rss>
