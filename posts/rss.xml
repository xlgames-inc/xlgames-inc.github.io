<?xml version="1.0"?>
<rss version="2.0">
  <channel>
    <title>XLE development blog</title>
    <description>XLE development blog</description>
    <link>http://xlgames-inc.github.io</link>
    <pubDate>February 01, 2016</pubDate>
    <item>
      <title>Improved IBL</title>
      <link>http://xlgames-inc.github.io/posts/improvedibl</link>
      <pubDate>February 01, 2016</pubDate>
      <description>&lt;p&gt;I&amp;#39;ve been working on improving the accuracy of the Imaged Based Lighting (IBL) solution for XLE. This is the technology that allows us to load in a background panorama map and use it for both diffuse and specular lighting.&lt;/p&gt;

&lt;p&gt;The best way to do this is by comparing our real-time result to other renderers. So, for example, I&amp;#39;ve been experimenting with Substance Designer (more on that later). It has nVidia&amp;#39;s &amp;quot;IRay&amp;quot; raytracer built-in -- so we can compare the non-real-time results from IRay with real-time XLE. I have other ways to do this, also -- for example, the shader in ToolsHelper/BRDFSlice.sh can generate textures directly comparable with Disney&amp;#39;s BRDFExplorer tool.&lt;/p&gt;

&lt;p&gt;While doing this (and working on the specular transmission stuff), I&amp;#39;ve found some interesting improvements to the IBL solution!&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is a really long post; so I&amp;#39;ll start with the conclusion. I&amp;#39;ve made a number of improvements that make the IBL solution appear more realistic, and more similar to ray tracers.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/Spheres.png"&gt;
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/SpheresMetal.png"&gt;&lt;/p&gt;

&lt;p&gt;I found some errors in some source material I referenced and fixed some incorrect assumptions I made. Each change is small. But when added up, they make a big difference. The IBL specular is has a lot much punch, things are jumping off the screen better.&lt;/p&gt;

&lt;p&gt;Now, I&amp;#39;ll go into detail about what&amp;#39;s changed.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;IBL basis&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve been using a split-term style IBL solution, as per Brian Karis&amp;#39; Siggraph 2013 course (see &lt;a href="http://blog.selfshadow.com/publications/s2013-shading-course/"&gt;http://blog.selfshadow.com/publications/s2013-shading-course/&lt;/a&gt;). As far as I know, this is the same method used in Unreal (though I haven&amp;#39;t checked that, there may have been some changes since 2013).&lt;/p&gt;

&lt;p&gt;This splits the IBL equation into two separate equations. In one equation, we calculate the reflected light from a uniform white environment. In a sense, this is calculating our reflective a given pixel is, with equal weighting to each direction. Our solution has to make some simplifications to the BRDF (to reduce the number of variables), but we use most of it.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Microfacets!&lt;/h2&gt;

&lt;p&gt;We can think about this on a microfacet level. Remember our specular equation is an approximate simulation of the microfacet detail of a surface.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/eyeontechfig2fibre.jpg"&gt;&lt;/p&gt;

&lt;p&gt;This is a microscopic photo by &lt;em&gt;Gang Xiong, Durham University&lt;/em&gt;. It shows a smooth plastic surface.&lt;/p&gt;

&lt;p&gt;Each microscopic surface has a normal -- this is the microfacet normal.
Remember that &lt;strong&gt;&amp;quot;D&amp;quot;&lt;/strong&gt; represents how the microfacet normal are distributed, relative to the surface normal. For example, in very smooth surfaces, microfacet normals are more likely to be close to the surface normal. This is the most important part of the equation in terms of the actual shape of highlights.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;&amp;quot;G&amp;quot;&lt;/strong&gt; term is for microscopic scale shadowing between different microfacets. This is why &lt;strong&gt;G&lt;/strong&gt; is close to 1 for most of the range and drops off to zero around the edges. It &amp;quot;fights&amp;quot; the &lt;strong&gt;F&lt;/strong&gt; term, and prevents excessive halos around the edges of things.&lt;/p&gt;

&lt;p&gt;And &lt;strong&gt;&amp;quot;F&amp;quot;&lt;/strong&gt; is for the fresnel effect, which determines how much incident light is actually reflected at a given angle. In the specular equation, we&amp;#39;re calculating the fresnel effect on a microfacet level, not on the level of the surface normal.&lt;/p&gt;

&lt;h3 id="toc_3"&gt;A little bit of history&lt;/h3&gt;

&lt;p&gt;Most of this stuff actually dates back to 1967 with &lt;em&gt;Torrance &amp;amp; Sparrow&lt;/em&gt;. It evolved up to around 1981 with &lt;em&gt;Cook &amp;amp; Torrance&lt;/em&gt; (yes, same Torrance). But then nothing much happened for awhile. Then &lt;em&gt;Walter, et al.,&lt;/em&gt; rediscovered some old math in 2007 (and introduced the term, GGX). And we kind of discovered everything we were doing between 1981 and 2007 was a little bit wrong.&lt;/p&gt;

&lt;p&gt;More recently, &lt;em&gt;Brent Burley and Disney&lt;/em&gt; found a novel way to visualize multi-parameter BRDFs in a 2D image. They used this to compare our equations to real world surfaces. And so now we have a clearer idea of what&amp;#39;s really working and not working.&lt;/p&gt;

&lt;p&gt;Anyway, that&amp;#39;s why specular highlights in PS4 games feel much &amp;quot;brighter&amp;quot; than previous generations. The shape of the highlight greatly affects the impression of brightness.&lt;/p&gt;

&lt;p&gt;There&amp;#39;s a great picture here:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/ggx_vs_Phong.jpg"&gt;&lt;/p&gt;

&lt;p&gt;The &amp;quot;GGX&amp;quot; dot is not any brighter... But it appears to glow because of the shape of the falloff.&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Second split term part&lt;/h2&gt;

&lt;p&gt;Anyway, the second part of the split term equation is the environment texture itself. In XLE, this is a cubemap.&lt;/p&gt;

&lt;p&gt;Each microfacet will reflect a slightly different part of the image. And our specular equation (&lt;strong&gt;D&lt;/strong&gt;, &lt;strong&gt;G&lt;/strong&gt; &amp;amp; &lt;strong&gt;F&lt;/strong&gt;) effect the brightness of that particular reflection.&lt;/p&gt;

&lt;p&gt;However, in this approximation, we only know the average effect of all microfacets, and we can only afford to take a single sample of this cubemap. So, we want to pre-blur the cubemap to most closely approximate the results if we had sampled every microfacet separately.&lt;/p&gt;

&lt;p&gt;In XLE, we can use the cvar &lt;em&gt;&amp;quot;cv.IBLRef = true&amp;quot;&lt;/em&gt; to enable a &amp;quot;reference&amp;quot; specular equation that samples many evenly distributed microfacets. We want to try to match that.&lt;/p&gt;

&lt;h2 id="toc_5"&gt;Sampling microfacets&lt;/h2&gt;

&lt;p&gt;Since our reflection simulation is  based on the microfacet normal, in order to calculate the specular for a pixel, we want to know what microfacets there are in that pixel. But that&amp;#39;s far too difficult. We can simplify it down a little bit. What we do know is the probability for the angle between the microfacet normal and the surface normal. This is the &lt;strong&gt;&amp;quot;D&amp;quot;&lt;/strong&gt; term.&lt;/p&gt;

&lt;p&gt;For our pre-calculation steps, we sometimes need an accurate sample for a pixel. To do this, we can generate random microfacet normals, and use the &lt;strong&gt;D&lt;/strong&gt; term to weight the effect of each microfacet normal. A little more on this later...&lt;/p&gt;

&lt;h2 id="toc_6"&gt;Blurring convolution&lt;/h2&gt;

&lt;p&gt;What is the ideal convolution to use? Brian Karis suggests a simple convolution involving &lt;em&gt;N dot L&lt;/em&gt;. Previously, I was using a method suggested by another project that weights the textures by the entire specular equation. So what is the best method to use?&lt;/p&gt;

&lt;p&gt;The blurring method is actually a little similar to the reference specular equation. For every direction in the cubemap, we imagine a parallel flat surface that is reflecting that point. For roughness=0, we should get a perfectly clear reflection. As the roughness value increases, our surface should become more and more blurry. The result is actually only current for surfaces that are parallel to what they are reflecting. On the edges, the reflection should stretch out -- but this phenomenon doesn&amp;#39;t occur in our approximation. In practice, this is not a major issue.&lt;/p&gt;

&lt;p&gt;The roughness value affects the microfacet distribution and this is what leads to the blurriness. So, we can generate a random set of microfacets, weight them by &lt;strong&gt;&amp;quot;D&amp;quot;&lt;/strong&gt; and then find the average of those normals. That should cause the blurriness to vary against roughness correctly.&lt;/p&gt;

&lt;p&gt;But is there better filtering than that? What is the ideal &amp;quot;weight&amp;quot; value for each microfacet normal?&lt;/p&gt;

&lt;p&gt;Our goals for filtering should be twofold:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Try to replicate the &amp;quot;GGX&amp;quot; falloff shape. A bright pixel in the reflection should appear the same as a dynamically calculated specular highlight. The GGX falloff is also just nice, visually. So we want the falloff to match closely.&lt;/li&gt;
&lt;li&gt;Try to prevent sampling errors. This can occur when low probability samples are weighted too highly.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I played around with this for awhile and eventually decided that there is a simple, intuitive answer to this equation.&lt;/p&gt;

&lt;p&gt;In our simple case where the surface is parallel to the reflection, when we sample the final blurred cubemap, we will be sampling in the direction of the normal. We&amp;#39;re actually trying to calculate the effect of the microfacets on this reflection. So, why don&amp;#39;t we just weight the microfacet normal by the ratio of the specular for the microfacet normal to the specular for the surface normal?&lt;/p&gt;

&lt;p&gt;Since the specular for the normal is constant over all samples, that factors out. And the final weighting is just the specular equation for that microfacet normal (what I mean is, we use the sampled microfacet normal as the &amp;quot;half-vector&amp;quot;, or &lt;strong&gt;M&lt;/strong&gt;, in the full specular equation).&lt;/p&gt;

&lt;p&gt;This weighting gives us the GGX shape (because it is GGX). It also makes sense intuitively. So, in the end the best method was actually what I originally had. But now the code &amp;amp; the reasons are a little clearer.&lt;/p&gt;

&lt;h3 id="toc_7"&gt;Example&lt;/h3&gt;

&lt;p&gt;Here is an example of a filtered specular map:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/ueno0.png"&gt;&lt;img src="/assets/media/ImprovedIBL/ueno1.png"&gt;&lt;img src="/assets/media/ImprovedIBL/ueno2.png"&gt;&lt;img src="/assets/media/ImprovedIBL/ueno3.png"&gt;&lt;img src="/assets/media/ImprovedIBL/ueno4.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Original image from sIBL archive: &lt;a href="http://www.hdrlabs.com/sibl/archive.html"&gt;http://www.hdrlabs.com/sibl/archive.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Notice that bright point still appears bright as they are filtered down. They are retaining the GGX shape. Also, when there are bright points, the light energy from those points will get spread out in lower mipmaps. So the bright points become successively wider but darker.&lt;/p&gt;

&lt;p&gt;Currently, we have a linear mapping from roughness -&amp;gt; mipmap. But as you can see, often we have more resolution that we actually need. So we could change this to a non-linear mapping.&lt;/p&gt;

&lt;h3 id="toc_8"&gt;Sampling errors&lt;/h3&gt;

&lt;p&gt;At high roughness values, we can get some sampling errors. In HDR environment textures, some pixels can be hundreds or thousands of times brighter than the average pixel. As a result, these pixels overwhelm hundreds or thousands of other pixels. Our sampling pattern attempts to cluster more samples in more areas. If a pixel like this is picked up in an &amp;quot;unimportant&amp;quot; area, where are are few samples, they can lead to artifacts.&lt;/p&gt;

&lt;p&gt;The following mip chain was generated from an image where the point in a middle of the sun is so bright that it can&amp;#39;t be represented as a 16-bit float. This causes errors, and you can see how the errors radiate outwards according to the sampling pattern.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/spec0.png"&gt;&lt;img src="/assets/media/ImprovedIBL/spec1.png"&gt;&lt;img src="/assets/media/ImprovedIBL/spec2.png"&gt;&lt;img src="/assets/media/ImprovedIBL/spec3.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Original image CC-Zero from &lt;a href="http://giantcowfilms.com/"&gt;http://giantcowfilms.com/&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This particular case can only be solved by a better float32 -&amp;gt; float16 converter than clamps values against the valid range. But it&amp;#39;s a good way to visualize how a few pixels can affect many other pixels.&lt;/p&gt;

&lt;p&gt;It may be possible to get improvements by sampling every input pixel for every output pixel. This might not be such a bad idea, so if I get a chance, I&amp;#39;ll try it.&lt;/p&gt;

&lt;p&gt;There&amp;#39;s also another possible sampling improvement related to the &lt;strong&gt;&amp;quot;G&amp;quot;&lt;/strong&gt; term. Using the microfacet normal rather than the surface normal could help reduce the effect of samples around the extreme edge of the sampling pattern. See &lt;em&gt;&amp;quot;M vs N in G&amp;quot;&lt;/em&gt; below for more details on that.&lt;/p&gt;

&lt;h2 id="toc_9"&gt;Improved microfacet sampling&lt;/h2&gt;

&lt;p&gt;When we build the set of random microfacets, we don&amp;#39;t necessarily have to distribute those microfacets evenly. We want some &amp;quot;important&amp;quot; microfacets to be more likely, and &amp;quot;unimportant&amp;quot; microfacets to be less likely.&lt;/p&gt;

&lt;p&gt;Previously, I was using the sampling pattern suggested by Karis in his course notes. However, it&amp;#39;s possible that there is an error in that equation.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve gone back to the original microfacet distribution function by Walter in &amp;quot;Microfacet Models for Refraction through Rough Surfaces&amp;quot; (the GGX paper). He built this pattern specifically for GGX, and he also designed it for both reflection and transmission. This means the view direction isn&amp;#39;t considered in the distribution (which is great for us).&lt;/p&gt;

&lt;p&gt;Karis&amp;#39; function for generating the microfacet direction actually agrees with this paper (though it is in an optimized form). However, there is a difference in the &amp;quot;Probability Density Functions&amp;quot; (or PDFs) for both:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Karis&amp;#39; PDF is &lt;em&gt;(D * NdotH) / (4.f * VdotH)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Walter&amp;#39;s PDF is &lt;em&gt;D * NdotH&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here, Karis&amp;#39; PDF seems immediately suspicious because the view direction is a factor in the PDF, but not a factor in the microfacet function itself. That seems odd. The factor of &amp;quot;4&amp;quot; is also and area of confusion, because it appears in some versions of the Cook-Torrance microfacet equations, but not others. It&amp;#39;s not clear why Karis included it, but it may be because he is using a different form of the Cook-Torrance equation to XLE?&lt;/p&gt;

&lt;p&gt;When I switched to Walter&amp;#39;s sampling equations, I found 2 immediate improvements:&lt;/p&gt;

&lt;h3 id="toc_10"&gt;Floating point errors in microfacet function&lt;/h3&gt;

&lt;p&gt;Karis&amp;#39; microfacet distribution function is not working perfectly, possibly because of floating point errors (or weird edge cases). I&amp;#39;ve replaced his equation with another equation that is mathematically identically, but I get much better results.&lt;/p&gt;

&lt;p&gt;Sometimes the shader compiler&amp;#39;s optimizations can change the mathematics to a point where is it visibly wrong. Could this be the case here?&lt;/p&gt;

&lt;p&gt;Old equation:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/MicroFacetSamplingBad.png"&gt;&lt;/p&gt;

&lt;p&gt;New equation
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/MicroFacetSamplingGood.png"&gt;&lt;/p&gt;

&lt;p&gt;They should be producing the same results, but the old version appears to be producing incorrect results for inputs within a certain range. The inputs are &amp;quot;dithered&amp;quot; according to a 4x4 pattern in this example, which is why is appears regular.&lt;/p&gt;

&lt;h3 id="toc_11"&gt;Edges are too dark&lt;/h3&gt;

&lt;p&gt;The differences in the PDF equation are actually making the edges too dark. This is related to the &lt;em&gt;VdotH&lt;/em&gt; term. It makes sense to remove &lt;em&gt;VdotH&lt;/em&gt; from the PDF for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The view direction has no impact on the microfacets generated&lt;/li&gt;
&lt;li&gt;We want to use this sampling in view independent equations&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Old version:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/MicrofacetEdgesOld.png"&gt;&lt;/p&gt;

&lt;p&gt;New version:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/MicrofacetEdgesNew.png"&gt;&lt;/p&gt;

&lt;p&gt;In the new version, the extreme edges are much brighter. The dark halo we get with the old version seems unnatural &amp;amp; incorrect.&lt;/p&gt;

&lt;h2 id="toc_12"&gt;NdotL&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve added the &lt;strong&gt;NdotL&lt;/strong&gt; term into IBL, as well. This was previously excluded but is required to match our dynamic specular equation.&lt;/p&gt;

&lt;p&gt;This is visually important -- without it, the edges become excessively bright, brighter than the thing they are reflecting. With &lt;em&gt;NdotL&lt;/em&gt;, the brightness correctly matches the reflected object.&lt;/p&gt;

&lt;p&gt;In effect, we&amp;#39;ve removed a &lt;strong&gt;VdotH&lt;/strong&gt; term, and added a &lt;strong&gt;NdotL&lt;/strong&gt;. Note that &lt;strong&gt;VdotH&lt;/strong&gt; is the same as &lt;strong&gt;LdotH&lt;/strong&gt; when the &lt;strong&gt;&amp;quot;H&amp;quot;&lt;/strong&gt; is the half-vector between &lt;strong&gt;L&lt;/strong&gt; and &lt;strong&gt;V&lt;/strong&gt;... So this might be source for the mysterious &lt;strong&gt;VdotH&lt;/strong&gt; term in Karis&amp;#39; PDF! He may have assumed that this was part of the PDF, when in fact he was just compensating for a part of his specular equation. If that&amp;#39;s the case, though, why isn&amp;#39;t it explained in his course notes?&lt;/p&gt;

&lt;h2 id="toc_13"&gt;Rebuilding the lookup table&lt;/h2&gt;

&lt;p&gt;Our split-term solution involves a texture lookup table &amp;quot;glosslut.dds.&amp;quot; This is built from all of the math on this page. Our previous version was very similar to Karis&amp;#39; version. But let&amp;#39;s rebuild it, and see what happens!&lt;/p&gt;

&lt;p&gt;Old version:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/OldR.png"&gt;&lt;img src="/assets/media/ImprovedIBL/OldG.png"&gt;&lt;/p&gt;

&lt;p&gt;New version:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/NewR.png"&gt;&lt;img src="/assets/media/ImprovedIBL/NewG.png"&gt;&lt;/p&gt;

&lt;p&gt;So, the changes are subtle, but important. The specular is general a little bit darker, except at the edges of objects where it has become significantly brighter.&lt;/p&gt;

&lt;h3 id="toc_14"&gt;Comparisons&lt;/h3&gt;

&lt;p&gt;Old look up table:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/OldGlossLUT.png"&gt;&lt;/p&gt;

&lt;p&gt;New look up table:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/NewGlossLUT.png"&gt;&lt;/p&gt;

&lt;p&gt;Reference:
&lt;br/&gt;&lt;img src="/assets/media/ImprovedIBL/ReferenceIBL.png"&gt;&lt;/p&gt;

&lt;p&gt;This is just changing the lookup-table. You can see how the reflection is slightly darker in the new version, except that the edges are much brighter. Also, new version matches our reference image much better.&lt;/p&gt;

&lt;p&gt;At high F0 values (and along the edges) the reflection appears to be the correct brightness to match the environment.&lt;/p&gt;

&lt;h3 id="toc_15"&gt;Alpha remapping&lt;/h3&gt;

&lt;p&gt;One thing to note is that in XLE, the same alpha remapping is used for IBL that is used for dynamic specular maps. Karis suggests using different remapping for IBL, explaining that the normal remapping produces edges that are too dark. It seems like we&amp;#39;ve discovered the true cause of the dark edges and the results seem to look fine for us with our normal remapping.&lt;/p&gt;

&lt;p&gt;Also, since we&amp;#39;re using the exact same specular equation as we use for dynamic lighting, the two will also match well.&lt;/p&gt;

&lt;h2 id="toc_16"&gt;Diffuse normalization&lt;/h2&gt;

&lt;p&gt;Previously, I had been assuming our texture pipeline had was premultiplying the &lt;em&gt;&amp;quot;1/pi&amp;quot;&lt;/em&gt; diffuse normalization factor into the diffuse IBL texture. However, that turned out to be incorrect. I&amp;#39;ve compensated for now by adding this term into the shader. However, I would be ideal if we could take care of this during the texture processing step.&lt;/p&gt;

&lt;p&gt;With this change (and the changes to specular brightness), the diffuse and specular are better matched. This is particularly important for surfaces that are partially metal and partially dielectric -- because the diffuse on the dielectric part must feel matched when compared to the specular on the metal part.&lt;/p&gt;

&lt;h2 id="toc_17"&gt;Further research&lt;/h2&gt;

&lt;p&gt;Here are some more ideal for further improvements.&lt;/p&gt;

&lt;h3 id="toc_18"&gt;H vs N in G calculation&lt;/h3&gt;

&lt;p&gt;There is an interesting problem related to the use of the normal in the &amp;quot;G&amp;quot; part of the specular equation. Different sources actually feed different dot products into this equation:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Some use &lt;strong&gt;N dot L&lt;/strong&gt; and &lt;strong&gt;N dot V&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Others use &lt;strong&gt;H dot L&lt;/strong&gt; and &lt;strong&gt;H dot V&lt;/strong&gt; (where H is the half-vector or microfacet normal)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For our dynamic specular lights, we are using a single value for &lt;strong&gt;N&lt;/strong&gt; and &lt;strong&gt;H&lt;/strong&gt;, so the difference should be very subtle. But when sampling multiple microfacets, we are using many different values for &lt;strong&gt;H&lt;/strong&gt;, with a constant &lt;strong&gt;N&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For example, when using the specular equation as a weight while filtering the cubemap, &lt;strong&gt;N dot L&lt;/strong&gt; and &lt;strong&gt;N dot V&lt;/strong&gt; are constant. So they will have no effect on the filtering.&lt;/p&gt;

&lt;p&gt;Remember that &lt;strong&gt;&amp;quot;G&amp;quot;&lt;/strong&gt; is the shadowing term. It drops off to zero around the extreme edges. So by using &lt;strong&gt;H dot L&lt;/strong&gt; and &lt;strong&gt;H dot V&lt;/strong&gt;, we could help reduce the effect of extreme samples.&lt;/p&gt;

&lt;p&gt;Also, when building the split-term lookup table, &lt;strong&gt;N dot V&lt;/strong&gt; is constant. So it has very little effect. It would be interesting experimenting with the other form in these cases.&lt;/p&gt;

&lt;p&gt;On the other hand, in the dynamic specular equation &lt;strong&gt;H dot L&lt;/strong&gt; and &lt;strong&gt;H dot V&lt;/strong&gt; should be equal and very rarely low numbers. So, in that case, it may not make sense to use the half-vector.&lt;/p&gt;

&lt;h3 id="toc_19"&gt;Problems when NdotV is &amp;lt; .15f&lt;/h3&gt;

&lt;p&gt;Rough objects current tend to have a few pixels around the edges that have no reflection. It&amp;#39;s isn&amp;#39;t really visible in the dielectric case. But with metals (where there is no diffuse) it shows up as black and looks very strange.&lt;/p&gt;

&lt;p&gt;The problem is related to how we&amp;#39;re sampling the microfacet normals. As you can imagine, when normals are near perpendicular to view direction, sometimes the microfacet normal will end up pointing away from the view direction. When this happens, we can reject the sample (or calculate some value close to black). When the number of samples rejected is too high, we will end up with a result that is not accurate.&lt;/p&gt;

&lt;p&gt;This tends to only affect a few pixels, but it&amp;#39;s enough to pop out. We need some better solution around these edge parts.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Transmitted specular progress</title>
      <link>http://xlgames-inc.github.io/posts/transmittedspecular2</link>
      <pubDate>January 29, 2016</pubDate>
      <description>&lt;p&gt;Just a quick update... I&amp;#39;ve been making some great progress with transmitted specular for IBL!&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/TransSphereMat.png"&gt;&lt;/p&gt;

&lt;p&gt;These screenshots will look a little strange (I mean that black borders &amp;amp; grainyness), because it&amp;#39;s a debugging rendering mode.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/TransSphereMat2.png"&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve got the split-term stuff going; and it looks like it should be practical. I&amp;#39;d prefer to avoid having yet another cubemap, so maybe there&amp;#39;s some way to just reuse the reflection filtered cubemap. Its seems reasonable to say that the filtering should be similar. We just need some way to calculate the amount of blurriness that is correct for transmissions.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/TransSphereMat3.png"&gt;&lt;/p&gt;

&lt;p&gt;Speaking of that, I&amp;#39;ve been thinking the type of filtering that is applied to the specular map. More on that later.&lt;/p&gt;

&lt;p&gt;Next week I&amp;#39;ll post some proper screenshots &amp;amp; a whole lot of details!&lt;/p&gt;

&lt;p&gt;I made some interesting observations while doing this, I&amp;#39;ve got a bunch of new changes and improvements for IBL. Check out the &amp;quot;experimental&amp;quot; branch for now.&lt;/p&gt;

&lt;p&gt;BTW, the node diagram for the GGX BSDF equation looks a little nicer now --&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/BSDFNodes2.png"&gt;&lt;/p&gt;

&lt;p&gt;(See the older version here:&lt;a href="transmissionnodegraph"&gt;Transmission Node Diagram&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;As you can see in the diagram, the equations are causing a lot of light to get focused in around the edges. This seems to be exaggerated when roughness is very high. That might require a little more investigation next week.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Processing textures with the XLE scriptable asset path</title>
      <link>http://xlgames-inc.github.io/posts/assetpathscripts</link>
      <pubDate>January 26, 2016</pubDate>
      <description>&lt;p&gt;I&amp;#39;ve included a few Gradle scripts in the XLE distribution. This is mostly just a simple set of tools I use for my own testing. But you may find it useful for your own needs.&lt;/p&gt;

&lt;p&gt;Of course this system isn&amp;#39;t designed to be 100% robust and fool proof. Actually, it&amp;#39;s just a few simple scripts. But it is scalable and flexible.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Gradle&lt;/h2&gt;

&lt;p&gt;At heart, an &lt;strong&gt;&amp;quot;asset path&amp;quot;&lt;/strong&gt; is some system that can identify changed assets, recognize processing working that needs to be performed with those assets, and schedule that work.&lt;/p&gt;

&lt;p&gt;For example, we might have an asset for a sky background texture. Various processing tools need to pre-filter this texture so it can be used for image based lighting. Whenever the texture changes (or when the processing tools change) we want to execute the processing steps and produce intermediate assets.&lt;/p&gt;

&lt;p&gt;XLE does some processing at runtime. But that is only practical for short processing steps. Expensive processing steps need some other solution.&lt;/p&gt;

&lt;p&gt;So we need some build system to manage assets and dependences. Are requirements are similar to build systems we use for code. But most code-oriented build systems don&amp;#39;t work well for assets.&lt;/p&gt;

&lt;p&gt;I picked &lt;em&gt;Gradle&lt;/em&gt; because of it&amp;#39;s procedural nature. It allows us to specify input assets -- but also to give instructions to the build path on how to handle that object. For example, we need to tell the build path if a texture is a sky texture, a normals texture, or some other type... That kind of thing just falls out of &lt;em&gt;Gradle&lt;/em&gt; very easily.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Executing Gradle&lt;/h2&gt;

&lt;p&gt;First, you need to install Gradle, from: &lt;a href="http://gradle.org/gradle-download/"&gt;http://gradle.org/gradle-download/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In Tools/AssetPath, we have the &amp;quot;settings.gradle&amp;quot; root project file. Also, this folder contains some &amp;quot;groovy&amp;quot; source files that contain gradle task type implementations. So, there is a task type for processing sky textures -- which basically just involves executing a number of command line operations.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve also defined a root task called &lt;strong&gt;&amp;quot;tex&amp;quot;&lt;/strong&gt;. This is where gradle starts to shine. The &amp;quot;tex&amp;quot; task searches through the entire working folder for build.gradle files that contain tasks whose names being with &lt;strong&gt;&amp;quot;tex...&amp;quot;&lt;/strong&gt;. All of the tasks found become subtasks of the root &amp;quot;tex&amp;quot; task.&lt;/p&gt;

&lt;p&gt;So, we can create a &lt;strong&gt;&amp;quot;build.gradle&amp;quot;&lt;/strong&gt; in any folder in the working directory. And if we add tasks to that file that being with &amp;quot;tex&amp;quot; (typically &amp;quot;tex0&amp;quot;, &amp;quot;tex1&amp;quot;, etc), they will be automatically added as subtasks of the root &amp;quot;tex&amp;quot; task.&lt;/p&gt;

&lt;p&gt;This is important because when we execute gradle (using the &lt;em&gt;&amp;quot;Tools/AssetPath/execute.bat&amp;quot;&lt;/em&gt; batch file) and pass the root task &amp;quot;tex&amp;quot; on the command line, this has the effect of execute all &amp;quot;tex...&amp;quot; tasks in the entire working directory.&lt;/p&gt;

&lt;p&gt;Pretty cool, right? We can also execute individual tasks using the normal gradle command line.&lt;/p&gt;

&lt;p&gt;There is an example in git in &lt;em&gt;&amp;quot;Working/Game/xleres/DefaultResources/build.gradle.&amp;quot;&lt;/em&gt; This project will generate the standard lookup tables.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Processing Textures&lt;/h2&gt;

&lt;p&gt;Executing Gradle is just the first step. Gradle is just going to chain together command line operations. Those operations require a bunch of tools.&lt;/p&gt;

&lt;p&gt;One of the most useful task types is &amp;quot;xle.EquiRectEnv.&amp;quot; This takes in a equirectangular (ie, paranoramic) environment map and produces 3 important textures: Cubemap background, Specular IBL texture and Diffuse IBL texture.&lt;/p&gt;

&lt;p&gt;This get a little complicated. This is just the pipeline I use for processing these textures myself. So it&amp;#39;s a little complicated and involved right now.&lt;/p&gt;

&lt;p&gt;First, install these:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;AMD CubemapGen&lt;/em&gt; (&lt;a href="http://developer.amd.com/tools-and-sdks/archive/legacy-cpu-gpu-tools/cubemapgen/"&gt;http://developer.amd.com/tools-and-sdks/archive/legacy-cpu-gpu-tools/cubemapgen/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;S&#xE9;bastien Lagarde&amp;#39;s modified CubemapGen&lt;/em&gt;: &lt;a href="https://seblagarde.wordpress.com/2012/06/10/amd-cubemapgen-for-physically-based-rendering/"&gt;https://seblagarde.wordpress.com/2012/06/10/amd-cubemapgen-for-physically-based-rendering/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;nvcompress&lt;/em&gt; ** from nvidia-texture-tools (see below)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;TextureProcess&lt;/em&gt; sample from XLE&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All of these must in the system environment variable &lt;strong&gt;path&lt;/strong&gt;. In the case of TextureProcess, don&amp;#39;t move the executable from the Finals_** folders (because it needs to find the working directory for shaders).&lt;/p&gt;

&lt;p&gt;I use a slightly slightly modified version of nvidia-texture-tools that works better with HDR textures. The standard nvidia-texture-tools always tonemaps HDR textures on load-in (frustratingly). You can find my modified version here: &lt;a href="https://github.com/djewsbury/nvidia-texture-tools"&gt;https://github.com/djewsbury/nvidia-texture-tools&lt;/a&gt;. This modified version allows us to read HDR files (from .hdr and other formats) and write unprocessed floating point .dds file.&lt;/p&gt;

&lt;p&gt;S&#xE9;bastien Lagarde&amp;#39;s &lt;strong&gt;ModifiedCubeMapGen&lt;/strong&gt; is used to generate the diffuse IBL texture. This goes via a spherical harmonic representation before arriving at a small cubemap.&lt;/p&gt;

&lt;p&gt;XLE&amp;#39;s &lt;strong&gt;TextureProcess.exe&lt;/strong&gt; is used to generate the specular IBL texture, as well as compressing to BC6. The specular IBL process reads from a equirectangular HDR map and writes a cubemap.&lt;/p&gt;

&lt;p&gt;All of these textures need to be compress to BC6. Awkwardly, nvidia-texture-tools doesn&amp;#39;t support compressing HDR data to BC6 properly, but &lt;strong&gt;TextureProcess.exe&lt;/strong&gt; can do that via the &lt;strong&gt;DirectXTex&lt;/strong&gt; library.&lt;/p&gt;

&lt;p&gt;So, once you&amp;#39;ve got all of that, compiled it all, put in all in the path... Then you&amp;#39;re ready to process textures.&lt;/p&gt;

&lt;p&gt;Intermediate outputs from this process get written into the &amp;quot;int/u&amp;quot; (or intermediate/universal) directory. From there, you can copy it out, or do what you like. You should get a &lt;em&gt;(file).dds&lt;/em&gt;, &lt;em&gt;(file)_diffuse.dds&lt;/em&gt; &amp;amp; &lt;em&gt;(file)_specular.dds&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Make it your own!&lt;/h2&gt;

&lt;p&gt;It&amp;#39;s difficult to get this working the first time. There&amp;#39;s a lot of work. But once it&amp;#39;s going, all of the parts are extensible and exchangeable. It&amp;#39;s a simple, but very flexible system. Handy for managing small, everyday processing tasks.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Transmitted specular</title>
      <link>http://xlgames-inc.github.io/posts/transmittedspecular</link>
      <pubDate>January 25, 2016</pubDate>
      <description>&lt;p&gt;I&amp;#39;ve modified the lighting equations to allow for transmitted (as well as reflected) specular for dynamic lightings. Transmitted specular means the light is on the opposite side of the object, but light is coming through the object towards the viewer.&lt;/p&gt;

&lt;p&gt;This is important for thin materials (such as leaves)&lt;/p&gt;

&lt;h3 id="toc_0"&gt;No transmitted specular:&lt;/h3&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/LeavesNoTransSpec.png"&gt;&lt;/p&gt;

&lt;h3 id="toc_1"&gt;With transmitted specular:&lt;/h3&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/TransSpec/LeavesTransSpec.png"&gt;&lt;/p&gt;

&lt;p&gt;Here the amount of transmission is maybe slightly too high -- but it shows the effect well.
This works particularly well with shadowed directional lights and geometry using order independent transparency. The above screenshot is exactly that situation. The tree leaves are rendered with stochastic transparency (which works particularly well for this model). So we get a nice halo effect around the edges of the tree.&lt;/p&gt;

&lt;p&gt;And, if you look closely, you can see that the tree shadowing is blocking the transmitted specular, giving a volumetric look.&lt;/p&gt;

&lt;p&gt;This model is built in a way to create a lot of noise in the normals. As you can see here:
&lt;br/&gt;&lt;img src="/assets/media/TransSpec/LeavesNormals.png"&gt;&lt;/p&gt;

&lt;p&gt;Normals point in every direction, and there is a lot of local variation.&lt;/p&gt;

&lt;p&gt;The transmitted specular really helps highlight this noise. It helps to make the tree canopy feel more 3D, and it helps to hide the &amp;quot;true&amp;quot; triangular geometry. The preview reflected-only specular just wasn&amp;#39;t able to do this as well.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;It&amp;#39;s expensive!&lt;/h2&gt;

&lt;p&gt;The transmitted specular calculation is actually a second, additional, specular calculation -- and it&amp;#39;s a little more expensive than the reflected specular. It&amp;#39;s using the node diagram from the previous post, &lt;a href="transmissionnodegraph"&gt;Transmission Node Diagram&lt;/a&gt;. So it&amp;#39;s not really cheap. It&amp;#39;s particularly expensive in this case, because stochastic transparency can result in many lighting calculations per pixel (for the different depth layers).&lt;/p&gt;

&lt;h2 id="toc_3"&gt;IBL implementation&lt;/h2&gt;

&lt;p&gt;This is not currently working with image based lighting. Part of the &lt;strong&gt;IBL&lt;/strong&gt; principle is that every effect applied to &amp;quot;dynamic&amp;quot; lighting should also be applied to &lt;strong&gt;IBL&lt;/strong&gt; -- (so material appear the same under both types of lights, and so we can fade distant dynamic lights into precalculated IBL).&lt;/p&gt;

&lt;p&gt;It might be interesting to try to get it working under IBL, so I&amp;#39;ll give it a shot.&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Better doubled sided GGX specular equation&lt;/h2&gt;

&lt;p&gt;When the shader flag &lt;em&gt;&amp;quot;MAT&lt;em&gt;DOUBLE&lt;/em&gt;SIDED_LIGHTING&amp;quot;&lt;/em&gt; is set, the specular equation is now double sided. The results will be symmetrical for a flipped normal (ie, &lt;em&gt;Specular(normal)&lt;/em&gt;==&lt;em&gt;Specular(-normal)&lt;/em&gt;, for all normals). It can be handy in cases like this, where the normals are pointing in all directions.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Transmission Node Diagram</title>
      <link>http://xlgames-inc.github.io/posts/transmissionnodegraph</link>
      <pubDate>January 22, 2016</pubDate>
      <description>&lt;p&gt;Here is a node diagram for the specular transmission function (BTDF) from Walter, et al, from &lt;em&gt;&amp;quot;Microfacet Models for Refraction through Rough Surfaces.&amp;quot;&lt;/em&gt;
Also known as the &amp;quot;GGX&amp;quot; model (or Trowbridge-Reitz).&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/MaterialTool/WalterGGXTransmission.png"&gt;&lt;/p&gt;

&lt;p&gt;You can see how the node graph comes in handy for debugging a shader function like this. It&amp;#39;s useful to visualize each term and see it&amp;#39;s effects.
For example, the &amp;quot;D&amp;quot; term of the BTDF is a node on the diagram. And we can visualize the effects of that term independent of everything else.&lt;/p&gt;

&lt;p&gt;In this case, the diagram saves a HLSL function, and that function is called by text-based HLSL code. This will become part of some new functionality for specular transmission through thin surfaces.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll pop this in a new &amp;quot;experimental&amp;quot; branch.&lt;/p&gt;

&lt;p&gt;See the post from yesterday, &lt;a href="materialtool"&gt;Material and Node Diagram Tool&lt;/a&gt;, for more information.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Material and Node Diagram Tool</title>
      <link>http://xlgames-inc.github.io/posts/materialtool</link>
      <pubDate>January 21, 2016</pubDate>
      <description>&lt;p&gt;The master branch has just been updated! It now includes a major new tool, called the &lt;em&gt;MaterialTool&lt;/em&gt;. This is a dedicated app for building materials and applying them to objects, and includes some cool new features...&lt;/p&gt;

&lt;p&gt;In XLE, most material information can be authored in standard graphics packages (like 3DS Max, Maya, Substance Painter, etc). In particular, the most critical material values (&lt;strong&gt;color&lt;/strong&gt;, &lt;strong&gt;roughness&lt;/strong&gt;, &lt;strong&gt;specular&lt;/strong&gt; &amp;amp; &lt;strong&gt;metal&lt;/strong&gt;) can come directly from standard packages.&lt;/p&gt;

&lt;p&gt;But in cases we want to add custom information to models, or even develop custom shaders for complex materials. This is were the &lt;em&gt;MaterialTool&lt;/em&gt; comes in. There is some functionality in common with the &lt;em&gt;ModelViewer&lt;/em&gt; and &lt;em&gt;LevelEditor&lt;/em&gt; tools -- but the &lt;em&gt;MaterialTool&lt;/em&gt; provides a convenient focused tool for this kind of work.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_0"&gt;Basic functionality&lt;/h2&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/MaterialTool/BasicWindow.png"&gt;&lt;/p&gt;

&lt;p&gt;Our core functionality allows us to preview a model (much like the &lt;em&gt;ModelViewer&lt;/em&gt;, with various rendering modes), click on materials and then change their properties (such as opacity, translucency modes, and various shader flags).&lt;/p&gt;

&lt;p&gt;This works within the Sony ATF framework, and so we have all of the handy features from the &lt;em&gt;LevelEditor&lt;/em&gt;, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;detachable, arrangeable windows&lt;/li&gt;
&lt;li&gt;&lt;em&gt;IronPython&lt;/em&gt; scripting and cvar access&lt;/li&gt;
&lt;li&gt;interface skinning, keyboard rebinding, etc...&lt;/li&gt;
&lt;li&gt;and, of course, it&amp;#39;s all very extensible C# code, convenient for adding custom features&lt;/li&gt;
&lt;/ul&gt;

&lt;hr&gt;

&lt;h2 id="toc_1"&gt;Node Diagram editor&lt;/h2&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/MaterialTool/NodeDiagramEditor.png"&gt;&lt;/p&gt;

&lt;p&gt;Also integrated is a new version of the node diagram editor. This is used for building custom shaders for special cases.
Make it possible to visually create shader logic. It&amp;#39;s designed for use by both programmers and technical artists.&lt;/p&gt;

&lt;p&gt;Each diagram becomes a expression in HLSL shader code (and this can be used, just like any other shader). But these expressions can become very complex, and can (indirectly) include loops and conditions.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/MaterialTool/DiagramCharts.png"&gt;&lt;/p&gt;

&lt;p&gt;Each node in the diagram has a real time preview. For mathematical nodes, this might be a &lt;strong&gt;chart&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/MaterialTool/Preview3D.png"&gt;&lt;/p&gt;

&lt;p&gt;But we can also have &lt;strong&gt;3D previews&lt;/strong&gt; (using a sphere, box or a full model).&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/MaterialTool/PreviewTexture.png"&gt;&lt;/p&gt;

&lt;p&gt;Texture nodes can also be previewed using a flat &lt;strong&gt;2D preview&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Previewing the diagram at every node makes the process of creating shaders much more visual. The effect of each function on the final shader becomes immediately apparent.&lt;/p&gt;

&lt;p&gt;For example, XLE has some shader code for converting a &amp;quot;&lt;em&gt;specular color&lt;/em&gt;&amp;quot; texture into the new &amp;quot;&lt;em&gt;roughness, specular, metal&lt;/em&gt;&amp;quot; scheme (for convenience reasons). Building this logic as a node diagram is infinitely easier than just working in raw &lt;em&gt;HLSL&lt;/em&gt; directly, because we can see the results immediately, and in detail.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_2"&gt;Nodes are HLSL functions!&lt;/h2&gt;

&lt;p&gt;So far, the node diagram tool sounds fairly standard. But there&amp;#39;s an important twist. A node diagram is just a collection of &amp;quot;nodes&amp;quot; that have been connected together. But where do those nodes come from, and what do they do?&lt;/p&gt;

&lt;p&gt;XLE contains a permissive &lt;em&gt;HLSL&lt;/em&gt; parser (written in &lt;em&gt;Antlr3&lt;/em&gt;). This parser can parse almost all valid &lt;em&gt;HLSL&lt;/em&gt; code and can build an abstract syntax tree of it&amp;#39;s contents. In particular, we can use this parser to extract the function signatures from a shader file.&lt;/p&gt;

&lt;p&gt;So, for example the shader file &lt;em&gt;Surface.h&lt;/em&gt; contains the functions &lt;em&gt;VSIn_GetLocalPosition&lt;/em&gt;, &lt;em&gt;VSIn_GetLocalTangent&lt;/em&gt;, etc... Our parser can read &lt;em&gt;Surface.h&lt;/em&gt; and find those functions, plus their parameters, output type, semantics, etc.&lt;/p&gt;

&lt;p&gt;So, there&amp;#39;s our answer! Our nodes are actually &lt;em&gt;HLSL&lt;/em&gt; functions. And since our parser works will every shader source file in XLE, that means that any shader function can be used as a node.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/MaterialTool/ShaderFragmentPalette.png"&gt;&lt;/p&gt;

&lt;p&gt;Nodes are dragged into the diagram from something called the shader fragment palette.&lt;/p&gt;

&lt;p&gt;There are no hard coded nodes, and the shader fragment palette is reloaded on the fly. So, if you&amp;#39;re building a diagram and suddenly realize you want a new node type... Just open a text editor, add a new function into a shader file, and it can immediately be dragged into your diagram.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;xleres/Nodes&lt;/em&gt; directory is set aside to contain functions that are specifically intended to be used as nodes. In some cases, functions in this folder are just thin wrappers over other functions. But it&amp;#39;s recommended to mostly use functions from this directory in node diagrams, so as to isolate diagrams from shader changes.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_3"&gt;Each diagram is both a node type and a shader function&lt;/h2&gt;

&lt;p&gt;Use of HLSL as nodes creates some interesting advantages. Each diagram itself is a shader function. And so, when you save a diagram to disk, you can then use that diagram as a node in another diagram. In this way, we can have embedded diagrams very easily.&lt;/p&gt;

&lt;p&gt;It also means that we have full control over when to use a diagram, and when to use text-based HLSL. Some expressions are just awkward to do in diagram form.&lt;/p&gt;

&lt;p&gt;For example, try implementing a Modulo function as a diagram using just divide, multiply, subtract and round nodes. It can be done, but it&amp;#39;s awkward. In cases like this, it&amp;#39;s better to just write a text based function (in this case, using built-in shader language functions) and then use that function within your diagram.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_4"&gt;Complex shaders&lt;/h2&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/MaterialTool/ProceduralWood.png"&gt;&lt;/p&gt;

&lt;p&gt;These methods can be used to create arbitrarily complex shaders. Above is an example of a node diagram that was duplicated from the (CC-Zero) Cycles Material Library:
    &lt;a href="http://www.blendswap.com/blends/view/56470"&gt;http://www.blendswap.com/blends/view/56470&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is a 100% procedural texture, with no texture inputs. The XLE implementation uses the same arrangements of nodes and the same constants to give the same final results as the Cycles render engine.&lt;/p&gt;

&lt;p&gt;Since the output is text &lt;em&gt;HLSL&lt;/em&gt; code, the normal &lt;em&gt;HLSL&lt;/em&gt; compiler and optimizers apply. There are certain cases in which hand written &lt;em&gt;HLSL&lt;/em&gt; code will be more efficient the diagram based stuff -- but that might be an advanced topic. In many cases, the diagram based shaders should be as efficient as hand written code.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_5"&gt;Using a diagram as a material&lt;/h2&gt;

&lt;p&gt;To use a diagram as an object material, follow these steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create an output node from &lt;strong&gt;Nodes/Outputs.sh:Output_PerPixel&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You may need some inputs from geometry, these are usually &amp;quot;Get&amp;quot; or &amp;quot;Sample&amp;quot; functions. For example, &lt;strong&gt;Nodes/Texture.sh:SampleTextureDiffuse&lt;/strong&gt;, and &lt;strong&gt;Surface.h:GetNormal&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;To create a material parameter, right click in empty space and select &amp;quot;Create Input&amp;quot;&lt;/li&gt;
&lt;li&gt;Go to &lt;em&gt;Edit/Diagram Settings...&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Select &amp;quot;Technique (material for any object)&amp;quot; as the diagram type&lt;/li&gt;
&lt;li&gt;Save your diagram! (you must save to see the results in the Model view window currently)&lt;/li&gt;
&lt;li&gt;Now you can go to the &amp;quot;Model view&amp;quot; window, right click an object and select &lt;em&gt;&amp;quot;Assign Technique (...)&amp;quot;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Check out &lt;strong&gt;Working/Game/xleres/Objects/Basic.tech&lt;/strong&gt; as a starting example.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_6"&gt;Node graph dynamic linking&lt;/h2&gt;

&lt;p&gt;You may notice some similarity between this technology and a previous post -- &lt;a href="functionlinkedshaders"&gt;Dynamic Function Linking Graph for Shaders&lt;/a&gt;. They are similar because they both involve linking together the inputs and outputs of shader functions. But unfortunately they don&amp;#39;t work together yet... Perhaps later...?&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_7"&gt;Other uses of the HLSL parser&lt;/h2&gt;

&lt;p&gt;The HLSL parser has some other cool applications... It can parse most valid HLSL code. And actually, it&amp;#39;s fairly permissive, so some invalid HLSL code will parse, as well.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been using this parser as a linter for HLSL code in the Atom editor! There is a linter plugin for Atom, and so all it involved was creating a script that ran the &amp;quot;ShaderScan&amp;quot; sample. This reads HLSL code and spits out parsing errors.&lt;/p&gt;

&lt;p&gt;And so, those parsing errors now appear in real time while writing HLSL code in Atom. This has two uses for me, currently... It catches certain errors in the HLSL (Intellisense-style). But it&amp;#39;s also serving as a way to test the parser itself!&lt;/p&gt;

&lt;p&gt;In theory, this parser could also be extended to provide automatic conversion between HLSL and GLSL (or other languages). Some engines use this kind of approach for dealing with cross platform issues. Another possibility is just to use a complex series of #defines... But either method would be awkward in it&amp;#39;s own unique way.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_8"&gt;Support for other languages&lt;/h2&gt;

&lt;p&gt;Currently the node diagram tool is designed for use with HLSL. However, in theory it can also be used with other languages. All we need is a parser that can extract the function signatures. Since HLSL is a fairly generic c-like syntax, it shares a lot of similarity with many other languages. So the code that builds HLSL from the shader diagram could probably be easily adapted for languages (like Lua, Python, D, Swift, whatever).&lt;/p&gt;

&lt;p&gt;This could be handy because HLSL is tied to GPU execution only. But another language would open the door for CPU side execution -- which could be used for game logic, physics or any other systems. This would be handy, because it would mean reusing the same core node diagram functionality for multiple separate tasks.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Dynamic Function Linking Graph for Shaders</title>
      <link>http://xlgames-inc.github.io/posts/functionlinkedshaders</link>
      <pubDate>December 16, 2015</pubDate>
      <description>Stitching shaders together at runtime to deal with shader configuration explosion</description>
    </item>
    <item>
      <title>Rectangle Light diffuse vs 3DS Max</title>
      <link>http://xlgames-inc.github.io/posts/rectlightdiffuse</link>
      <pubDate>December 11, 2015</pubDate>
      <description>&lt;p&gt;Just a quick note on rectangle light diffuse. In XLE, rectangle lights are slightly different from the default rectangle lights in 3DS Max.&lt;/p&gt;

&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-yw4l{vertical-align:top}
&lt;/style&gt;

&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-yw4l"&gt;XLE lights&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;3DS Max lights (Quicksilver renderer)&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightDiffuse/XLELightDiag.png"&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightDiffuse/MaxLightDiag.png"&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightDiffuse/XLEDefault.png"&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightDiffuse/MaxComparison.png"&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;XLE rectangle lights emit light mostly in the forward direction (and a reduced amount sideways). But in the 3DS Max Quicksilver renderer, rectangle lights emit light in all direction.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Edge cases&lt;/h2&gt;

&lt;p&gt;This is partially because the method we&amp;#39;re using for diffuse doesn&amp;#39;t work well on extreme angles. In the above example, the light plane and the &lt;strong&gt;top&lt;/strong&gt;/&lt;strong&gt;bottom&lt;/strong&gt;/&lt;strong&gt;left&lt;/strong&gt;/&lt;strong&gt;right&lt;/strong&gt; walls are actually perpendicular.&lt;/p&gt;

&lt;p&gt;This causes weird problems right on the edges.&lt;/p&gt;

&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-yw4l"&gt;XLE emulating 3DS Max&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;3DS Max lights (Quicksilver Renderer)&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightDiffuse/XLEEmulateMax.png"&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightDiffuse/MaxComparison.png"&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;In the above example, the left and right walls look fine. But the top and bottom walls are missing their highlight. This is because the light is quite tall. A square light might be fine.&lt;/p&gt;

&lt;p&gt;But you can see that, in general, the 3DS Max result and the XLE result is quite similar.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Practical uses&lt;/h2&gt;

&lt;p&gt;It might be possible to fix this with some research and a few tricks. But it&amp;#39;s not clear if that would really be useful. Rectangle lights might be most useful for things like windows, TV screens, photography reflectors, etc. In these cases we normally want the light to always go forward, anyway.&lt;/p&gt;

&lt;p&gt;Plus, if we block off the extreme edges, we have some easy options for shadowing.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Working some improvements to rectangle lights</title>
      <link>http://xlgames-inc.github.io/posts/workingonrectlights</link>
      <pubDate>December 09, 2015</pubDate>
      <description>&lt;p&gt;&lt;strong&gt;Post updated on 2015-12-10&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is just some notes about some ideas I&amp;#39;ve been playing with lately. I&amp;#39;ve been thinking about some improvements to the specular highlights for rectangle lights!&lt;/p&gt;

&lt;p&gt;The current implementation is based on the method by &lt;em&gt;Michal Drobot&lt;/em&gt; in &lt;strong&gt;GPU Pro 5&lt;/strong&gt;. I&amp;#39;m not going to repeat the description here (I&amp;#39;ll only give a few details) -- but I recommend buying the book and having a look! It&amp;#39;s quite a good method, and interesting read. Drobot describes a very practical method for diffuse and specular for rectangle and disc lights.&lt;/p&gt;

&lt;p&gt;However, at extreme angles, the specular reflection of rectangle lights can sometimes show some distortion.&lt;/p&gt;

&lt;p&gt;Here is a comparison:
&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-yw4l{vertical-align:top}
&lt;/style&gt;
&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-yw4l"&gt;Original Method&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;New Method&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/BasicRectLight.png"&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/ImprovedRectLight.png"&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;It appears quite distorted here, but this is a contrived example. This is a material with high &amp;quot;roughness&amp;quot; but the geometry is completely flat. In a normal scene, flat geometry will probably have a less rough material. And in a less rough material, the distortion is much less visible.&lt;/p&gt;

&lt;h2 id="toc_0"&gt;Specular Cone&lt;/h2&gt;

&lt;p&gt;Part of the problem is related to how the integration across the specular highlight is performed. We can calculate a cone that represents the part of the scene that contributes most greatly to the specular for a given point. When calculating the specular for a point, we assume that everything in that cone contributes very strongly to specular, and everything outside of it doesn&amp;#39;t contribute at all.&lt;/p&gt;

&lt;p&gt;This is a simplification, because really we don&amp;#39;t want a binary on/off -- we want to weight all incoming light by the BRDF. But out simplification can be quite good for many cases.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Rectangle and cone intersection&lt;/h2&gt;

&lt;p&gt;So, once we have our cone, we want to find how much of it is covered by the rectangle light. Ignoring shadows, we can do this by finding the intersection of the cone and the rectangle light.&lt;/p&gt;

&lt;p&gt;But, of course, this is quite difficult and expensive. This is starting to go into conic sections, which are a particularly complication type of geometry.&lt;/p&gt;

&lt;p&gt;So we need a simplification. Drobot describes a method that is very similar to his method for disc lights.&lt;/p&gt;

&lt;p&gt;His method treats the intersection between a plane and the cone as a circle, and then further simplifies it into a square. This method works really well when the light direction is directly towards to the sample point. But on angles, the intersection should start to distort into an ellipse.&lt;/p&gt;

&lt;p&gt;See, for example, this diagram from wikimedia commons:
&lt;br/&gt;&lt;a title="By http://commons.wikimedia.org/wiki/User:Magister_Mathematicae [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AConic_sections_small.png"&gt;&lt;img width="256" alt="Conic sections small" src="https://upload.wikimedia.org/wikipedia/commons/b/bf/Conic_sections_small.png"/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We don&amp;#39;t have to care about the parabola or hyberbola cases (because our cone is infinitely long).&lt;/p&gt;

&lt;h2 id="toc_2"&gt;Ellipse estimations&lt;/h2&gt;

&lt;p&gt;Our goal is to find some way to estimate the area of an intersection of that ellipse and a rectangle. And we also need to find the geometric center of that intersection.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/RectLightImprovements/LightGeoClipping.png"&gt;&lt;/p&gt;

&lt;p&gt;Working with the true ellipse here is far too expensive. The method would be extremely complex. However, we can make some estimations. Also, the rectangle and the ellipse are not aligned in any easy way -- so that makes it more complex.&lt;/p&gt;

&lt;p&gt;One idea is to use 2 squares, instead of one. We will place the squares near the vertices of the of the ellipse, and balance the area of the squares so that they roughly match the ellipse.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/RectLightImprovements/LightGeoClipping2.png"&gt;&lt;/p&gt;

&lt;p&gt;With this method, as the ellipse widens, our simulation takes into account that widening.&lt;/p&gt;

&lt;h2 id="toc_3"&gt;Flat on&lt;/h2&gt;

&lt;p&gt;2 squares seems to help in the flat-on case, also.&lt;/p&gt;

&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-yw4l"&gt;Original Method&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;New Method&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/BasicRectLight2.png"&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/ImprovedRectLight2.png"&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;As you can see, improved method helps smooth out the result. The light retains it&amp;#39;s rectangular shape better.&lt;/p&gt;

&lt;h2 id="toc_4"&gt;Representative point tweaks&lt;/h2&gt;

&lt;p&gt;The goal of these methods is to find a quick estimate to integrating the BRDF across all angles. When we&amp;#39;re finding the intersection of the specular cone and the light, we&amp;#39;re really trying to estimate the intersection of the light geometry and the BRDF equation. If we could then find the mean of the BRDF in the intersection area, we would have a good estimate of the full integral.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/RectLightImprovements/LightGeoClipping3.png"&gt;&lt;/p&gt;

&lt;p&gt;To estimate the mean, we adjust the reflection direction to find a new &amp;quot;representative point&amp;quot; for specular. As Drobot explains in his article, he was working with a phong based BRDF. However, with GGX, I&amp;#39;m finding the equation is very steep, and that small adjustments to the representative point have a huge effect on the result!&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/RectLightImprovements/LightGeoClipping4.png"&gt;&lt;/p&gt;

&lt;p&gt;This might be accentuated by the fact that I&amp;#39;m using a very wide angle for the specular cone. The wide angle helps give us really blurry highlights, but it makes some artifact worse.&lt;/p&gt;

&lt;p&gt;So I&amp;#39;m finding that we have to be a little more conservative with the representative point. I&amp;#39;m experimenting with taking the average of the intersection center and the unmodified reflection direction.&lt;/p&gt;

&lt;h2 id="toc_5"&gt;New artifacts&lt;/h2&gt;

&lt;p&gt;Unfortunately, the new method can add some artifacts. Sometimes the influence of each square can separate, which can end up giving the impression of 2 separate specular highlights. This can cause some real problems if one square clipped by one side of the light, and the other square is clipped by the other side.&lt;/p&gt;

&lt;p&gt;There are also some cases where the single-square method looks better, even though it may be incorrect... With a single square, the highlight will be incorrectly sharp on extreme angles. But it still looks right to the viewer.&lt;/p&gt;

&lt;p&gt;Here is an example of the artifacts that can occur in some cases&lt;/p&gt;

&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-yw4l"&gt;Original Method&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;New Method&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/BasicRectLightBad1.png"&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/ImprovedRectLightBad1.png"&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Here the rectangle keeps it&amp;#39;s shape better, but the separate effects of the 2 squares starts to become visible.&lt;/p&gt;

&lt;p&gt;In a real world situation, this is unlikely to be as visible. Since this is a surface with a high &amp;quot;roughness&amp;quot; we would normally expect there to be a normal map and more geometric detail. That will serve to hide the artifacts.&lt;/p&gt;

&lt;h2 id="toc_6"&gt;More screenshots&lt;/h2&gt;

&lt;p&gt;Here are a few more comparison shots:&lt;/p&gt;

&lt;p&gt;&lt;/style&gt;
&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-yw4l"&gt;Original Method&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;New Method&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/BasicRectLight3.png"&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/ImprovedRectLight3.png"&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/BasicRectLight4.png"&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;img src="/assets/media/RectLightImprovements/ImprovedRectLight4.png"&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;h2 id="toc_7"&gt;In-engine visualization&lt;/h2&gt;

&lt;p&gt;You can use the cvar &amp;quot;LightResolveDebugging&amp;quot; to visualize the squares, intersection ellipse and representative points. Type &amp;quot;cv.LightResolveDebugging = True&amp;quot; in the IronPython window in the editor.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/RectLightImprovements/Visualization.png"&gt;&lt;/p&gt;
</description>
    </item>
    <item>
      <title>What to do first</title>
      <link>http://xlgames-inc.github.io/posts/whattodofirst</link>
      <pubDate>December 04, 2015</pubDate>
      <description>&lt;p&gt;So, you&amp;#39;ve just downloaded XLE, and you&amp;#39;re wondering what to do first? Here&amp;#39;s a suggestion for the first 10 minutes:&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_0"&gt;Startup the level editor&lt;/h2&gt;

&lt;p&gt;Select the Level Editor as the startup project:
&lt;br/&gt;&lt;img src="/assets/media/SolutionExplorerLE.png"&gt;&lt;/p&gt;

&lt;p&gt;You should use the &lt;strong&gt;Debug/x64&lt;/strong&gt; or &lt;strong&gt;Release/x64&lt;/strong&gt; configuration. Though XLE works in both 32 bit and 64 bits modes, normal usage for the level editor should be in 64 bits mode.&lt;/p&gt;

&lt;p&gt;If you have trouble compiling, see the &lt;a href="https://github.com/xlgames-inc/XLE/wiki/CompilingFirstSteps"&gt;Getting Started Compiling page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Start up this application, and you should see a large 3D window and various other windows about.&lt;/p&gt;

&lt;p&gt;The level editor is based on a great project from Sony WWS &lt;a href="https://github.com/SonyWWS/LevelEditor"&gt;SonyWWSLevelEditor&lt;/a&gt;.
It has been modified to work with XLE, and some XLE specific behaviour has been added. I think this is a great example of the Open Source concept working for games developers.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_1"&gt;Creating an object&lt;/h2&gt;

&lt;p&gt;For the first minute, the 3D window will appear black. This is because shaders are compiling in the background. They will get flushed to disk on application shutdown, so it only happens the first time.&lt;/p&gt;

&lt;p&gt;Select &lt;em&gt;&amp;quot;Window/Resources&amp;quot;&lt;/em&gt; to open the resources panel. You should see the directory structure under the &amp;quot;working&amp;quot; folder. Here, you can find &lt;strong&gt;.dae&lt;/strong&gt; (Collada) files.&lt;/p&gt;

&lt;p&gt;Find a dae file, and drag it into the 3D viewport (or you can drag from Windows Explorer).&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/DesignView.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id="toc_2"&gt;Collada notes&lt;/h3&gt;

&lt;p&gt;XLE should support all well formatted Collada files. But unfortunately some exporters produce poorly formatted files. For example, sometimes texture names are prefixed with &lt;em&gt;&amp;quot;file://&amp;quot;&lt;/em&gt; -- this isn&amp;#39;t supported in XLE.&lt;/p&gt;

&lt;p&gt;But you should be fine with the Blender Collada exporter, or the OpenCollada exporter for Max/Maya. You can also use &lt;a href="http://www.assimp.org/"&gt;Open Asset Import Library&lt;/a&gt; to convert files to Collada format.&lt;/p&gt;

&lt;h3 id="toc_3"&gt;Placement documents&lt;/h3&gt;

&lt;p&gt;In the level editor, the main &amp;quot;game&amp;quot; file is like a solution file in Visual Studio. It contains links to project documents under it. One type of document is a &amp;quot;placements&amp;quot; document (called a &lt;em&gt;placements cell&lt;/em&gt;). This contains a list of static objects with basic properties.&lt;/p&gt;

&lt;p&gt;Normal large world games should have many placement cells. These can be arranged in a grid structure. But cells can also be overlapping, or have irregular shapes.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/DefaultPlacementCell.png"&gt;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;When you first startup the Level Editor, you get a default untitled placements document. However, you can reconfigure it by right-clicking &amp;quot;Placements&amp;quot; in the Project Lister and selecting &amp;quot;Configure placements...&amp;quot;&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_4"&gt;Adding lighting settings&lt;/h2&gt;

&lt;p&gt;Probably, you will want to play with some lighting settings next.&lt;/p&gt;

&lt;p&gt;First, Find the &amp;quot;Palette&amp;quot; window (select &amp;quot;Window/Palette&amp;quot; to enable it if it isn&amp;#39;t visible).&lt;/p&gt;

&lt;p&gt;Here, open up the &amp;quot;Lights&amp;quot; category. You can drag items from the &amp;quot;Palette&amp;quot; window into the &amp;quot;Project Lister&amp;quot;.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/PaletteWindow.png"&gt;&lt;/p&gt;

&lt;p&gt;Drag these 4 items from the &amp;quot;Lights&amp;quot; category of the &amp;quot;Palette&amp;quot; into the &amp;quot;Project Lister&amp;quot;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DirLight&lt;/li&gt;
&lt;li&gt;AreaLight&lt;/li&gt;
&lt;li&gt;AmbientSettings&lt;/li&gt;
&lt;li&gt;ToneMapSettings&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Try selecting the newly created &amp;quot;DirLight&amp;quot; in the Project Lister. You can press &amp;quot;M&amp;quot; to enable the &amp;quot;move&amp;quot; manipulator to move it around.&lt;/p&gt;

&lt;p&gt;Now, find the &amp;quot;Property Editor&amp;quot; window (select &amp;quot;Window/Property Editor&amp;quot; if it is hidden).
Here you can see all of the properties for the item. You can customize the lighting environment by playing with the settings for the 4 items you created.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;img src="/assets/media/PropertyEditor.png"&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_5"&gt;Console debugging modes&lt;/h2&gt;

&lt;p&gt;Next, find the &amp;quot;IronPython&amp;quot; window (use Window/Iron Python to enable it if it is hidden).&lt;/p&gt;

&lt;p&gt;Try typing &lt;strong&gt;&amp;quot;cv.DeferredDebugging = 1&amp;quot;&lt;/strong&gt;. This will enable a debugging mode. You can type &lt;strong&gt;&amp;quot;cv.DeferredDebugging = 0&amp;quot;&lt;/strong&gt; to disable it again.&lt;/p&gt;

&lt;p&gt;Here, &amp;quot;cv&amp;quot; stands for &lt;em&gt;&amp;quot;console variable&amp;quot;&lt;/em&gt;.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id="toc_6"&gt;What about the next 10 minutes?&lt;/h2&gt;

&lt;p&gt;Well, that&amp;#39;s the end of the quick introduction. The rest is up to you. Have fun!&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Area Lights with Physically Based Rendering</title>
      <link>http://xlgames-inc.github.io/posts/arealightsandpbr</link>
      <pubDate>December 02, 2015</pubDate>
      <description>&lt;p&gt;XLE now support a few area light types: &lt;em&gt;sphere&lt;/em&gt;, &lt;em&gt;tube&lt;/em&gt; &amp;amp; &lt;em&gt;rectangle&lt;/em&gt; (with &lt;em&gt;disc&lt;/em&gt; and maybe some other shapes coming soon). Maybe in a future post I&amp;#39;ll go into some details about the implementation (actually, it&amp;#39;s quite interesting!). But this post is about something different: this post is about why they are important.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m finding that it makes a huge difference. So much so that not only do we want to support area lights -- we also want to outlaw non-area lights. Point light sources are now the enemy!&lt;/p&gt;

&lt;h2 id="toc_0"&gt;PBR Concept&lt;/h2&gt;

&lt;p&gt;The biggest buzzword at the moment is Physically Based Renderering (or PBR, or sometimes Physically Based Shading). But what does that mean, really?
Partially it means sampling values from the real world. But really it&amp;#39;s just a container under which we&amp;#39;re placing many related concepts.&lt;/p&gt;

&lt;p&gt;One of these &amp;quot;sub-concepts&amp;quot; of PBR is the idea that a single material should work in many different lighting environments.&lt;/p&gt;

&lt;p&gt;In the past, artists often tweaked the material settings for objects to suit the lighting environment they are in. This is a problem because it means that if the object moves into another lighting environment, the material settings must be tweaked to match.&lt;/p&gt;

&lt;p&gt;This happen in both movies and games. In games, it&amp;#39;s particularly a problem for games with a day/night cycle. In games where time is changing, the lighting environment is also always changing. Previously this means that extreme lighting settings (sunsets, etc) had to be more subtle than real-life. What we want is materials that look correct in all possible lighting environments.&lt;/p&gt;

&lt;h2 id="toc_1"&gt;Specular highlight size&lt;/h2&gt;

&lt;p&gt;There&amp;#39;s a simple way to think about this. Let&amp;#39;s consider the size of a specular highlight.&lt;/p&gt;

&lt;p&gt;Here are 5 spheres rendered in Blender with Cook Torrence lighting.
&lt;img src="/assets/media/SpecularExample.png"&gt;&lt;/p&gt;

&lt;p&gt;Each sphere is reflecting the same light, but the material settings are changed to change the size of highlight in each sphere.&lt;/p&gt;

&lt;p&gt;In older games, this was the only way to change the size of the highlight. If a highlight appeared too small or too big, an artist would change the material settings.&lt;/p&gt;

&lt;p&gt;However there should be something else that should effect the size of the highlight: obviously, it&amp;#39;s the shape of the light! We need area lights to achieve that.&lt;/p&gt;

&lt;h2 id="toc_2"&gt;New model for specular highlights&lt;/h2&gt;

&lt;p&gt;So, we have two important concepts to control the size of the specular highlights:
- &lt;strong&gt;material roughness&lt;/strong&gt;
- &lt;strong&gt;light size&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Notice that these ideas are simpler and clearer than concepts in older engines. Previously we might talk about &lt;em&gt;&amp;quot;gloss&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;specular power&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;specular hardness.&amp;quot;&lt;/em&gt; But what do those really mean?&lt;/p&gt;

&lt;p&gt;Roughness is much clearer. It&amp;#39;s just a single value between 0 and 1. And it&amp;#39;s linear -- not exponential! It&amp;#39;s probably the most important variable of our material model (other than Color, I guess).&lt;/p&gt;

&lt;h3 id="toc_3"&gt;Roughness&lt;/h3&gt;

&lt;p&gt;Roughness controls the size of the highlight by spreading the light energy over a large area.&lt;/p&gt;

&lt;p&gt;Notice the image above. In one extreme the highlight is a dense, concentrated point of light. In the other extreme, the specular light energy is spread out over a very large area, so that it appear almost like diffuse lighting.&lt;/p&gt;

&lt;p&gt;This is critical for us because in XLE, all materials are reflective. But when the roughness is very high, the reflections get spread out over a very large area, so that they appear almost diffuse.&lt;/p&gt;

&lt;p&gt;Also, both specular highlights and IBL reflections get spread out in the same way. Sometimes it&amp;#39;s even difficult to tell what is a specular highlight, and what is a reflection.&lt;/p&gt;

&lt;h3 id="toc_4"&gt;Light size&lt;/h3&gt;

&lt;p&gt;With rectangle and sphere lights, the size of the highlight will change significantly with the size of the light (and also the distance to the light). But it changes in a very different way to roughness. Large lights give an appearance that just wasn&amp;#39;t possible previously.&lt;/p&gt;

&lt;p&gt;If we want to make a very large specular highlight, we can do that by changing the size of the light. So there is no need to tweak the material any more!&lt;/p&gt;

&lt;h2 id="toc_5"&gt;No more point lights&lt;/h2&gt;

&lt;p&gt;Now, the only problem with this is point lights no longer look correct. If the material is balanced correctly for use with area lights, then highlights from point lights will look unnaturally too small. This is logical, because point light sources don&amp;#39;t exist in reality.&lt;/p&gt;

&lt;p&gt;Fortunately, sphere lights are only slightly more expensive than point lights. And we can use the same BRDFs with sphere lights. So it&amp;#39;s an easy transition.&lt;/p&gt;

&lt;p&gt;So the solution is to always use area lights, and never use point lights.&lt;/p&gt;

&lt;h2 id="toc_6"&gt;Authoring material settings&lt;/h2&gt;

&lt;p&gt;Even though we&amp;#39;ve created a separation between material settings and lighting environment settings, I&amp;#39;m still finding that we will probably need some kind of standard lighting environment for authoring the material settings.&lt;/p&gt;

&lt;p&gt;That is, the values that artists will select for roughness may vary with the lighting environment they are testing with.&lt;/p&gt;

&lt;p&gt;Probably an &lt;em&gt;Image Based Lighting&lt;/em&gt; environment would be best for this. This is a great reason why BRDF for IBL and dynamic lights should match as closely as possible. We should author the material settings in a 100% IBL environment, and then we should expect that dynamic lights should just work.&lt;/p&gt;

&lt;h2 id="toc_7"&gt;Simpler... and better&lt;/h2&gt;

&lt;p&gt;So, I found that area lights are critical to the PBR concept. So much so that an engine that doesn&amp;#39;t support area lights shouldn&amp;#39;t properly be called PBR.&lt;/p&gt;

&lt;p&gt;And I think it meets two important concepts of PBR:
- materials should look correct in all lighting environments
- it should be simpler and better (at the same time)&lt;/p&gt;

&lt;p&gt;3 cheers for area lights!&lt;/p&gt;
</description>
    </item>
    <item>
      <title>First post</title>
      <link>http://xlgames-inc.github.io/posts/first</link>
      <pubDate>December 01, 2015</pubDate>
      <description>&lt;p&gt;This blog will contain some day-to-day information about XLE.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s start with some screenshots, rendered with XLE:&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/media/CaptainSplitBrightLogo.png"&gt;&lt;br&gt;
The Captain character from &lt;a href="http://archeage.xlgames.com/"&gt;Archeage&lt;/a&gt; (&lt;a href="http://www.trionworlds.com/archeage/en/"&gt;US site&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/media/Archepunk4.png"&gt;&lt;br&gt;
One of the (many) costumes from Archeage.&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/media/SkeleCommander.png"&gt;&lt;br&gt;
A skeleton enemy from Archeage.&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/media/Copperhead1.png"&gt;&lt;br&gt;
A military spacecraft.&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/media/gweonid4.png"&gt;&lt;br&gt;
The &amp;quot;Gweonid&amp;quot; character from Archeage.&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/media/MetalGolem0.png"&gt;&lt;br&gt;
A metal golem character from Archeage.&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/media/NyraAndDragon1.png"&gt;&lt;br&gt;
One of the costumes from Archeage, and &amp;quot;Nyra&amp;quot; character.
Nyra model comes from &lt;a href="http://www.paultosca.com/"&gt;Paul Tosca&lt;/a&gt;. Thanks for the great model, Paul!&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/media/Nyra9.png"&gt;&lt;br&gt;
Detail of Nyra&lt;/p&gt;

&lt;p&gt;&lt;img src="/assets/media/Nyra10.png"&gt;&lt;br&gt;
One more from Nyra.&lt;/p&gt;

&lt;p&gt;Many of the backgrounds in these images are from &lt;a href="http://www.hdrlabs.com/sibl/archive.html"&gt;sIBL Archive&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
  </channel>
</rss>
